{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation into diagnoses\n",
    "\n",
    "Proof of concept to see whether hospitals can move from diagnoses written by doctors in freetext, to a dropdown menu of choices.\n",
    "\n",
    "This notebook is one of the main examples of using NLP in this project.\n",
    "\n",
    "Start with loading all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade --user pip\n",
    "!{sys.executable} -m pip install pandas pyodbc numpy sklearn nltk wordcloud matplotlib --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for manipulating data in dataframes\n",
    "import pyodbc # for reading sql into pandas\n",
    "import numpy as np # for numerical calculations\n",
    "from collections import Counter # for counting the number of words in dictionaries\n",
    "import re # for finding regular expressions in text\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk import word_tokenize # to break sentences into words\n",
    "from nltk.stem import WordNetLemmatizer # to find the lemmas of words\n",
    "\n",
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from wordcloud import WordCloud # to visualise wordclous\n",
    "import matplotlib.pyplot as plt # other visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define medical dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomedct = pd.read_csv('sct2_Description_Snapshot-en_INT_20190731.txt',sep=\"\\t\",usecols=['term'])\n",
    "medical_terms_series = snomedct['term'].str.lower().str.split().dropna()\n",
    "medical_terms_list = []\n",
    "for x in medical_terms_series:\n",
    "    medical_terms_list.extend(x)\n",
    "medical_terms_list = [medical_term for medical_term in medical_terms_list if medical_term.isalpha()]\n",
    "medical_terms_list = [medical_term.strip(\"()\") for medical_term in medical_terms_list]\n",
    "medical_terms_list = [medical_term.strip(\"(\") for medical_term in medical_terms_list]\n",
    "medical_terms_counts = Counter(medical_terms_list)\n",
    "print(medical_terms_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read SQL into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_conn = pyodbc.connect('DRIVER={SQL Server};'\n",
    "                            'SERVER=L_AAGwebapptest;'\n",
    "                            'DATABASE=BedView;'\n",
    "                            'Trusted_Connection=yes') \n",
    "query = \"set transaction isolation level read uncommitted select cn.Diagnosis,ps.SpecialtyDesc,ps.AdmissionDate,ps.DischargeDate,pn.AmuTriage from tblClinicalNote cn inner join tblPatientSpell ps on ps.pkPatientSpellID=cn.fkPatientSpellID inner join tblPatientNote pn on pn.fkPatientSpellID=ps.pkPatientSpellID\"\n",
    "df = pd.read_sql(query, sql_conn)\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.index < 10]\n",
    "df_19 = df[df['Diagnosis'].str.lower().str.contains(\"acute coronary syndrome\",regex=False)]\n",
    "print(df_19['Diagnosis'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the abbreviations that doctors use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_caps_after = snomedct['term'].str.findall(r\"((?:\\b[A-Za-z]+\\b\\s)+\\([A-Z][A-Z]+\\))\")\n",
    "consecutive_caps_after = consecutive_caps_after[consecutive_caps_after.astype(str)!='[]']\n",
    "consecutive_caps_after = consecutive_caps_after.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_after:\n",
    "    slist.extend(x)\n",
    "\n",
    "term_to_abbreviation_dict = {}\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\(([A-Za-z]+)\\)\", l)[0]\n",
    "    len_inside_brackets = len(inside_brackets)\n",
    "    num_words = len(l.split()) - 1\n",
    "    if len_inside_brackets == num_words:\n",
    "        words_before_brackets = []\n",
    "        for i in range(len_inside_brackets):\n",
    "            if i<num_words: words_before_brackets.insert(0,l.split(\" \")[-i-2])\n",
    "        string_before_brackets = \" \".join(words_before_brackets)\n",
    "        if all(words_before_brackets[i].lower()[0]==inside_brackets[i].lower() for i in range(len_inside_brackets)):\n",
    "            if string_before_brackets not in term_to_abbreviation_dict.keys() and inside_brackets not in term_to_abbreviation_dict.values():\n",
    "                term_to_abbreviation_dict[string_before_brackets] = inside_brackets\n",
    "        \n",
    "\n",
    "consecutive_caps_before = snomedct['term'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_before = consecutive_caps_before[consecutive_caps_before.astype(str)!='[]']\n",
    "consecutive_caps_before = consecutive_caps_before.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_before:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\", l)[0]\n",
    "    inside_brackets = inside_brackets.strip('()')\n",
    "    words_inside_brackets = inside_brackets.split()\n",
    "    num_words_inside_brackets = len(words_inside_brackets)\n",
    "    word_before_brackets = l.split()[0]\n",
    "    if num_words_inside_brackets==len(word_before_brackets) and all(words_inside_brackets[i].lower()[0]==word_before_brackets[i].lower() for i in range(num_words_inside_brackets)):\n",
    "        if inside_brackets not in term_to_abbreviation_dict.keys() and word_before_brackets not in term_to_abbreviation_dict.values():\n",
    "            term_to_abbreviation_dict[inside_brackets] = word_before_brackets\n",
    "            \n",
    "\n",
    "consecutive_caps_dash = snomedct['term'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\-\\s(?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\")\n",
    "consecutive_caps_dash = consecutive_caps_dash[consecutive_caps_dash.astype(str)!='[]']\n",
    "consecutive_caps_dash = consecutive_caps_dash.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_dash:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    after_dash = l.split(' - ')[1]\n",
    "    words_after_dash = after_dash.split()\n",
    "    num_words_after_dash = len(words_after_dash)\n",
    "    word_before_dash = l.split(' - ')[0]\n",
    "    if num_words_after_dash==len(word_before_dash) and all(words_after_dash[i].lower()[0]==word_before_dash[i].lower() for i in range(num_words_after_dash)):\n",
    "        if after_dash not in term_to_abbreviation_dict.keys():\n",
    "            term_to_abbreviation_dict[after_dash] = word_before_dash\n",
    "\n",
    "\n",
    "\n",
    "consecutive_caps_after = df['Diagnosis'].str.findall(r\"((?:\\b[A-Za-z]+\\b\\s)+\\([A-Z][A-Z]+\\))\")\n",
    "consecutive_caps_after = consecutive_caps_after[consecutive_caps_after.astype(str)!='[]']\n",
    "consecutive_caps_after = consecutive_caps_after.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_after:\n",
    "    slist.extend(x)\n",
    "\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\(([A-Za-z]+)\\)\", l)[0]\n",
    "    len_inside_brackets = len(inside_brackets)\n",
    "    num_words = len(l.split()) - 1\n",
    "    if len_inside_brackets == num_words:\n",
    "        words_before_brackets = []\n",
    "        for i in range(len_inside_brackets):\n",
    "            if i<num_words: words_before_brackets.insert(0,l.split()[-i-2])\n",
    "        string_before_brackets = \" \".join(words_before_brackets)\n",
    "        if all(words_before_brackets[i].lower()[0]==inside_brackets[i].lower() for i in range(len_inside_brackets)):\n",
    "            if string_before_brackets not in term_to_abbreviation_dict.keys() and inside_brackets not in term_to_abbreviation_dict.values():\n",
    "                term_to_abbreviation_dict[string_before_brackets] = inside_brackets\n",
    "        \n",
    "\n",
    "consecutive_caps_before = df['Diagnosis'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_before = consecutive_caps_before[consecutive_caps_before.astype(str)!='[]']\n",
    "consecutive_caps_before = consecutive_caps_before.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_before:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\", l)[0]\n",
    "    inside_brackets = inside_brackets.strip('()')\n",
    "    words_inside_brackets = inside_brackets.split()\n",
    "    num_words_inside_brackets = len(words_inside_brackets)\n",
    "    word_before_brackets = l.split()[0]\n",
    "    if num_words_inside_brackets==len(word_before_brackets) and all(words_inside_brackets[i].lower()[0]==word_before_brackets[i].lower() for i in range(num_words_inside_brackets)):\n",
    "        if inside_brackets not in term_to_abbreviation_dict.keys() and word_before_brackets not in term_to_abbreviation_dict.values():\n",
    "            term_to_abbreviation_dict[inside_brackets] = word_before_brackets\n",
    "            \n",
    "\n",
    "consecutive_caps_dash = df['Diagnosis'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\-\\s(?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\")\n",
    "consecutive_caps_dash = consecutive_caps_dash[consecutive_caps_dash.astype(str)!='[]']\n",
    "consecutive_caps_dash = consecutive_caps_dash.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_dash:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    after_dash = l.split('-')[1]\n",
    "    words_after_dash = after_dash.split()\n",
    "    num_words_after_dash = len(words_after_dash)\n",
    "    word_before_dash = l.split(' - ')[0]\n",
    "    if num_words_after_dash==len(word_before_dash) and all(words_after_dash[i].lower()[0]==word_before_dash[i].lower() for i in range(num_words_after_dash)):\n",
    "        after_dash = after_dash.strip()\n",
    "        if after_dash not in term_to_abbreviation_dict.keys() and word_before_dash not in term_to_abbreviation_dict.values():\n",
    "            term_to_abbreviation_dict[after_dash] = word_before_dash\n",
    "        \n",
    "term_to_abbreviation_dict['trop'] = 'troponin'\n",
    "term_to_abbreviation_dict['inf ex'] = 'infective ex'\n",
    "term_to_abbreviation_dict['inf asthma'] = 'infective asthma'\n",
    "term_to_abbreviation_dict[' exa '] = ' exacerbation '\n",
    "term_to_abbreviation_dict['exac[^a-z]'] = 'exacerbation '\n",
    "term_to_abbreviation_dict['ex copd'] = 'exacerbation copd'\n",
    "term_to_abbreviation_dict['copd ex '] = 'copd exacerbation '\n",
    "term_to_abbreviation_dict['ex of'] = 'exacerbation of'\n",
    "term_to_abbreviation_dict['ex asthma'] = 'exacerbation asthma'\n",
    "term_to_abbreviation_dict['ex due'] = 'exacerbation due'\n",
    "term_to_abbreviation_dict['ex chronic'] = 'exacerbation chronic'\n",
    "term_to_abbreviation_dict['infective exacerbation'] = 'ie'\n",
    "term_to_abbreviation_dict['infected exacerbation'] = 'ie'\n",
    "term_to_abbreviation_dict['ie of copd'] = 'iecopd'\n",
    "term_to_abbreviation_dict['ie copd'] = 'iecopd'\n",
    "term_to_abbreviation_dict['ie- copd'] = 'iecopd'\n",
    "term_to_abbreviation_dict['ie-copd'] = 'iecopd'\n",
    "term_to_abbreviation_dict['pulmonary embolism'] = 'pe'\n",
    "term_to_abbreviation_dict['nebuliser'] = 'neb'\n",
    "term_to_abbreviation_dict['nebulizer'] = 'neb'\n",
    "term_to_abbreviation_dict['nebulisers'] = 'nebs'\n",
    "term_to_abbreviation_dict['nebulizers'] = 'nebs'\n",
    "term_to_abbreviation_dict['influenza'] = 'flu'\n",
    "term_to_abbreviation_dict['over dose'] = 'overdose'\n",
    "term_to_abbreviation_dict['[^a-z]od[^a-z]'] = ' overdose '\n",
    "term_to_abbreviation_dict['msk'] = 'musculoskeletal'\n",
    "term_to_abbreviation_dict['o2 sat'] = 'oxygen saturation'\n",
    "term_to_abbreviation_dict['o sat'] = 'oxygen saturation'\n",
    "term_to_abbreviation_dict['sat of'] = 'saturation of'\n",
    "term_to_abbreviation_dict['sat %'] = 'saturation %'\n",
    "term_to_abbreviation_dict['sat ?'] = 'saturday %'\n",
    "term_to_abbreviation_dict['sat 0'] = 'saturday 0'\n",
    "term_to_abbreviation_dict['(\\s)sat(\\s[0-9][0-9]%)'] = \"\\1saturated\\2\"\n",
    "term_to_abbreviation_dict['(\\s)sat(\\s[0-9]\\.)'] = \"\\1saturday\\2\"\n",
    "term_to_abbreviation_dict['\"(\\s)sat(.{3,})sun'] = \"\\1saturday\\2sunday\"\n",
    "term_to_abbreviation_dict['\"(\\s)sat(\\s[0-9]{1,2}th)'] = \"\\1saturday\\2\"\n",
    "\n",
    "#term_to_abbreviation_dict = {key.lower():val.lower() for (key,val) in term_to_abbreviation_dict.items()}\n",
    "for key,val in term_to_abbreviation_dict.items():\n",
    "    print(key+\" & \"+val+'\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_caps_series = df[~df['Diagnosis'].str.isupper()]['Diagnosis'].str.findall(r\"\\b[a-z]{2,}\\b\\s\\b[A-Z]{2,}\\b\\s\\b[a-z]{2,}\\b\")\n",
    "consecutive_caps_series = consecutive_caps_series[consecutive_caps_series.map(lambda d: len(d)) > 0]\n",
    "slist = []\n",
    "for x in consecutive_caps_series:\n",
    "    slist.extend(x)\n",
    "\n",
    "# function to get unique values \n",
    "def unique(list1): \n",
    "  \n",
    "    # intilize a null list \n",
    "    unique_list = [] \n",
    "      \n",
    "    # traverse for all elements \n",
    "    for x in list1: \n",
    "        # check if exists in unique_list or not \n",
    "        if x.split()[1] not in unique_list: \n",
    "            unique_list.append(x.split()[1])\n",
    "    return [string.lower() for string in unique_list]\n",
    "        \n",
    "abbreviations = [l.split()[1].lower() for l in slist]\n",
    "term_to_abbreviation_dict_lower = [v.lower() for v in term_to_abbreviation_dict.values()]\n",
    "abbreviations = [abbreviation for abbreviation in abbreviations if abbreviation in term_to_abbreviation_dict_lower]\n",
    "abbreviation_counts = Counter(abbreviations)\n",
    "print(abbreviation_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove entries where AmuTriage is -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AMU = df[df['AmuTriage']!='-1'].copy()\n",
    "df_AMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AMU = df_AMU[df_AMU['AdmissionDate']>'2018-11-07 09:13:59']\n",
    "df_AMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_nonAMU = df[df['AmuTriage']=='-1'].copy()\n",
    "#df_nonAMU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define spelling correction tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter()\n",
    "english_WORDS = Counter(words(open('big.txt').read()))\n",
    "dict.update(WORDS,english_WORDS)\n",
    "dict.update(WORDS,medical_terms_counts)\n",
    "dict.update(WORDS,abbreviation_counts)\n",
    "'''floors = ['A','B','C','D','E','F','G']\n",
    "for floor in floors:\n",
    "    for i in range(9):\n",
    "        WORDS[floor+str(i+1)] = 1'''\n",
    "print(WORDS)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add words that you don't want in the wordclouds like, union(['useless','word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define American->British correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def american_to_british(tokens):\n",
    "    for t in tokens:\n",
    "        t = re.sub(r\"(...)or$\", r\"\\1our\", t)\n",
    "        t = re.sub(r\"([bt])er$\", r\"\\1re\", t)\n",
    "        t = re.sub(r\"([iy])z(e[drs]|e$|ing|ation)\", r\"\\1s\\2\", t)\n",
    "        t = re.sub(r\"^(s.?[iy])s(e[drs]|e$|ing|ation)\", r\"\\1z\\2\", t) # convert back words starting with s like size, seize\n",
    "        t = re.sub(r\"og$\", \"ogue\", t)\n",
    "        yield t\n",
    "        \n",
    "class CustomVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(american_to_british(tokenize(doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add words that come out wrong after lemmatization, like {'dos':'dose'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_lemma_dict = {'cathetre':'catheter','ac':'acs'} \n",
    "# undo American->British in catheter\n",
    "# undo removal of plural ACS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AMU['Diagnosis'] = df_AMU['Diagnosis'].str.lower()\n",
    "df_AMU['Diagnosis'] = df_AMU['Diagnosis'].replace(term_to_abbreviation_dict, regex=True)\n",
    "df_AMU['Diagnosis'] = df_AMU['Diagnosis'].replace(corrected_lemma_dict, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AMU[df_AMU['Diagnosis'].str.contains('acute corononary syndrome')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define counters to check amount of spelling correction needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words_before_correction = 0\n",
    "non_english_words_before_correction = 0\n",
    "english_words_after_correction = 0\n",
    "non_english_words_after_correction = 0\n",
    "medical_words_before_correction = 0\n",
    "medical_words_after_correction = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        \n",
    "        tokens = [t for t in word_tokenize(doc) if t.isalpha()]\n",
    "        no_stops = [t for t in tokens if t not in my_stop_words]\n",
    "        lemmatized = [self.wnl.lemmatize(t) for t in no_stops]\n",
    "        corrected = [correction(t) for t in lemmatized]\n",
    "        \n",
    "        '''\n",
    "        global english_words_before_correction, non_english_words_before_correction, english_words_after_correction, non_english_words_after_correction, medical_words_before_correction, medical_words_after_correction\n",
    "        for t in corrected_lemma:\n",
    "            if len(t)>1:\n",
    "                if t in english_WORDS: english_words_before_correction+=1\n",
    "                elif t in medical_terms_counts: medical_words_before_correction+=1\n",
    "                else: non_english_words_before_correction+=1\n",
    "        for t in corrected:\n",
    "            if len(t)>1:\n",
    "                if t in english_WORDS: english_words_after_correction+=1\n",
    "                elif t in medical_terms_counts: medical_words_after_correction+=1\n",
    "                else: non_english_words_after_correction+=1\n",
    "        '''\n",
    "        return [t for t in corrected if len(t)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer: tfidf\n",
    "tfidf = CustomVectorizer(tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fit_transform to document: csr_mat\n",
    "csr_mat = tfidf.fit_transform(df_AMU['Diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the words: words\n",
    "words = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print words \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "total_words_before_correction = english_words_before_correction + medical_words_before_correction + non_english_words_before_correction\n",
    "english_percentage_before_correction = 100*english_words_before_correction/total_words_before_correction\n",
    "non_english_percentage_before_correction = 100*(non_english_words_before_correction+medical_words_before_correction)/total_words_before_correction\n",
    "plt.barh(1,english_percentage_before_correction,color='g')\n",
    "plt.barh(1,non_english_percentage_before_correction,left=english_percentage_before_correction,color='r')\n",
    "plt.text(english_percentage_before_correction/2,1,\"{0:.0f}%\".format(english_percentage_before_correction),ha='center')\n",
    "plt.text(english_percentage_before_correction+non_english_percentage_before_correction/2,1,\"{0:.0f}%\".format(non_english_percentage_before_correction),ha='center')\n",
    "plt.text(english_percentage_before_correction/2,0.4,\"in\\nenglish\\ndictionary\",ha='center')\n",
    "plt.text(english_percentage_before_correction+non_english_percentage_before_correction/2,0.4,\"not\\nin\\nenglish\\ndictionary\",ha='center')\n",
    "plt.title('before spelling correction')\n",
    "plt.gca().axis('off')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "total_words_after_correction = english_words_after_correction + medical_words_after_correction + non_english_words_after_correction\n",
    "english_percentage_after_correction = 100*english_words_after_correction/total_words_after_correction\n",
    "non_english_percentage_after_correction = 100*(non_english_words_after_correction+medical_words_after_correction)/total_words_after_correction\n",
    "plt.barh(1,english_percentage_after_correction,color='g')\n",
    "plt.barh(1,non_english_percentage_after_correction,left=english_percentage_after_correction,color='r')\n",
    "plt.text(english_percentage_after_correction/2,1,\"{0:.0f}%\".format(english_percentage_after_correction),ha='center')\n",
    "plt.text(english_percentage_after_correction+non_english_percentage_after_correction/2,1,\"{0:.0f}%\".format(non_english_percentage_after_correction),ha='center')\n",
    "plt.text(english_percentage_after_correction/2,0.4,\"in\\nenglish\\ndictionary\",ha='center')\n",
    "plt.text(english_percentage_after_correction+non_english_percentage_after_correction/2,0.4,\"not\\nin\\nenglish\\ndictionary\",ha='center')\n",
    "plt.title('after spelling correction')\n",
    "plt.gca().axis('off')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "total_words_before_correction = english_words_before_correction + medical_words_before_correction + non_english_words_before_correction\n",
    "english_percentage_before_correction = 100*english_words_before_correction/total_words_before_correction\n",
    "medical_percentage_before_correction = 100*medical_words_before_correction/total_words_before_correction\n",
    "non_english_percentage_before_correction = 100*(non_english_words_before_correction)/total_words_before_correction\n",
    "plt.barh(1,english_percentage_before_correction,color='g')\n",
    "plt.barh(1,medical_percentage_before_correction,left=english_percentage_before_correction,color='orange')\n",
    "plt.barh(1,non_english_percentage_before_correction,left=english_percentage_before_correction+medical_percentage_before_correction,color='r')\n",
    "plt.text(english_percentage_before_correction/2,1,\"{0:.0f}% \".format(english_percentage_before_correction),ha='center')\n",
    "plt.text(english_percentage_before_correction+medical_percentage_before_correction/2,1,\"{0:.0f}%\".format(medical_percentage_before_correction),ha='center')\n",
    "plt.text(english_percentage_before_correction+medical_percentage_before_correction+non_english_percentage_before_correction/2,1,\"{0:.0f}%\".format(non_english_percentage_before_correction),ha='center')\n",
    "plt.text(english_percentage_before_correction/2,0.4,\"in\\nenglish\\ndictionary\",ha='center')\n",
    "plt.text(english_percentage_before_correction+medical_percentage_before_correction/2,0.4,\"in\\nmedical\\ndictionary\",ha='center')\n",
    "plt.text(english_percentage_before_correction+medical_percentage_before_correction+non_english_percentage_before_correction/2,0.4,\"in\\nneither\\ndictionary\",ha='center')\n",
    "plt.title('before spelling correction')\n",
    "plt.gca().axis('off')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "total_words_after_correction = english_words_after_correction + medical_words_after_correction + non_english_words_after_correction\n",
    "english_percentage_after_correction = 100*english_words_after_correction/total_words_after_correction\n",
    "medical_percentage_after_correction = 100*medical_words_after_correction/total_words_after_correction\n",
    "non_english_percentage_after_correction = 100*(non_english_words_after_correction)/total_words_after_correction\n",
    "plt.barh(1,english_percentage_after_correction,color='g')\n",
    "plt.barh(1,medical_percentage_after_correction,left=english_percentage_after_correction,color='orange')\n",
    "plt.barh(1,non_english_percentage_after_correction,left=english_percentage_after_correction+medical_percentage_after_correction,color='r')\n",
    "plt.text(english_percentage_after_correction/2,1,\"{0:.0f}% \".format(english_percentage_after_correction),ha='center')\n",
    "plt.text(english_percentage_after_correction+medical_percentage_after_correction/2,1,\"{0:.0f}%\".format(medical_percentage_after_correction),ha='center')\n",
    "#plt.text(english_percentage_after_correction+medical_percentage_after_correction+non_english_percentage_after_correction/2,1,\"{0:.0f}%\".format(non_english_percentage_after_correction),ha='center')\n",
    "plt.text(english_percentage_after_correction/2,0.4,\"in\\nenglish\\ndictionary\",ha='center')\n",
    "plt.text(english_percentage_after_correction+medical_percentage_after_correction/2,0.4,\"in\\nmedical\\ndictionary\",ha='center')\n",
    "#plt.text(english_percentage_after_correction+medical_percentage_after_correction+non_english_percentage_after_correction/2,0.4,\"in\\nneither\\ndictionary\",ha='center')\n",
    "plt.title('after spelling correction')\n",
    "plt.gca().axis('off')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud_words = []\n",
    "def print_wordclouds_and_pies():\n",
    "    donut_seg = 0\n",
    "    for i,counts in theme_counts_series.items():\n",
    "        # Initialize the word cloud\n",
    "        width = 1024 #int(1024*counts/theme_counts_max)\n",
    "        height = 720 #int(720*counts/theme_counts_max)\n",
    "        wc = WordCloud(\n",
    "            background_color=\"white\",\n",
    "            width = width,\n",
    "            height = height\n",
    "        )\n",
    "\n",
    "        # Select row : component\n",
    "        component = components_df.iloc[i]\n",
    "\n",
    "        # Generate the cloud\n",
    "        component.nlargest().index = component.nlargest().index.map(str)\n",
    "        wc.generate_from_frequencies(component.nlargest())\n",
    "        wordcloud_words.append(component.nlargest().index)\n",
    "\n",
    "        # Display the generated image:\n",
    "        figure, (wc_fig, counts_fig) = plt.subplots(nrows=1,ncols=2, figsize=(width/50,height/100))\n",
    "        wc_fig.imshow(wc, interpolation='bilinear')\n",
    "        wc_fig.axis(\"off\");\n",
    "\n",
    "        counts_fig.axis('equal')\n",
    "        colors = ['w' for j in theme_counts_series.index]\n",
    "        colors[donut_seg] = 'b'\n",
    "        labels = ['' for val in theme_counts_series.values]\n",
    "        labels[donut_seg] = str(counts)+\"/\"+str(theme_counts_series.values.sum())\n",
    "        donut_seg += 1\n",
    "        mypie, texts = counts_fig.pie(theme_counts_series.values/theme_counts_max, colors=colors, labels=labels, startangle=90, counterclock=False)\n",
    "        for text in texts: text.set_fontsize(20)\n",
    "        plt.setp( mypie, width=0.4, edgecolor='black')\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig('bedview_'+str(k)+'_'+str(i)+'_wordcloud.png')\n",
    "        plt.show()\n",
    "\n",
    "        print('-------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triage_counts = df_AMU['AmuTriage'].value_counts()\n",
    "triage_counts = triage_counts.rename({'-1':'not via AMU'})\n",
    "\n",
    "sizes = triage_counts*2\n",
    "y = []\n",
    "cumulative_y = 0\n",
    "for size in sizes:\n",
    "    cumulative_y+=np.sqrt(size)/2\n",
    "    y.append(cumulative_y)\n",
    "    cumulative_y+=np.sqrt(size)/2\n",
    "\n",
    "cm = plt.get_cmap('RdYlGn_r')\n",
    "colors = cm(sizes/max(sizes))\n",
    "triage_color_dict = {}\n",
    "\n",
    "plt.figure(figsize=(17,14*y[-1]/400))\n",
    "plt.ylim(max(y),np.sqrt(min(y)))\n",
    "for i,triage in enumerate(triage_counts.index):\n",
    "    triage_color_dict[triage] = colors[i]\n",
    "    plt.scatter(x=0,y=y[i],s=sizes[i],marker='s',color=triage_color_dict[triage])\n",
    "    if i==0 or y[i]-y[i-1]>4: plt.text((y[-1]/500)*np.sqrt(sizes)[i]/10000,y[i],triage,va='center')\n",
    "plt.gca().axis('off');\n",
    "plt.title('AMU Triage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialty_counts = df_AMU['SpecialtyDesc'].value_counts()\n",
    "\n",
    "sizes = specialty_counts*4\n",
    "y = []\n",
    "cumulative_y = 0\n",
    "for size in sizes:\n",
    "    cumulative_y+=np.sqrt(size/np.pi)\n",
    "    y.append(cumulative_y)\n",
    "    cumulative_y+=np.sqrt(size/np.pi)\n",
    "\n",
    "cm = plt.get_cmap('RdYlGn_r')\n",
    "colors = cm(sizes/max(sizes))\n",
    "specialty_color_dict = {}\n",
    "\n",
    "plt.figure(figsize=(17,14*y[-1]/600))\n",
    "plt.ylim(max(y),np.sqrt(min(y)))\n",
    "for i,specialty in enumerate(specialty_counts.index):\n",
    "    specialty_color_dict[specialty] = colors[i]\n",
    "    plt.scatter(x=0,y=y[i],s=sizes[i],marker='o',color=specialty_color_dict[specialty])\n",
    "    if i==0 or y[i]-y[i-1]>5: plt.text((y[-1]/500)*np.sqrt(sizes)[i]/10000,y[i],specialty,va='center')\n",
    "plt.gca().axis('off');\n",
    "plt.title('Specialty at discharge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triage_specialty_plots(dataframe):\n",
    "\n",
    "    triage_counts = dataframe['AmuTriage'].value_counts()\n",
    "\n",
    "    triage_sizes = triage_counts*2\n",
    "    triage_y = []\n",
    "    cumulative_triage_y = 0\n",
    "    for size in triage_sizes:\n",
    "        cumulative_triage_y+=np.sqrt(size)/2\n",
    "        triage_y.append(cumulative_triage_y)\n",
    "        cumulative_triage_y+=np.sqrt(size)/2\n",
    "\n",
    "\n",
    "    specialty_counts = dataframe['SpecialtyDesc'].value_counts()\n",
    "\n",
    "    specialty_sizes = specialty_counts*4\n",
    "    specialty_y = []\n",
    "    cumulative_specialty_y = 0\n",
    "    for size in specialty_sizes:\n",
    "        cumulative_specialty_y+=np.sqrt(size/np.pi)\n",
    "        specialty_y.append(cumulative_specialty_y)\n",
    "        cumulative_specialty_y+=np.sqrt(size/np.pi)\n",
    "\n",
    "    # Display the generated image:\n",
    "    figure, (triage_fig, specialty_fig) = plt.subplots(nrows=1,ncols=2, figsize=(8,14*triage_y[-1]/400))\n",
    "\n",
    "    triage_fig.set_ylim(max(triage_y),np.sqrt(min(triage_y)))\n",
    "    for i,triage in enumerate(triage_counts.index):\n",
    "        triage_fig.scatter(x=0,y=triage_y[i],s=triage_sizes[i],marker='s',color=triage_color_dict[triage])\n",
    "        if i==0 or triage_y[i]-triage_y[i-1]>4: triage_fig.text((triage_y[-1]/400)*np.sqrt(triage_sizes)[i]/500,triage_y[i],triage,va='center')\n",
    "    triage_fig.axis('off');\n",
    "    triage_fig.set_title('AMU Triage')\n",
    "\n",
    "    specialty_fig.set_ylim(max(specialty_y),np.sqrt(min(specialty_y)))\n",
    "    for i,specialty in enumerate(specialty_counts.index):\n",
    "        specialty_fig.scatter(x=0,y=specialty_y[i],s=specialty_sizes[i],marker='o',color=specialty_color_dict[specialty])\n",
    "        if i==0 or specialty_y[i]-specialty_y[i-1]>5: specialty_fig.text((triage_y[-1]/400)*np.sqrt(specialty_sizes)[i]/400,specialty_y[i],specialty,va='center')\n",
    "    specialty_fig.axis('off');\n",
    "    specialty_fig.set_title('Specialty at discharge')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wordclouds_and_plots():\n",
    "    donut_seg = 0\n",
    "    for i,counts in theme_counts_series.items():\n",
    "        # Initialize the word cloud\n",
    "        width = 1024 #int(1024*counts/theme_counts_max)\n",
    "        height = 720 #int(720*counts/theme_counts_max)\n",
    "        wc = WordCloud(\n",
    "            background_color=\"white\",\n",
    "            width = width,\n",
    "            height = height\n",
    "        )\n",
    "\n",
    "        # Select row : component\n",
    "        component = components_df.iloc[i]\n",
    "\n",
    "        # Generate the cloud\n",
    "        component.nlargest().index = component.nlargest().index.map(str)\n",
    "        wc.generate_from_frequencies(component.nlargest())\n",
    "        wordcloud_words.append(component.nlargest().index)\n",
    "\n",
    "        # Display the generated image:\n",
    "        figure, (wc_fig, counts_fig) = plt.subplots(nrows=1,ncols=2, figsize=(width/50,height/100))\n",
    "        wc_fig.imshow(wc, interpolation='bilinear')\n",
    "        wc_fig.axis(\"off\");\n",
    "\n",
    "        counts_fig.axis('equal')\n",
    "        colors = ['w' for j in theme_counts_series.index]\n",
    "        colors[donut_seg] = 'b'\n",
    "        labels = ['' for val in theme_counts_series.values]\n",
    "        labels[donut_seg] = str(counts)+\"/\"+str(theme_counts_series.values.sum())\n",
    "        donut_seg += 1\n",
    "        mypie, texts = counts_fig.pie(theme_counts_series.values/theme_counts_max, colors=colors, labels=labels, startangle=90, counterclock=False)\n",
    "        for text in texts: text.set_fontsize(20)\n",
    "        plt.setp( mypie, width=0.4, edgecolor='black')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        df_max_feature = df_AMU[df_AMU['max_feature']==str(i)].copy()\n",
    "        \n",
    "        triage_specialty_plots(df_max_feature)\n",
    "\n",
    "        print('-------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to 17 components are tried because there are 17 possible AMU triage categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ks = list(range(2,18))\n",
    "for k in ks:\n",
    "    # Create an NMF instance: model\n",
    "    model = NMF(n_components=k)\n",
    "\n",
    "    # Fit the model to articles\n",
    "    model.fit(csr_mat)\n",
    "    \n",
    "    # Transform the articles: nmf_features\n",
    "    nmf_features = model.transform(csr_mat)\n",
    "\n",
    "    # Create a pandas DataFrame: df\n",
    "    df_nmf = pd.DataFrame(nmf_features,index=df_AMU['Diagnosis'])\n",
    "\n",
    "    # Create a DataFrame: components_df\n",
    "    components_df = pd.DataFrame(model.components_,columns=words)\n",
    "    \n",
    "    df_nmf.columns = df_nmf.columns.astype(str)\n",
    "    df_nmf['max_feature'] = df_nmf.idxmax(axis=1)\n",
    "    \n",
    "    df_AMU['max_feature'] = df_nmf['max_feature'].values\n",
    "    theme_counts_series = df_AMU['max_feature'].value_counts()\n",
    "    theme_counts_series.index = theme_counts_series.index.astype(int)\n",
    "    theme_counts_max = theme_counts_series.values.max()\n",
    "    \n",
    "    print(\"number themes: \"+str(k))\n",
    "    print_wordclouds_and_plots()\n",
    "    print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
