{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with loading all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.container import ErrorbarContainer\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import nltk\n",
    "#nltk.set_proxy('https://username:password@proxy:8080') # uncomment this with your username, password, proxy\n",
    "#nltk.download('wordnet')\n",
    "from nltk import word_tokenize,sent_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus.reader import wordnet\n",
    "\n",
    "from lmfit.models import PolynomialModel, Model\n",
    "\n",
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Import figure from bokeh.plotting\n",
    "from bokeh.plotting import figure as bokeh_figure\n",
    "\n",
    "# Import output_file and show from bokeh.io\n",
    "from bokeh.io import show, output_notebook\n",
    "\n",
    "from bokeh.models import ColumnDataSource, Whisker, Band, Legend, LegendItem, Span, HoverTool\n",
    "\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomedct = pd.read_csv('sct2_Description_Snapshot-en_INT_20190731.txt',sep=\"\\t\",usecols=['term'])\n",
    "medical_terms_series = snomedct['term'].str.lower().str.split().dropna()\n",
    "medical_terms_list = []\n",
    "for x in medical_terms_series:\n",
    "    medical_terms_list.extend(x)\n",
    "medical_terms_list = [medical_term for medical_term in medical_terms_list if medical_term.isalpha()]\n",
    "medical_terms_list = [medical_term.strip(\"()\") for medical_term in medical_terms_list]\n",
    "medical_terms_list = [medical_term.strip(\"(\") for medical_term in medical_terms_list]\n",
    "medical_terms_counts = Counter(medical_terms_list)\n",
    "print(medical_terms_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_conn = pyodbc.connect('DRIVER={SQL Server};'\n",
    "                            'SERVER=L_AAGname;'\n",
    "                            'DATABASE=database_name;'\n",
    "                            'Trusted_Connection=yes') \n",
    "query = \"set transaction isolation level read uncommitted select a.recordid, b.description, c.description, d.description, a.inc_dincident,a.inc_time,a.inc_dreported,a.inc_submittedtime,a.inc_loctype,a.inc_result,a.inc_severity,a.show_other_contacts,a.show_employee,a.show_witness,a.show_document,a.inc_reportedby,a.inc_rep_email,a.inc_notes,a.inc_actiontaken from DatixCRM.dbo.code_unit b join DatixCRM.dbo.incidents_main a on a.inc_unit = b.code join DatixCRM.dbo.code_locactual c on a.inc_locactual = c.code join DatixCRM.dbo.code_specialty d on a.inc_specialty = d.code where a.inc_type='PAT' and a.inc_category='MEDIC' and a.inc_organisation='QA' and c.cod_parent2='QA'\"\n",
    "df = pd.read_sql(query, sql_conn, index_col='recordid')\n",
    "column_list = list(df.columns)\n",
    "column_list[0] = 'division-care_group'\n",
    "column_list[1] = 'location'\n",
    "column_list[2] = 'specialty'\n",
    "df['inc_rep_email'] = df['inc_rep_email'].str.split(\"@\",n=1,expand=True)[0]\n",
    "df.columns = column_list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_conn = pyodbc.connect('DRIVER={SQL Server};'\n",
    "                            'SERVER=L_AAGDATIX;'\n",
    "                            'DATABASE=DatixCRM;'\n",
    "                            'Trusted_Connection=yes') \n",
    "query = \"set transaction isolation level read uncommitted select a.recordid,c.description,b.con_email from contacts_main b join incidents_main a on a.recordid=b.recordid join code_con_type c on b.con_type=c.code where a.inc_type='PAT' and a.inc_category='MEDIC' and a.inc_organisation='QA' and a.show_employee='Y'\"\n",
    "df_con = pd.read_sql(query, sql_conn, index_col='recordid')\n",
    "df_con.columns = ['employee_involved','email']\n",
    "df_con['email'] = df_con['email'].str.split(\"@\",n=1,expand=True)[0]\n",
    "df_con = df_con[df_con['employee_involved'].str.contains('Employee')]\n",
    "df_con['employee_involved'] = df_con['employee_involved'].str.strip('Employee (')\n",
    "df_con['employee_involved'] = df_con['employee_involved'].str.strip('Registered ')\n",
    "df_con['employee_involved'] = df_con['employee_involved'].str.strip('\\)')\n",
    "df_con['employee_involved'] = df_con['employee_involved'].str.replace('administrative','administrative)')\n",
    "df_con['employee_involved'] = df_con['employee_involved'].str.replace(' / ','/')\n",
    "df_con['employee_involved'] = df_con['employee_involved'].str.replace(' and ',' & ')\n",
    "df_con['employee_involved'] = df_con['employee_involved'].str.replace(' staff','')\n",
    "df_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_con,on='recordid')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.index < 1000]\n",
    "# index < 3491 is the highest number that doesn't give memory error during n_components loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_caps_after = snomedct['term'].str.findall(r\"((?:\\b[A-Za-z]+\\b\\s)+\\([A-Z][A-Z]+\\))\")\n",
    "consecutive_caps_after = consecutive_caps_after[consecutive_caps_after.astype(str)!='[]']\n",
    "consecutive_caps_after = consecutive_caps_after.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_after:\n",
    "    slist.extend(x)\n",
    "\n",
    "term_to_abbreviation_dict = {}\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\(([A-Za-z]+)\\)\", l)[0]\n",
    "    len_inside_brackets = len(inside_brackets)\n",
    "    num_words = len(l.split()) - 1\n",
    "    if len_inside_brackets == num_words:\n",
    "        words_before_brackets = []\n",
    "        for i in range(len_inside_brackets):\n",
    "            if i<num_words: words_before_brackets.insert(0,l.split(\" \")[-i-2])\n",
    "        string_before_brackets = \" \".join(words_before_brackets)\n",
    "        if all(words_before_brackets[i].lower()[0]==inside_brackets[i].lower() for i in range(len_inside_brackets)):\n",
    "            if string_before_brackets not in term_to_abbreviation_dict.keys() and inside_brackets not in term_to_abbreviation_dict.values():\n",
    "                term_to_abbreviation_dict[string_before_brackets] = inside_brackets\n",
    "        \n",
    "\n",
    "consecutive_caps_before = snomedct['term'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_before = consecutive_caps_before[consecutive_caps_before.astype(str)!='[]']\n",
    "consecutive_caps_before = consecutive_caps_before.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_before:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\", l)[0]\n",
    "    inside_brackets = inside_brackets.strip('()')\n",
    "    words_inside_brackets = inside_brackets.split()\n",
    "    num_words_inside_brackets = len(words_inside_brackets)\n",
    "    word_before_brackets = l.split()[0]\n",
    "    if num_words_inside_brackets==len(word_before_brackets) and all(words_inside_brackets[i].lower()[0]==word_before_brackets[i].lower() for i in range(num_words_inside_brackets)):\n",
    "        if inside_brackets not in term_to_abbreviation_dict.keys() and word_before_brackets not in term_to_abbreviation_dict.values():\n",
    "            term_to_abbreviation_dict[inside_brackets] = word_before_brackets\n",
    "            \n",
    "\n",
    "consecutive_caps_dash = snomedct['term'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\-\\s(?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\")\n",
    "consecutive_caps_dash = consecutive_caps_dash[consecutive_caps_dash.astype(str)!='[]']\n",
    "consecutive_caps_dash = consecutive_caps_dash.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_dash:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    after_dash = l.split(' - ')[1]\n",
    "    words_after_dash = after_dash.split()\n",
    "    num_words_after_dash = len(words_after_dash)\n",
    "    word_before_dash = l.split(' - ')[0]\n",
    "    if num_words_after_dash==len(word_before_dash) and all(words_after_dash[i].lower()[0]==word_before_dash[i].lower() for i in range(num_words_after_dash)):\n",
    "        if after_dash not in term_to_abbreviation_dict.keys():\n",
    "            term_to_abbreviation_dict[after_dash] = word_before_dash\n",
    "\n",
    "\n",
    "\n",
    "consecutive_caps_series_location = df['location'].str.findall(r\"((?:\\b[A-Za-z&]+\\b\\s)+\\([A-Za-z][A-Za-z]+\\))\")\n",
    "consecutive_caps_series_notes = df['inc_notes'].str.findall(r\"((?:\\b[A-Za-z]+\\b\\s)+\\([A-Za-z][A-Za-z]+\\))\")\n",
    "consecutive_caps_series_action = df['inc_actiontaken'].str.findall(r\"((?:\\b[A-Za-z]+\\b\\s)+\\([A-Za-z][A-Za-z]+\\))\")\n",
    "consecutive_caps_after = pd.concat([consecutive_caps_series_location,consecutive_caps_series_notes,consecutive_caps_series_action])\n",
    "consecutive_caps_after = consecutive_caps_after[consecutive_caps_after.astype(str)!='[]']\n",
    "consecutive_caps_after = consecutive_caps_after.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_after:\n",
    "    slist.extend(x)\n",
    "\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\(([A-Za-z]+)\\)\", l)[0]\n",
    "    len_inside_brackets = len(inside_brackets)\n",
    "    num_words = len(l.split()) - 1\n",
    "    if len_inside_brackets == num_words:\n",
    "        words_before_brackets = []\n",
    "        for i in range(len_inside_brackets):\n",
    "            if i<num_words: words_before_brackets.insert(0,l.split()[-i-2])\n",
    "        string_before_brackets = \" \".join(words_before_brackets)\n",
    "        if all(words_before_brackets[i].lower()[0]==inside_brackets[i].lower() for i in range(len_inside_brackets)):\n",
    "            if string_before_brackets not in term_to_abbreviation_dict.keys() and inside_brackets not in term_to_abbreviation_dict.values():\n",
    "                term_to_abbreviation_dict[string_before_brackets] = inside_brackets\n",
    "        \n",
    "\n",
    "consecutive_caps_series_location = df['location'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_series_notes = df['inc_notes'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_series_action = df['inc_actiontaken'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_before = pd.concat([consecutive_caps_series_location,consecutive_caps_series_notes,consecutive_caps_series_action])\n",
    "consecutive_caps_before = consecutive_caps_before[consecutive_caps_before.astype(str)!='[]']\n",
    "consecutive_caps_before = consecutive_caps_before.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_before:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\", l)[0]\n",
    "    inside_brackets = inside_brackets.strip('()')\n",
    "    words_inside_brackets = inside_brackets.split()\n",
    "    num_words_inside_brackets = len(words_inside_brackets)\n",
    "    word_before_brackets = l.split()[0]\n",
    "    if num_words_inside_brackets==len(word_before_brackets) and all(words_inside_brackets[i].lower()[0]==word_before_brackets[i].lower() for i in range(num_words_inside_brackets)):\n",
    "        if inside_brackets not in term_to_abbreviation_dict.keys() and word_before_brackets not in term_to_abbreviation_dict.values():\n",
    "            term_to_abbreviation_dict[inside_brackets] = word_before_brackets\n",
    "            \n",
    "\n",
    "consecutive_caps_series_location = df['location'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\-\\s(?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\")\n",
    "consecutive_caps_series_notes = df['inc_notes'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\-\\s(?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\")\n",
    "consecutive_caps_series_action = df['inc_actiontaken'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\-\\s(?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\")\n",
    "consecutive_caps_dash = pd.concat([consecutive_caps_series_location,consecutive_caps_series_notes,consecutive_caps_series_action])\n",
    "consecutive_caps_dash = consecutive_caps_dash[consecutive_caps_dash.astype(str)!='[]']\n",
    "consecutive_caps_dash = consecutive_caps_dash.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_dash:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    after_dash = l.split('-')[1]\n",
    "    words_after_dash = after_dash.split()\n",
    "    num_words_after_dash = len(words_after_dash)\n",
    "    word_before_dash = l.split(' - ')[0]\n",
    "    if num_words_after_dash==len(word_before_dash) and all(words_after_dash[i].lower()[0]==word_before_dash[i].lower() for i in range(num_words_after_dash)):\n",
    "        after_dash = after_dash.strip()\n",
    "        if after_dash not in term_to_abbreviation_dict.keys() and word_before_dash not in term_to_abbreviation_dict.values():\n",
    "            term_to_abbreviation_dict[after_dash] = word_before_dash\n",
    "            \n",
    "for key,val in dict(term_to_abbreviation_dict).items():\n",
    "    if val=='OD': del term_to_abbreviation_dict[key]\n",
    "    elif val=='PIVOTAL': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower().startswith('pri'): del term_to_abbreviation_dict[key]\n",
    "    elif val=='fresh': del term_to_abbreviation_dict[key]\n",
    "    elif val=='West': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower()=='oxynorm': del term_to_abbreviation_dict[key]\n",
    "    elif val=='methylprednisolone': del term_to_abbreviation_dict[key]\n",
    "    elif val=='cetraben': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Levemir': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Desmopressin': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower()=='oramorph': del term_to_abbreviation_dict[key]\n",
    "    elif val=='insulatard': del term_to_abbreviation_dict[key]\n",
    "    elif val=='missing': del term_to_abbreviation_dict[key]\n",
    "    elif val=='insulatard': del term_to_abbreviation_dict[key]\n",
    "    elif val=='SS': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Tramadol': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower()=='eprex': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Tuesday': del term_to_abbreviation_dict[key]\n",
    "    elif val=='cloudy': del term_to_abbreviation_dict[key]\n",
    "    elif val=='stable': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Solent': del term_to_abbreviation_dict[key]\n",
    "    elif val=='carer': del term_to_abbreviation_dict[key]\n",
    "term_to_abbreviation_dict['Intravenous Antibiotics'] = 'IV'\n",
    "term_to_abbreviation_dict['Intravenous'] = 'IV'\n",
    "term_to_abbreviation_dict['Morphine sulphate MR'] = 'MS'\n",
    "term_to_abbreviation_dict['Morphine sulphate'] = 'MS'\n",
    "term_to_abbreviation_dict['milligram'] = 'mg'\n",
    "term_to_abbreviation_dict['Department of Critical Care'] = 'DCCQ'\n",
    "term_to_abbreviation_dict['mau'] = 'amu'\n",
    "term_to_abbreviation_dict['controlled drug'] = 'cd'\n",
    "term_to_abbreviation_dict['patient(.{1,3})own drug'] = \"pod\"\n",
    "term_to_abbreviation_dict['twice a day'] = \"bd\"\n",
    "term_to_abbreviation_dict['twice daily'] = \"bd\"\n",
    "term_to_abbreviation_dict['to take out'] = \"tto\"\n",
    "\n",
    "#term_to_abbreviation_dict = {key.lower():val.lower() for (key,val) in term_to_abbreviation_dict.items()}\n",
    "for key,val in term_to_abbreviation_dict.items():\n",
    "    print(key+\" & \"+val+'\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = [v.lower() for v in term_to_abbreviation_dict.values()]\n",
    "abbreviation_counts = Counter(abbreviations)\n",
    "print(abbreviation_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter()\n",
    "english_WORDS = Counter(words(open('big.txt').read()))\n",
    "dict.update(WORDS,english_WORDS)\n",
    "dict.update(WORDS,medical_terms_counts)\n",
    "dict.update(WORDS,abbreviation_counts)\n",
    "floors = ['A','B','C','D','E','F','G']\n",
    "for floor in floors:\n",
    "    for i in range(9):\n",
    "        WORDS[floor+str(i+1)] = 1\n",
    "WORDS['moprs'] = 1\n",
    "WORDS['locker'] = 1\n",
    "WORDS['nomad'] = 1\n",
    "WORDS['gik'] = 1\n",
    "print(WORDS)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_correction_frequent_words = ['dose', 'given', 'chart', 'drug', 'prescribed', 'cd', 'cupboard', 'book', 'missing', 'tablet', 'tto', 'discharge', 'ward', 'home', 'pharmacist', 'iv', 'insulin', 'round', 'written', 'missed', 'signed', 'infusion', 'unit', 'nurse', 'fluid', 'warfarin', 'paracetamol', 'blood', 'humulin', 'glucose', 'morning', 'prescription', 'bm', 'bottle', 'bag', 'box', 'pod', 'locker', 'dispensed', 'evening', 'antibiotic', 'day', 'administered', 'bd', 'labelled', 'gentamicin', 'inr', 'clinic', 'referral', 'staff', 'medication', 'locked', 'tpn', 'fridge', 'room', 'belonging', 'bed']\n",
    "pre_correction_frequent_words = ['dose', 'given', 'chart', 'drug', 'prescribed', 'cd', 'cupboard', 'book', 'missing', 'tablet', 'tto', 'discharge', 'ward', 'home', 'pharmacist', 'iv', 'insulin', 'round', 'written', 'missed', 'signed', 'infusion', 'unit', 'nurse', 'fluid', 'warfarin', 'paracetamol', 'blood', 'humulin', 'glucose', 'morning', 'prescription', 'bm', 'bottle', 'bag', 'box', 'pod', 'locker', 'dispensed', 'evening', 'antibiotic', 'day', 'administered', 'bd', 'gentamicin', 'inr', 'clinic', 'referral', 'staff', 'labelled', 'medication', 'locked', 'hour', 'night', 'belonging']\n",
    "post_corrected_frequent_words = ['dose', 'given', 'chart', 'drug', 'prescribed', 'cd', 'cupboard', 'book', 'missing', 'tablet', 'tto', 'discharge', 'ward', 'home', 'pharmacist', 'iv', 'insulin', 'round', 'written', 'missed', 'signed', 'infusion', 'unit', 'nurse', 'fluid', 'warfarin', 'paracetamol', 'blood', 'humulin', 'glucose', 'morning', 'prescription', 'bag', 'bm', 'bottle', 'box', 'pod', 'locker', 'dispensed', 'evening', 'hour', 'antibiotic', 'day', 'administered', 'bd', 'gentamicin', 'inr', 'clinic', 'referral', 'staff', 'labelled', 'medication', 'locked', 'night', 'belonging']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"patient\",\"patients\",\"pt\",\"pharmacy\",\"medicine\",\"kd\",\"mso\",\"event\",\"reported\",\"recoded\",\"coding\",\"did\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def american_to_british(tokens):\n",
    "    for t in tokens:\n",
    "        t = re.sub(r\"(...)or$\", r\"\\1our\", t)\n",
    "        t = re.sub(r\"([bt])er$\", r\"\\1re\", t)\n",
    "        t = re.sub(r\"([iy])z(e[drs]|e$|ing|ation)\", r\"\\1s\\2\", t)\n",
    "        t = re.sub(r\"^(s.?[iy])s(e[drs]|e$|ing|ation)\", r\"\\1z\\2\", t) # convert back words starting with s like size, seize\n",
    "        t = re.sub(r\"og$\", \"ogue\", t)\n",
    "        yield t\n",
    "        \n",
    "class CustomVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(american_to_british(tokenize(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward_num_series = df['location'].str.findall(r\"[A-G][0-9]\\s.+\")\n",
    "ward_num_series = ward_num_series[ward_num_series.map(lambda d: len(d)) > 0]\n",
    "slist = []\n",
    "for x in ward_num_series:\n",
    "    slist.extend(x)\n",
    "ward_name_to_num_dict = {}\n",
    "for l in slist:\n",
    "    l_split = l.split()\n",
    "    name = \" \".join(l_split[1:])\n",
    "    name = name.strip(\"- \")\n",
    "    ward_name_to_num_dict[name.lower()] = l_split[0].lower()\n",
    "ward_name_to_num_dict['dccq'] = 'e5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_correction_dict = {' dos ':' dose ', ' doses ':' dose ', ' ttos ':' tto ', ' cds ':' cd ', ' discharged ':' discharge ', 'non clinical':''}\n",
    "corrected_lemma_dict = {'stat':'stated','errour':'error','doctour':'doctor','floour':'floor'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['inc_notes'] = df['inc_notes'].str.lower()\n",
    "df['inc_notes'] = df['inc_notes'].replace(term_to_abbreviation_dict, regex=True)\n",
    "df['inc_notes'] = df['inc_notes'].replace(pre_correction_dict, regex=True)\n",
    "df['inc_notes'] = df['inc_notes'].replace(ward_name_to_num_dict, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.porter_stemmer = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        \n",
    "        tokens = [t for t in word_tokenize(doc) if t.isalpha()]\n",
    "        no_stops = [t for t in tokens if t not in my_stop_words]\n",
    "        lemmatized = [self.wnl.lemmatize(t) for t in no_stops]\n",
    "        corrected = [correction(t) for t in lemmatized]\n",
    "        return [t for t in corrected if len(t)>1]#corrected_lemma if len(t)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer: tfidf\n",
    "tfidf = CustomVectorizer(tokenizer=LemmaTokenizer())\n",
    "# add argument ngram_range=(1,2) to get word pairs like \"drug chart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fit_transform to document: csr_mat\n",
    "csr_mat = tfidf.fit_transform(df['inc_notes'])\n",
    "\n",
    "# Print result of toarray() method\n",
    "#print(csr_mat.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the words: words\n",
    "words = tfidf.get_feature_names()\n",
    "words = [corrected_lemma_dict.get(t,t) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print words \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs_train = []\n",
    "ks = list(range(1,7))\n",
    "#ks = list(range(1,23))\n",
    "# score decreases for k=23 with 3491 training events\n",
    "for k in ks:\n",
    "    # Create an NMF instance: model\n",
    "    model = NMF(n_components=k)\n",
    "\n",
    "    # Fit the model to articles\n",
    "    model.fit(csr_mat)\n",
    "\n",
    "    # Transform the articles: nmf_features\n",
    "    nmf_features = model.transform(csr_mat)\n",
    "\n",
    "    # Print the NMF features\n",
    "    #print(nmf_features)\n",
    "\n",
    "    # Create a pandas DataFrame: df\n",
    "    df_nmf = pd.DataFrame(nmf_features,index=df['inc_notes'])\n",
    "\n",
    "    # Create a DataFrame: components_df\n",
    "    components_df = pd.DataFrame(model.components_,columns=words)\n",
    "\n",
    "    # Print the shape of the DataFrame\n",
    "    print(components_df.shape)\n",
    "    \n",
    "    #perfs_train.append(get_score(model, csr_mat.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmf.columns = df_nmf.columns.astype(str)\n",
    "df_nmf['max_feature'] = df_nmf.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_feature'] = df_nmf['max_feature'].values\n",
    "theme_counts = df['max_feature'].value_counts().sort_index().values\n",
    "theme_counts_max = theme_counts.max()\n",
    "print(theme_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud_words = []\n",
    "def print_wordclouds_and_pies():\n",
    "    donut_seg = 0\n",
    "    for i,counts in theme_counts_series.items():\n",
    "        # Initialize the word cloud\n",
    "        width = 1024 #int(1024*counts/theme_counts_max)\n",
    "        height = 720 #int(720*counts/theme_counts_max))\n",
    "        wc = WordCloud(\n",
    "            background_color=\"white\",\n",
    "            width = width,\n",
    "            height = height\n",
    "        )\n",
    "\n",
    "        # Select row : component\n",
    "        component = components_df.iloc[i]\n",
    "\n",
    "        # Generate the cloud\n",
    "        component.nlargest().index = component.nlargest().index.map(str)\n",
    "        wc.generate_from_frequencies(component.nlargest())\n",
    "        wordcloud_words.append(component.nlargest().index)\n",
    "\n",
    "        # Display the generated image:\n",
    "        figure, (wc_fig, counts_fig) = plt.subplots(nrows=1,ncols=2, figsize=(width/50,height/100))\n",
    "        wc_fig.imshow(wc, interpolation='bilinear')\n",
    "        wc_fig.axis(\"off\");\n",
    "\n",
    "        counts_fig.axis('equal')\n",
    "        colors = ['w' for j in theme_counts_series.index]\n",
    "        colors[donut_seg] = 'b'\n",
    "        labels = ['' for val in theme_counts_series.values]\n",
    "        labels[donut_seg] = str(counts)+\"/\"+str(theme_counts_series.values.sum())\n",
    "        donut_seg += 1\n",
    "        mypie, texts = counts_fig.pie(theme_counts_series.values/theme_counts_max, colors=colors, labels=labels, startangle=90, counterclock=False)\n",
    "        for text in texts: text.set_fontsize(20)\n",
    "        plt.setp( mypie, width=0.4, edgecolor='black')\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(str(k)+'_'+str(i)+'_wordcloud_donut.png')\n",
    "        #plt.savefig(str(k)+'_'+str(i)+'_wordcloud_donut.pdf')\n",
    "        plt.show()\n",
    "\n",
    "        print('-------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ks = list(range(3,13))\n",
    "for k in ks:\n",
    "    # Create an NMF instance: model\n",
    "    model = NMF(n_components=k)\n",
    "\n",
    "    # Fit the model to articles\n",
    "    model.fit(csr_mat)\n",
    "\n",
    "    # Transform the articles: nmf_features\n",
    "    nmf_features = model.transform(csr_mat)\n",
    "\n",
    "    # Create a pandas DataFrame: df\n",
    "    df_nmf = pd.DataFrame(nmf_features,index=df['inc_notes'])\n",
    "\n",
    "    # Create a DataFrame: components_df\n",
    "    components_df = pd.DataFrame(model.components_,columns=words)\n",
    "    \n",
    "    df_nmf.columns = df_nmf.columns.astype(str)\n",
    "    #print(list(df_nmf.nlargest(10,'0').index))\n",
    "    #print(list(df_nmf.nlargest(10,'1').index))\n",
    "    #print(list(df_nmf.nlargest(10,'2').index))\n",
    "    df_nmf['max_feature'] = df_nmf.idxmax(axis=1)\n",
    "    \n",
    "    df['max_feature'] = df_nmf['max_feature'].values\n",
    "    theme_counts_series = df['max_feature'].value_counts()\n",
    "    theme_counts_series.index = theme_counts_series.index.astype(int)\n",
    "    theme_counts_max = theme_counts_series.values.max()\n",
    "    \n",
    "    print(\"number themes: \"+str(k))\n",
    "    print_wordclouds_and_pies()\n",
    "    print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_wordcloud_words = []\n",
    "for top5 in wordcloud_words:\n",
    "    for i in range(5):\n",
    "        if top5[i] not in unique_wordcloud_words: unique_wordcloud_words.append(top5[i])\n",
    "print(unique_wordcloud_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an NMF instance: model\n",
    "model = NMF(n_components=3)\n",
    "\n",
    "# Fit the model to articles\n",
    "model.fit(csr_mat)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = model.transform(csr_mat)\n",
    "\n",
    "# Create a pandas DataFrame: df\n",
    "df_nmf = pd.DataFrame(nmf_features,index=df['inc_notes'])\n",
    "\n",
    "# Create a DataFrame: components_df\n",
    "components_df = pd.DataFrame(model.components_,columns=words)\n",
    "    \n",
    "df_nmf.columns = df_nmf.columns.astype(str)\n",
    "df_nmf['max_feature'] = df_nmf.idxmax(axis=1)\n",
    "pd.options.display.max_colwidth = 80\n",
    "print(df_nmf.head(n=11).to_latex(columns=['0','1','2']))\n",
    "    \n",
    "df['max_feature'] = df_nmf['max_feature'].values\n",
    "theme_counts_series = df['max_feature'].value_counts()\n",
    "theme_counts_series.index = theme_counts_series.index.astype(int)\n",
    "theme_counts_max = theme_counts_series.values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tto_notes = list(df_nmf.nlargest(6,'1').index)\n",
    "for i,note in enumerate(tto_notes):\n",
    "    sentences = sent_tokenizer.tokenize(note)\n",
    "    sentences = [sent.capitalize() for sent in sentences]\n",
    "    sentences = [sent.replace('gp','GP') for sent in sentences]\n",
    "    sentences = [sent.replace('. d','. D') for sent in sentences]\n",
    "    sentences = [sent.replace('discharge','\\\\textcolor[RGB]{60,80,139}{discharge}') for sent in sentences]\n",
    "    sentences = [sent.replace('Discharge','\\\\textcolor[RGB]{60,80,139}{Discharge}') for sent in sentences]\n",
    "    sentences = [sent.replace('home','\\\\textcolor[RGB]{68,59,132}{home}') for sent in sentences]\n",
    "    sentences = [sent.replace('Tto','tto') for sent in sentences]\n",
    "    sentences = [sent.replace('tto','\\\\textcolor[RGB]{72,40,120}{tto}') for sent in sentences]\n",
    "    sentences = [sent.replace('ward','\\\\textcolor[RGB]{44,115,142}{ward}') for sent in sentences]\n",
    "    sentences = [sent.replace('medication','\\\\textcolor[RGB]{61,78,138}{medication}') for sent in sentences]\n",
    "    sentences = [sent.replace('Medication','\\\\textcolor[RGB]{61,78,138}{Medication}') for sent in sentences]\n",
    "    print(str(i+1)+' & ')\n",
    "    for sent in sentences:\n",
    "        print(sent+' ')\n",
    "    print('\\\\\\\\')\n",
    "    #print(note+'\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['inc_time'].str[:2]\n",
    "df = df.astype({'hour':'int'})\n",
    "df['reported_hour'] = df['inc_submittedtime'].str[:2]\n",
    "df = df.astype({'reported_hour':'int'})\n",
    "df['weekday'] = df['inc_dincident'].dt.weekday\n",
    "df_Apr16_Nov19 = df[df.inc_dincident > '2016-03-31']\n",
    "df_Apr16_Nov19 = df[df.inc_dincident < '2019-12-01']\n",
    "df['month'] = df_Apr16_Nov19['inc_dincident'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_days = (max(df.inc_dincident)-min(df.inc_dincident)).days\n",
    "total_weeks = total_days/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_occurences = np.array([3,3,3,4,4,4,4,4,4,4,4,3])\n",
    "days_in_month = np.ones(12)*31\n",
    "days_in_month[1] = 28\n",
    "days_in_month[3] = 30\n",
    "days_in_month[5] = 30\n",
    "days_in_month[8] = 30\n",
    "days_in_month[10] = 30\n",
    "division_factor = month_occurences*days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_labels = ['Apr 16', 'May 16', 'Jun 16', 'Jul 16', 'Aug 16', 'Sep 16', 'Oct 16', 'Nov 16', 'Dec 16', 'Jan 17', 'Feb 17', 'Mar 17', 'Apr 17', 'May 17', 'Jun 17', 'Jul 17', 'Aug 17', 'Sep 17', 'Oct 17', 'Nov 17', 'Dec 17', 'Jan 18', 'Feb 18', 'Mar 18', 'Apr 18', 'May 18', 'Jun 18', 'Jul 18', 'Aug 18', 'Sep 18', 'Oct 18', 'Nov 18', 'Dec 18', 'Jan 19', 'Feb 19', 'Mar 19', 'Apr 19', 'May 19', 'Jun 19', 'Jul 19', 'Aug 19', 'Sep 19', 'Oct 19', 'Nov 19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_month_year = np.ones(len(months_labels))*31\n",
    "days_in_month_year[0] = 30\n",
    "days_in_month_year[2] = 30\n",
    "days_in_month_year[5] = 30\n",
    "days_in_month_year[7] = 30\n",
    "days_in_month_year[10] = 28\n",
    "days_in_month_year[12] = 30\n",
    "days_in_month_year[14] = 30\n",
    "days_in_month_year[17] = 30\n",
    "days_in_month_year[19] = 30\n",
    "days_in_month_year[22] = 28\n",
    "days_in_month_year[24] = 30\n",
    "days_in_month_year[26] = 30\n",
    "days_in_month_year[29] = 30\n",
    "days_in_month_year[31] = 30\n",
    "days_in_month_year[34] = 28\n",
    "days_in_month_year[36] = 30\n",
    "days_in_month_year[38] = 30\n",
    "days_in_month_year[41] = 30\n",
    "days_in_month_year[43] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportedby_plot():\n",
    "    reportedby_counts = df_max_feature['inc_reportedby'].value_counts()\n",
    "    reportedby_counts = reportedby_counts.drop('',axis=0)\n",
    "    thresh = reportedby_counts[reportedby_counts.index == 'Other'].values[0] + 1\n",
    "    reportedby_counts = reportedby_counts.append(pd.Series(reportedby_counts[reportedby_counts.values < thresh].sum(),index=['Other']))\n",
    "    reportedby_counts = reportedby_counts[reportedby_counts.values >= thresh]\n",
    "    plt.figure(figsize=(17,13))\n",
    "    cm = plt.get_cmap('RdYlGn')\n",
    "    cm_subsection = np.linspace(0, 1, len(reportedby_counts.values))\n",
    "    x_range = range(0,len(reportedby_counts.values))\n",
    "    colors = [cm(x) for x in cm_subsection]\n",
    "    reportedby_list = list(reportedby_counts.index)\n",
    "    for i in x_range:\n",
    "        plt.barh(reportedby_list[-(i+1)],reportedby_counts.values[-(i+1)],color=colors[-(i+1)])\n",
    "        plt.text(reportedby_counts.values[-(i+1)],i,reportedby_list[-(i+1)])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"number of medicine patient safety events reported by different staff roles\");\n",
    "    img = plt.imread(\"nhsportsmouth.png\");\n",
    "    ax = plt.axes([0.4,0, 0.5, 0.5], frameon=True)  # Change the numbers in this array to position your image [left, bottom, width, height])\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')  # get rid of the ticks and ticklabels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporter_email():\n",
    "    email_counts = df_max_feature['inc_rep_email'].value_counts(ascending=True)\n",
    "    people_per_counts = email_counts.value_counts()[email_counts.unique()]\n",
    "    cumulative_num_people = []\n",
    "    cumulative_percentages = []\n",
    "    num_people = people_per_counts.values.sum()\n",
    "    percentage = 100\n",
    "    for i in range(len(people_per_counts)):\n",
    "        cumulative_num_people.append(str(num_people))\n",
    "        num_people -= people_per_counts.values[i]\n",
    "        cumulative_percentages.append(percentage)\n",
    "        percentage -= 100*people_per_counts.index[i]*people_per_counts.values[i]/email_counts.values.sum()\n",
    "    red_percentages = [100-percentage for percentage in cumulative_percentages]\n",
    "\n",
    "    plt.figure(figsize=(14,14))\n",
    "    plt.barh(cumulative_num_people,cumulative_percentages,color='g')\n",
    "    plt.barh(cumulative_num_people,red_percentages,left=cumulative_percentages,color='r')\n",
    "    plt.ylabel('number of people reporting the events')\n",
    "    for i, percentage in enumerate(cumulative_percentages):\n",
    "        plt.text(percentage,i-0.2,\"{0:.1f}% \".format(percentage))\n",
    "        if int(cumulative_num_people[i])!=1 and i>2: \n",
    "            plt.text(69,i-0.2,\"the same \"+cumulative_num_people[i]+\" people reported \"+\"{0:.1f}% \".format(percentage)+\" of events\")\n",
    "    plt.text(69,len(cumulative_percentages)-1-0.2,\"the same 1 person reported \"+\"{0:.1f}% \".format(percentage)+\" of events\");\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staff_involved_role():\n",
    "\n",
    "    reportedby_counts = df_max_feature['employee_involved'].value_counts().dropna()\n",
    "    plt.figure(figsize=(17,13))\n",
    "    cm = plt.get_cmap('RdYlGn')\n",
    "    cm_subsection = np.linspace(0, 1, len(reportedby_counts.values))\n",
    "    x_range = range(0,len(reportedby_counts.values))\n",
    "    colors = [cm(x) for x in cm_subsection]\n",
    "    reportedby_list = list(reportedby_counts.index)\n",
    "    for i in x_range:\n",
    "        plt.barh(reportedby_list[-(i+1)],reportedby_counts.values[-(i+1)],color=colors[-(i+1)])\n",
    "        plt.text(reportedby_counts.values[-(i+1)],i,reportedby_list[-(i+1)])\n",
    "    plt.yticks([])\n",
    "    plt.xlim([0,max(reportedby_counts)+len(reportedby_list[0])])\n",
    "    plt.title(\"employees involved in medicine patient safety events\");\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_trend_plot():\n",
    "    df_month_year = df_max_feature[df_max_feature.inc_dincident > '2016-03-31']\n",
    "    df_month_year = df_month_year[df_month_year.inc_dincident < '2019-12-01']\n",
    "    month_year = df_month_year.groupby([df_month_year.inc_dincident.dt.year, df_month_year.inc_dincident.dt.month]).agg('count')\n",
    "    num_per_day = month_year.inc_dincident.values/days_in_month_year\n",
    "    num_per_day_err = np.sqrt(month_year.inc_dincident.values)/days_in_month_year\n",
    "    \n",
    "    data_x = np.array([i for i in range(len(months_labels))])\n",
    "    data_y = num_per_day\n",
    "\n",
    "    # data fit\n",
    "    polynomial_mod = PolynomialModel(1)\n",
    "    pars = polynomial_mod.guess(data_y, x=data_x, c0=data_y[0], c1=0)\n",
    "    model = polynomial_mod\n",
    "    out = model.fit(data_y, pars, x=data_x)\n",
    "\n",
    "    season_labels = ['Spring' for i in range(len(months_labels))]\n",
    "    for i in range(len(season_labels)):\n",
    "        if 'Ju' in months_labels[i]: season_labels[i]='Summer'\n",
    "        elif 'Au' in months_labels[i]: season_labels[i]='Summer'\n",
    "        elif 'Se' in months_labels[i]: season_labels[i]='Autumn'\n",
    "        elif 'Oc' in months_labels[i]: season_labels[i]='Autumn'\n",
    "        elif 'No' in months_labels[i]: season_labels[i]='Autumn'\n",
    "        elif 'De' in months_labels[i]: season_labels[i]='Winter'\n",
    "        elif 'Ja' in months_labels[i]: season_labels[i]='Winter'\n",
    "        elif 'Fe' in months_labels[i]: season_labels[i]='Winter'\n",
    "    colors = ['red','green','blue','purple']\n",
    "    season_codes = []\n",
    "    for season in season_labels:\n",
    "        if season == 'Spring': season_codes.append(0)\n",
    "        elif season == 'Summer': season_codes.append(1)\n",
    "        elif season == 'Autumn': season_codes.append(2)\n",
    "        else: season_codes.append(3)\n",
    "    season_colours = []\n",
    "    for season in season_labels:\n",
    "        if season == 'Spring': season_colours.append('red')\n",
    "        elif season == 'Summer': season_colours.append('green')\n",
    "        elif season == 'Autumn': season_colours.append('blue')\n",
    "        else: season_colours.append('purple')\n",
    "            \n",
    "    num_per_spring_day = np.ones(len(num_per_day))*100\n",
    "    num_per_summer_day = np.ones(len(num_per_day))*100\n",
    "    num_per_autumn_day = np.ones(len(num_per_day))*100\n",
    "    num_per_winter_day = np.ones(len(num_per_day))*100\n",
    "    for idx,season in enumerate(season_labels):\n",
    "        if season == 'Spring': num_per_spring_day[idx] = num_per_day[idx]\n",
    "        elif season == 'Summer': num_per_summer_day[idx] = num_per_day[idx]\n",
    "        elif season == 'Autumn': num_per_autumn_day[idx] = num_per_day[idx]\n",
    "        else: num_per_winter_day[idx] = num_per_day[idx]\n",
    "        \n",
    "    upper_spring = [x+e for x,e in zip(num_per_spring_day, num_per_day_err) ]\n",
    "    lower_spring = [x-e for x,e in zip(num_per_spring_day, num_per_day_err) ]\n",
    "    upper_summer = [x+e for x,e in zip(num_per_summer_day, num_per_day_err) ]\n",
    "    lower_summer = [x-e for x,e in zip(num_per_summer_day, num_per_day_err) ]\n",
    "    upper_autumn = [x+e for x,e in zip(num_per_autumn_day, num_per_day_err) ]\n",
    "    lower_autumn = [x-e for x,e in zip(num_per_autumn_day, num_per_day_err) ]\n",
    "    upper_winter = [x+e for x,e in zip(num_per_winter_day, num_per_day_err) ]\n",
    "    lower_winter = [x-e for x,e in zip(num_per_winter_day, num_per_day_err) ]\n",
    "\n",
    "    spring_source = ColumnDataSource(data=dict(groups=months_labels, counts=num_per_spring_day, upper=upper_spring, lower=lower_spring))\n",
    "    summer_source = ColumnDataSource(data=dict(groups=months_labels, counts=num_per_summer_day, upper=upper_summer, lower=lower_summer))\n",
    "    autumn_source = ColumnDataSource(data=dict(groups=months_labels, counts=num_per_autumn_day, upper=upper_autumn, lower=lower_autumn))\n",
    "    winter_source = ColumnDataSource(data=dict(groups=months_labels, counts=num_per_winter_day, upper=upper_winter, lower=lower_winter))\n",
    "\n",
    "    y_max = math.ceil(max(num_per_day) / 2.) * 2\n",
    "    p = bokeh_figure(x_range=months_labels, title=\"Daily number of medicine patient safety events at QA\", y_range=(0,y_max),\n",
    "              x_axis_label='Month Year',y_axis_label='events per day',plot_width=900, plot_height=450,\n",
    "               tools='pan,box_select,box_zoom,wheel_zoom,save,reset,help')\n",
    "\n",
    "    p.circle(months_labels,num_per_spring_day,color='green',size=10,alpha=0.5,nonselection_alpha=0, \n",
    "             hover_fill_color='black')\n",
    "    p.circle(months_labels,num_per_summer_day,color='red',size=10,alpha=0.5,nonselection_alpha=0, hover_fill_color='black')\n",
    "    p.circle(months_labels,num_per_autumn_day,color='yellow',size=10,alpha=0.5,nonselection_alpha=0, hover_fill_color='black')\n",
    "    p.circle(months_labels,num_per_winter_day,color='blue',size=10,alpha=0.5,nonselection_alpha=0, hover_fill_color='black')\n",
    "\n",
    "    p.add_layout(\n",
    "        Whisker(source=spring_source, base=\"groups\", upper=\"upper\", lower=\"lower\", level=\"overlay\", line_color='green')\n",
    "    )\n",
    "    p.add_layout(\n",
    "        Whisker(source=summer_source, base=\"groups\", upper=\"upper\", lower=\"lower\", level=\"overlay\", line_color='red')\n",
    "    )\n",
    "    p.add_layout(\n",
    "        Whisker(source=autumn_source, base=\"groups\", upper=\"upper\", lower=\"lower\", level=\"overlay\", line_color='yellow')\n",
    "    )\n",
    "    p.add_layout(\n",
    "        Whisker(source=winter_source, base=\"groups\", upper=\"upper\", lower=\"lower\", level=\"overlay\", line_color='blue')\n",
    "    )\n",
    "\n",
    "    r_linear = p.line(months_labels,out.best_fit,line_color='purple')\n",
    "\n",
    "    legend = Legend(items=[\n",
    "        LegendItem(label=\"spring\", renderers=[p.circle(0,0,color='green')]),\n",
    "        LegendItem(label=\"summer\", renderers=[p.circle(0,0,color='red')]),\n",
    "        LegendItem(label=\"autumn\", renderers=[p.circle(0,0,color='yellow')]),\n",
    "        LegendItem(label=\"winter\", renderers=[p.circle(0,0,color='blue')]),\n",
    "        LegendItem(label='fit χ²/Nᵈᶠ = '+str(round(out.redchi,1)), renderers=[r_linear]),\n",
    "    ])\n",
    "    p.add_layout(legend)\n",
    "\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.xaxis.major_label_orientation = math.pi/2\n",
    "    p.legend\n",
    "    #p.legend.orientation = \"horizontal\"\n",
    "    p.legend.location = \"bottom_right\"\n",
    "\n",
    "    # Create a HoverTool: hover\n",
    "    hover = HoverTool(tooltips=None,mode='hline')\n",
    "\n",
    "    # Add the hover tool to the figure p\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    height = math.ceil(max(num_per_day) / 2.) * 2/6\n",
    "    width = 13.314516129032258\n",
    "    p.image_url(url=['nhsportsmouth.png'], x=0.2, y=y_max-0.1, w=width, h=height)\n",
    "\n",
    "    output_notebook()\n",
    "    \n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_plot():\n",
    "    # Compute pie slices\n",
    "    N = 24\n",
    "    bins = [i-0.5 for i in range(25)]\n",
    "    data_x = [i for i in range(24)]\n",
    "    data_y_tot,_ = np.histogram(df_max_feature['hour'].values, bins=bins)\n",
    "    data_y = data_y_tot/total_days\n",
    "    data_y_err = np.sqrt(data_y_tot)/total_days\n",
    "    width = 2 * np.pi/N\n",
    "    theta = np.linspace(0.0 + width, 2 * np.pi + width, N, endpoint=False)\n",
    "    theta_deg = np.linspace(0.0 + 7.5, 360 + 7.5, N, endpoint=False)\n",
    "    radii = data_y\n",
    "    radii_err = data_y_err\n",
    "    morning_radii = np.zeros(len(radii))\n",
    "    afternoon_radii = np.zeros(len(radii))\n",
    "    evening_radii = np.zeros(len(radii))\n",
    "    for i in range(24):\n",
    "        if i <= 7: morning_radii[i] = radii[i]\n",
    "        elif i >= 8 and i <= 15: afternoon_radii[i] = radii[i]\n",
    "        else: evening_radii[i] = radii[i]\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111, projection='polar');\n",
    "    ax.bar(theta, morning_radii, width=width,label='night');\n",
    "    ax.bar(theta, afternoon_radii, width=width,label='daytime');\n",
    "    ax.bar(theta, evening_radii, width=width,label='evening');\n",
    "    ax.bar(theta, 2*radii_err, bottom=radii-radii_err,width=width,alpha=0.5,color='none',hatch=\"////\",label='Uncertainty');\n",
    "    ax.set_theta_offset(np.pi/2 + width/2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    yticklabels = ax.get_yticks()\n",
    "    yticklabels = [round(item,1) for item in yticklabels]\n",
    "    for i,item in enumerate(yticklabels):\n",
    "        if i%2==0: yticklabels[i]=''\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    plt.xlabel('hour occured')\n",
    "    plt.title('Hourly number of patient medicine safety events',y=1.1)\n",
    "    plt.legend(loc=(1.04,0))\n",
    "    lines, labels = plt.thetagrids( theta_deg, (data_x) );\n",
    "    ax.axvspan(theta[np.argmax(radii)]-width/2,theta[np.argmax(radii)]+width/2,color='red',alpha=0.3);\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.bar(theta, radii, width=width,label='Data');\n",
    "    ax.bar(theta, radii_err, bottom=radii-radii_err/2,width=width,alpha=0.5,color='none',hatch=\"////\",label='Uncertainty');\n",
    "    ax.set_theta_offset(np.pi/2 + width/2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    yticklabels = ax.get_yticks()\n",
    "    yticklabels = [round(item,1) for item in yticklabels]\n",
    "    for i,item in enumerate(yticklabels):\n",
    "        if i%2==0: yticklabels[i]=''\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    plt.title('hour occured',y=1.1)\n",
    "    lines, labels = plt.thetagrids( theta_deg, (data_x) )\n",
    "    ax.axvline(theta[7]-width/2,color='orange');\n",
    "    ax.axvline(theta[15]-width/2,color='orange');\n",
    "    ax.axvline(theta[12],color='green');\n",
    "    ax.axvline(theta[20],color='green');\n",
    "    ax.axvline(theta[20]-width/2,color='red');\n",
    "    ax.axvline(theta[8]-width/2,color='red');\n",
    "    ax.annotate('Early shift start',xy=(theta[7]-width/2,ax.get_ylim()[1]),xytext=(theta[7]-width/2,ax.get_ylim()[1]*11/7),color='orange',ha='center',\n",
    "           arrowprops=dict(facecolor='orange',alpha=0.1));\n",
    "    ax.annotate('Early shift end',xy=(theta[15]-width/2,ax.get_ylim()[1]),xytext=(theta[15]-width/2,ax.get_ylim()[1]*9/7),color='orange',ha='right',\n",
    "               arrowprops=dict(facecolor='orange',alpha=0.1));\n",
    "    ax.annotate('Late shift start',xy=(theta[12],ax.get_ylim()[1]),xytext=(theta[12],ax.get_ylim()[1]*9/7),color='green',ha='center',\n",
    "               arrowprops=dict(facecolor='green',alpha=0.1));\n",
    "    ax.annotate('Late shift end',xy=(theta[20],ax.get_ylim()[1]),xytext=(theta[20],ax.get_ylim()[1]*9/7),color='green',ha='right',\n",
    "               arrowprops=dict(facecolor='green',alpha=0.1));\n",
    "    ax.annotate('Night shift start',xy=(theta[20]-width/2,ax.get_ylim()[1]),xytext=(theta[20]-width/2,ax.get_ylim()[1]*9/7),color='red',ha='right',\n",
    "               arrowprops=dict(facecolor='red',alpha=0.1));\n",
    "    ax.annotate('Night shift end',xy=(theta[8]-width/2,ax.get_ylim()[1]),xytext=(theta[8]-width/2,ax.get_ylim()[1]*9/7),color='red',ha='left',\n",
    "           arrowprops=dict(facecolor='red',alpha=0.1));\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    reportedby_counts = df_max_feature['inc_reportedby'].value_counts().drop(labels=[''])\n",
    "    reportedby_counts_nlargest = reportedby_counts.nlargest(9)\n",
    "    thresh = reportedby_counts_nlargest[-1]\n",
    "    reportedby_counts = reportedby_counts.append(pd.Series(reportedby_counts[reportedby_counts.values < thresh].sum(),index=['Other']))\n",
    "    reportedby_counts = reportedby_counts[reportedby_counts.values >= thresh]\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    cumulative_bottom = 0\n",
    "    for i,counts in reportedby_counts.items():\n",
    "        radii_reported_tot,_ = np.histogram(df_max_feature[df_max_feature['inc_reportedby']==i].hour.values, bins=bins)\n",
    "        radii_reported = radii_reported_tot/total_days\n",
    "        ax.bar(theta, radii_reported, bottom=cumulative_bottom, width=width, label=i);\n",
    "        cumulative_bottom += radii_reported\n",
    "    ax.bar(theta, radii_err, bottom=cumulative_bottom-radii_err/2,width=width,alpha=0.5,color='none',hatch=\"////\",label='Total Uncertainty');\n",
    "    ax.set_theta_offset(np.pi/2 + width/2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    yticklabels = ax.get_yticks()\n",
    "    yticklabels = [round(item,1) for item in yticklabels]\n",
    "    for i,item in enumerate(yticklabels):\n",
    "        if i%2==0: yticklabels[i]=''\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    plt.xlabel('hour occured')\n",
    "    plt.title('Hourly number of patient medicine safety events at QA',y=1.1)\n",
    "    plt.legend(loc=(1.1,0),title='Reported by:')\n",
    "    lines, labels = plt.thetagrids( theta_deg, (data_x) )\n",
    "    plt.show()\n",
    "    \n",
    "    night_radii = np.zeros(len(radii))\n",
    "    night_early_radii = np.zeros(len(radii))\n",
    "    early_radii = np.zeros(len(radii))\n",
    "    early_late_radii = np.zeros(len(radii))\n",
    "    late_radii = np.zeros(len(radii))\n",
    "    late_night_radii = np.zeros(len(radii))\n",
    "    for i in range(24):\n",
    "        if i==7: night_early_radii[i] = radii[i]\n",
    "        elif i >= 8 and i <= 12: early_radii[i] = radii[i]\n",
    "        elif i==13 or i==14: early_late_radii[i] = radii[i]\n",
    "        elif i >= 15 and i <= 19: late_radii[i] = radii[i]\n",
    "        elif i==20: late_night_radii[i] = radii[i]\n",
    "        else: night_radii[i] = radii[i]\n",
    "    plt.figure(figsize=(11,11))\n",
    "    ax = plt.subplot(111, projection='polar');\n",
    "    ax.bar(theta, night_radii, width=width,label='night',color='green');\n",
    "    ax.bar(theta, night_early_radii, width=width, label='night-early crossover', color='yellow')\n",
    "    ax.bar(theta, early_radii, width=width,label='early',color='orange');\n",
    "    ax.bar(theta, early_late_radii, width=width, label='early-late crossover', color='red')\n",
    "    ax.bar(theta, late_radii, width=width,label='late',color='purple');\n",
    "    ax.bar(theta, late_night_radii, width=width, label='late-night crossover', color='blue')\n",
    "    ax.bar(theta, 2*radii_err, bottom=radii-radii_err,width=width,alpha=0.5,color='none',hatch=\"////\",label='uncertainty');\n",
    "    ax.set_theta_offset(np.pi/2 + width/2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    yticklabels = ax.get_yticks()\n",
    "    yticklabels = [round(item,1) for item in yticklabels]\n",
    "    for i,item in enumerate(yticklabels):\n",
    "        if i%2==0: yticklabels[i]=''\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    #plt.xlabel('hour occured')\n",
    "    plt.title('Hourly number of medicine patient safety events',y=1)\n",
    "    plt.legend(loc=(0.9,0.81),title='Nursing shifts')\n",
    "    lines, labels = plt.thetagrids( theta_deg, (data_x) );\n",
    "    ax.axvspan(theta[np.argmax(radii)]-width/2,theta[np.argmax(radii)]+width/2,color='red',alpha=0.3);\n",
    "    ax.axvline(theta[7]-width/2,color='orange');\n",
    "    ax.axvline(theta[15]-width/2,color='orange');\n",
    "    ax.axvline(theta[12],color='purple');\n",
    "    ax.axvline(theta[20],color='purple');\n",
    "    ax.axvline(theta[20]-width/2,color='green');\n",
    "    ax.axvline(theta[8]-width/2,color='green');\n",
    "    ymax = ax.get_ylim()[1]\n",
    "    ax.annotate('Early shift start',xy=(theta[7]-width/2,ax.get_ylim()[1]),xytext=(theta[7]-width/2,ymax*1.2),color='orange',ha='center',\n",
    "               arrowprops=dict(facecolor='orange',alpha=0.1));\n",
    "    ax.annotate('Early shift end',xy=(theta[15]-width/2,ax.get_ylim()[1]),xytext=(theta[15]-width/2,ymax*1.1),color='orange',ha='right',\n",
    "               arrowprops=dict(facecolor='orange',alpha=0.1));\n",
    "    ax.annotate('Late shift start',xy=(theta[12],ax.get_ylim()[1]),xytext=(theta[12],ymax*1.1),color='purple',ha='center',\n",
    "               arrowprops=dict(facecolor='purple',alpha=0.1));\n",
    "    ax.annotate('Late shift end',xy=(theta[20],ax.get_ylim()[1]),xytext=(theta[20],ymax*1.1),color='purple',ha='right',\n",
    "               arrowprops=dict(facecolor='purple',alpha=0.1));\n",
    "    ax.annotate('Night shift start',xy=(theta[20]-width/2,ax.get_ylim()[1]),xytext=(theta[20]-width/2,ymax*1.1),color='green',ha='right',\n",
    "               arrowprops=dict(facecolor='green',alpha=0.1));\n",
    "    ax.annotate('Night shift end',xy=(theta[8]-width/2,ax.get_ylim()[1]),xytext=(theta[8]-width/2,ymax*1.1),color='green',ha='left',\n",
    "               arrowprops=dict(facecolor='green',alpha=0.1));\n",
    "    plt.show()\n",
    "    \n",
    "    night_radii = np.zeros(len(radii))\n",
    "    night_early_radii = np.zeros(len(radii))\n",
    "    early_radii = np.zeros(len(radii))\n",
    "    early_late_radii = np.zeros(len(radii))\n",
    "    late_radii = np.zeros(len(radii))\n",
    "    late_night_radii = np.zeros(len(radii))\n",
    "    for i in range(24):\n",
    "        if i==7: night_early_radii[i] = radii[i]\n",
    "        elif i >= 8 and i <= 12: early_radii[i] = radii[i]\n",
    "        elif i==13 or i==14: early_late_radii[i] = radii[i]\n",
    "        elif i >= 15 and i <= 19: late_radii[i] = radii[i]\n",
    "        elif i==20: late_night_radii[i] = radii[i]\n",
    "        else: night_radii[i] = radii[i]\n",
    "    plt.figure(figsize=(11,11))\n",
    "    ax = plt.subplot(111, projection='polar');\n",
    "    ax.bar(theta, night_radii, width=width,label='night',color='green');\n",
    "    ax.bar(theta, night_early_radii, width=width, label='night-early crossover', color='yellow')\n",
    "    ax.bar(theta, early_radii, width=width,label='early',color='orange');\n",
    "    ax.bar(theta, early_late_radii, width=width, label='early-late crossover', color='red')\n",
    "    ax.bar(theta, late_radii, width=width,label='late',color='purple');\n",
    "    ax.bar(theta, late_night_radii, width=width, label='late-night crossover', color='blue')\n",
    "    uncertainty = ax.bar(theta, 2*radii_err, bottom=radii-radii_err,width=width,alpha=0.5,color='none',hatch=\"////\");\n",
    "    ax.set_theta_offset(np.pi/2 + width/2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_yticklabels(['',0.2,'',0.4,'',0.6])\n",
    "    #plt.xlabel('hour occured')\n",
    "    plt.title('Hourly number of medicine patient safety events',y=1)\n",
    "    shift_legend = plt.legend(loc=(0.9,0.81),title='Nursing shifts')\n",
    "    lines, labels = plt.thetagrids( theta_deg, (data_x) );\n",
    "    for morning_hour in range(8,10):\n",
    "        morning_round = ax.axvspan(theta[morning_hour]-width/2,theta[morning_hour]+width/2,color='orange',alpha=0.1);\n",
    "    for lunch_hour in range(12,14):\n",
    "        lunch_round = ax.axvspan(theta[lunch_hour]-width/2,theta[lunch_hour]+width/2,color='red',alpha=0.1);\n",
    "    for afternoon_hour in range(17,19):\n",
    "        afternoon_round = ax.axvspan(theta[afternoon_hour]-width/2,theta[afternoon_hour]+width/2,color='purple',alpha=0.1);\n",
    "    for night_hour in range(21,23):\n",
    "        night_round = ax.axvspan(theta[night_hour]-width/2,theta[night_hour]+width/2,color='green',alpha=0.1);\n",
    "    round_legend = plt.legend([morning_round,lunch_round,afternoon_round,night_round],['morning','lunch','afternoon','night'],\n",
    "                             title='Drug rounds',loc=(1.0,0.66))\n",
    "    uncertainty_legend = plt.legend([uncertainty],['uncertainty'],loc=(0.73,0.97))\n",
    "    ax.add_artist(shift_legend)\n",
    "    ax.add_artist(round_legend)\n",
    "    ax.axvline(theta[7]-width/2,color='orange');\n",
    "    ax.axvline(theta[15]-width/2,color='orange');\n",
    "    ax.axvline(theta[12],color='purple');\n",
    "    ax.axvline(theta[20],color='purple');\n",
    "    ax.axvline(theta[20]-width/2,color='green');\n",
    "    ax.axvline(theta[8]-width/2,color='green');\n",
    "    ymax = ax.get_ylim()[1]\n",
    "    ax.annotate('Early shift start',xy=(theta[7]-width/2,ax.get_ylim()[1]),xytext=(theta[7]-width/2,ymax*1.2),color='orange',ha='center',\n",
    "               arrowprops=dict(facecolor='orange',alpha=0.1));\n",
    "    ax.annotate('Early shift end',xy=(theta[15]-width/2,ax.get_ylim()[1]),xytext=(theta[15]-width/2,ymax*1.1),color='orange',ha='right',\n",
    "               arrowprops=dict(facecolor='orange',alpha=0.1));\n",
    "    ax.annotate('Late shift start',xy=(theta[12],ax.get_ylim()[1]),xytext=(theta[12],ymax*1.1),color='purple',ha='center',\n",
    "               arrowprops=dict(facecolor='purple',alpha=0.1));\n",
    "    ax.annotate('Late shift end',xy=(theta[20],ax.get_ylim()[1]),xytext=(theta[20],ymax*1.1),color='purple',ha='right',\n",
    "               arrowprops=dict(facecolor='purple',alpha=0.1));\n",
    "    ax.annotate('Night shift start',xy=(theta[20]-width/2,ax.get_ylim()[1]),xytext=(theta[20]-width/2,ymax*1.1),color='green',ha='right',\n",
    "               arrowprops=dict(facecolor='green',alpha=0.1));\n",
    "    ax.annotate('Night shift end',xy=(theta[8]-width/2,ax.get_ylim()[1]),xytext=(theta[8]-width/2,ymax*1.1),color='green',ha='left',\n",
    "               arrowprops=dict(facecolor='green',alpha=0.1));\n",
    "    plt.show()\n",
    "    print('after hour')\n",
    "    '''\n",
    "    data_y_tot,_ = np.histogram(df_max_feature['reported_hour'].values, bins=bins)\n",
    "    data_y = data_y_tot/total_days\n",
    "    data_y_err = np.sqrt(data_y_tot)/total_days\n",
    "    radii = data_y\n",
    "    radii_err = data_y_err\n",
    "    morning_radii = np.zeros(len(radii))\n",
    "    afternoon_radii = np.zeros(len(radii))\n",
    "    evening_radii = np.zeros(len(radii))\n",
    "    for i in range(24):\n",
    "        if i <= 7: morning_radii[i] = radii[i]\n",
    "        elif i >= 8 and i <= 15: afternoon_radii[i] = radii[i]\n",
    "        else: evening_radii[i] = radii[i]\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111, projection='polar');\n",
    "    ax.bar(theta, morning_radii, width=width,label='night');\n",
    "    ax.bar(theta, afternoon_radii, width=width,label='daytime');\n",
    "    ax.bar(theta, evening_radii, width=width,label='evening');\n",
    "    ax.bar(theta, 2*radii_err, bottom=radii-radii_err,width=width,alpha=0.5,color='none',hatch=\"////\",label='Uncertainty');\n",
    "    ax.set_theta_offset(np.pi/2 + width/2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    yticklabels = ax.get_yticks()\n",
    "    yticklabels = [round(item,1) for item in yticklabels]\n",
    "    for i,item in enumerate(yticklabels):\n",
    "        if i%2==0: yticklabels[i]=''\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    plt.xlabel('hour reported')\n",
    "    plt.title('Hourly number of patient medicine safety events at QA',y=1.1)\n",
    "    plt.legend(loc=(1.04,0))\n",
    "    lines, labels = plt.thetagrids( theta_deg, (data_x) );\n",
    "    ax.axvspan(theta[np.argmax(radii)]-width/2,theta[np.argmax(radii)]+width/2,color='red',alpha=0.3);\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.bar(theta, radii, width=width,label='Data');\n",
    "    ax.bar(theta, radii_err, bottom=radii-radii_err/2,width=width,alpha=0.5,color='none',hatch=\"////\",label='Uncertainty');\n",
    "    ax.set_theta_offset(np.pi/2 + width/2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    yticklabels = ax.get_yticks()\n",
    "    yticklabels = [round(item,1) for item in yticklabels]\n",
    "    for i,item in enumerate(yticklabels):\n",
    "        if i%2==0: yticklabels[i]=''\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    plt.title('hour reported',y=1.1)\n",
    "    lines, labels = plt.thetagrids( theta_deg, (data_x) )\n",
    "    ax.axvline(theta[7]-width/2,color='orange');\n",
    "    ax.axvline(theta[15]-width/2,color='orange');\n",
    "    ax.axvline(theta[12],color='green');\n",
    "    ax.axvline(theta[20],color='green');\n",
    "    ax.axvline(theta[20]-width/2,color='red');\n",
    "    ax.axvline(theta[8]-width/2,color='red');\n",
    "    ax.annotate('Early shift start',xy=(theta[7]-width/2,ax.get_ylim()[1]),xytext=(theta[7]-width/2,ax.get_ylim()[1]*11/7),color='orange',ha='center',\n",
    "           arrowprops=dict(facecolor='orange',alpha=0.1));\n",
    "    ax.annotate('Early shift end',xy=(theta[15]-width/2,ax.get_ylim()[1]),xytext=(theta[15]-width/2,ax.get_ylim()[1]*9/7),color='orange',ha='right',\n",
    "               arrowprops=dict(facecolor='orange',alpha=0.1));\n",
    "    ax.annotate('Late shift start',xy=(theta[12],ax.get_ylim()[1]),xytext=(theta[12],ax.get_ylim()[1]*9/7),color='green',ha='center',\n",
    "               arrowprops=dict(facecolor='green',alpha=0.1));\n",
    "    ax.annotate('Late shift end',xy=(theta[20],ax.get_ylim()[1]),xytext=(theta[20],ax.get_ylim()[1]*9/7),color='green',ha='right',\n",
    "               arrowprops=dict(facecolor='green',alpha=0.1));\n",
    "    ax.annotate('Night shift start',xy=(theta[20]-width/2,ax.get_ylim()[1]),xytext=(theta[20]-width/2,ax.get_ylim()[1]*9/7),color='red',ha='right',\n",
    "               arrowprops=dict(facecolor='red',alpha=0.1));\n",
    "    ax.annotate('Night shift end',xy=(theta[8]-width/2,ax.get_ylim()[1]),xytext=(theta[8]-width/2,ax.get_ylim()[1]*9/7),color='red',ha='left',\n",
    "           arrowprops=dict(facecolor='red',alpha=0.1));\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    reportedby_counts = df_max_feature['inc_reportedby'].value_counts().drop(labels=[''])\n",
    "    reportedby_counts_nlargest = reportedby_counts.nlargest(9)\n",
    "    thresh = reportedby_counts_nlargest[-1]\n",
    "    reportedby_counts = reportedby_counts.append(pd.Series(reportedby_counts[reportedby_counts.values < thresh].sum(),index=['Other']))\n",
    "    reportedby_counts = reportedby_counts[reportedby_counts.values >= thresh]\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    cumulative_bottom = 0\n",
    "    for i,counts in reportedby_counts.items():\n",
    "        radii_reported_tot,_ = np.histogram(df_max_feature[df_max_feature['inc_reportedby']==i].reported_hour.values, bins=bins)\n",
    "        radii_reported = radii_reported_tot/total_days\n",
    "        ax.bar(theta, radii_reported, bottom=cumulative_bottom, width=width, label=i);\n",
    "        cumulative_bottom += radii_reported\n",
    "    ax.bar(theta, radii_err, bottom=cumulative_bottom-radii_err/2,width=width,alpha=0.5,color='none',hatch=\"////\",label='Total Uncertainty');\n",
    "    ax.set_theta_offset(np.pi/2 + width/2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    yticklabels = ax.get_yticks()\n",
    "    yticklabels = [round(item,1) for item in yticklabels]\n",
    "    for i,item in enumerate(yticklabels):\n",
    "        if i%2==0: yticklabels[i]=''\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    plt.xlabel('hour reported')\n",
    "    plt.title('Hourly number of patient medicine safety events at QA',y=1.1)\n",
    "    plt.legend(loc=(1.1,0),title='Reported by:')\n",
    "    lines, labels = plt.thetagrids( theta_deg, (data_x) )\n",
    "    plt.show()\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekday_plot():\n",
    "    bins = [i-0.5 for i in range(8)]\n",
    "    data_x = [i for i in range(7)]\n",
    "    data_y_tot,_ = np.histogram(df_max_feature['weekday'].values, bins=bins)\n",
    "    data_y = data_y_tot/total_weeks\n",
    "    data_y_err = np.sqrt(data_y_tot)/total_weeks\n",
    "    \n",
    "    # data fit\n",
    "\n",
    "    def raised_backward_step(x, A, raised):\n",
    "        return [A if xi<4.5 else raised for xi in x]\n",
    "\n",
    "    step_model = Model(raised_backward_step)\n",
    "    out = step_model.fit(data_y, x=data_x, weights=1/data_y_err, A=data_y[0], raised=data_y[-1])\n",
    "\n",
    "    groups= ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']\n",
    "    counts = data_y\n",
    "    error = data_y_err\n",
    "    upper = [x+e for x,e in zip(counts, error) ]\n",
    "    lower = [x-e for x,e in zip(counts, error) ]\n",
    "\n",
    "    source = ColumnDataSource(data=dict(groups=groups, counts=counts, upper=upper, lower=lower))\n",
    "\n",
    "    p = bokeh_figure(x_range=groups, plot_height=450, title=\"Daily medicine patient safety events at QA\", y_range=(0,math.ceil(max(upper))),\n",
    "              x_axis_label='Weekday',y_axis_label='events per day',\n",
    "              tools='pan,box_zoom,wheel_zoom,save,reset,help')\n",
    "\n",
    "    r_fit = p.line(groups,out.best_fit,line_color='purple')\n",
    "\n",
    "    band = Band(base='groups', lower='lower', upper='upper', source=source, level='underlay',\n",
    "                fill_alpha=1.0, line_width=1, line_color='black')\n",
    "    p.add_layout(band)\n",
    "\n",
    "    p.xgrid.grid_line_color = None\n",
    "\n",
    "    legend = Legend(items=[\n",
    "        LegendItem(label=\"uncertainty\", renderers=[p.vbar(x=0, top=0, width=0.9, color='yellow', line_width=1, line_color='black')]),\n",
    "        LegendItem(label='fit χ²/Nᵈᶠ = '+str(round(out.redchi,1)), renderers=[r_fit]),\n",
    "    ])\n",
    "    p.add_layout(legend)\n",
    "\n",
    "    vline = Span(location=5, dimension='height', line_color='red', line_width=3)\n",
    "    p.renderers.extend([vline])\n",
    "\n",
    "    output_notebook()\n",
    "    \n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_plot():\n",
    "    bins = [i+0.5 for i in range(13)]\n",
    "    data_x = [i+1 for i in range(12)]\n",
    "    data_y_tot,_ = np.histogram(df_max_feature['month'].values, bins=bins)\n",
    "    data_y = data_y_tot/division_factor\n",
    "    data_y_err = np.sqrt(data_y_tot)/division_factor\n",
    "    \n",
    "    # data fit\n",
    "    polynomial_mod = PolynomialModel(0)\n",
    "    pars = polynomial_mod.guess(data_y, x=data_x, c0=data_y[0])\n",
    "    model = polynomial_mod\n",
    "    out = model.fit(data_y, pars, x=data_x, weights=1/data_y_err)\n",
    "\n",
    "    groups= ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "    counts = data_y\n",
    "    error = data_y_err\n",
    "    upper = [x+e for x,e in zip(counts, error) ]\n",
    "    lower = [x-e for x,e in zip(counts, error) ]\n",
    "    color = ['blue','blue','green','green','green','red','red','red','yellow','yellow','yellow','blue']\n",
    "\n",
    "    source = ColumnDataSource(data=dict(groups=groups, counts=counts, upper=upper, lower=lower, color=color))\n",
    "\n",
    "    p = bokeh_figure(x_range=groups, plot_height=450, title=\"Daily medicine patient safety events at QA\", y_range=(0,math.ceil(max(counts))),\n",
    "              x_axis_label='Month',y_axis_label='events per day',\n",
    "              tools='pan,box_select,box_zoom,wheel_zoom,save,reset,help')\n",
    "\n",
    "    p.vbar(x='groups', top='counts', width=0.9, source=source, \n",
    "           line_color='white', color='color',nonselection_alpha=0, \n",
    "             hover_fill_color='black')\n",
    "    r_err = p.vbar(x='groups', bottom='lower', top='upper', width=0.9, source=source, fill_color='grey', fill_alpha=0.9,\n",
    "          line_color='white')\n",
    "    r_linear = p.line(groups,out.best_fit,line_color='purple')\n",
    "\n",
    "    p.xgrid.grid_line_color = None\n",
    "\n",
    "    legend = Legend(items=[\n",
    "        LegendItem(label=\"winter\", renderers=[p.vbar(x=0, top=0, width=0.9, color='blue')]),\n",
    "        LegendItem(label=\"spring\", renderers=[p.vbar(x=0, top=0, width=0.9, color='green')]),\n",
    "        LegendItem(label=\"summer\", renderers=[p.vbar(x=0, top=0, width=0.9, color='red')]),\n",
    "        LegendItem(label=\"autumn\", renderers=[p.vbar(x=0, top=0, width=0.9, color='yellow')]),\n",
    "        LegendItem(label=\"uncertainty\", renderers=[r_err]),\n",
    "        LegendItem(label='fit χ²/Nᵈᶠ = '+str(round(out.redchi,1)), renderers=[r_linear]),\n",
    "    ])\n",
    "    p.add_layout(legend)\n",
    "    p.legend.location = \"bottom_right\"\n",
    "\n",
    "    # Create a HoverTool: hover\n",
    "    hover = HoverTool(tooltips=None,mode='hline')\n",
    "\n",
    "    # Add the hover tool to the figure p\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    output_notebook()\n",
    "    \n",
    "    show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_plot():\n",
    "    plt.figure()\n",
    "    df_day = df_max_feature.copy()\n",
    "    df_day = df_day[df_day.inc_dincident > '2016-03-31']\n",
    "    df_day = df_day[df_day.inc_dincident < '2019-12-01']\n",
    "    df_day.head()\n",
    "\n",
    "    df_day['day'] = df_day['inc_dincident'].dt.day\n",
    "    df_day.head()\n",
    "\n",
    "    bins = [i+0.5 for i in range(32)]\n",
    "    data_x = [i+1 for i in range(31)]\n",
    "    df_day_grouped_by_day = df_day.groupby(['inc_dincident']).size().reset_index(name='counts')\n",
    "    df_day_grouped_by_day['day'] = df_day_grouped_by_day['inc_dincident'].dt.day\n",
    "\n",
    "    day_occurences = np.ones(31)*44\n",
    "    day_occurences[-1] = 25\n",
    "    day_occurences[-2] = 41\n",
    "    day_occurences[-3] = 41\n",
    "\n",
    "    data_y_tot,_ = np.histogram(df_day['day'].values, bins=bins)\n",
    "    data_y = data_y_tot/day_occurences\n",
    "    data_y_err = np.sqrt(data_y_tot)/day_occurences\n",
    "\n",
    "    # data fit\n",
    "    polynomial_mod = PolynomialModel(0)\n",
    "    pars = polynomial_mod.guess(data_y, x=data_x, c0=data_y[0])\n",
    "    model = polynomial_mod\n",
    "    out = model.fit(data_y, pars, x=data_x, weights=1/data_y_err)\n",
    "\n",
    "    df_day_grouped_by_day.boxplot(by='day',column=['counts'],figsize=(17,13));\n",
    "    plt.plot(data_x, out.best_fit, '-r')\n",
    "    plt.ylim(bottom=0);\n",
    "    plt.ylabel('# events per day');\n",
    "    plt.title('Daily number of patient medicine safety events at QA');\n",
    "    plt.suptitle('');\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    line = Line2D([],[], ls=\"none\",markeredgecolor='k')\n",
    "    barline = LineCollection(np.empty((2,2,2)))\n",
    "    custom_lines = [Line2D([0], [0], color='green'),\n",
    "                    mpatches.Patch(facecolor='w',edgecolor='b'),\n",
    "                    ErrorbarContainer((line, [line], [barline]), has_yerr=True),\n",
    "                   Line2D([0], [0], marker='o', color='w', markerfacecolor='w', markeredgecolor='k',markersize=10),\n",
    "                   Line2D([0], [0], color='red')]\n",
    "    plt.legend(custom_lines,['median','quartiles','extremes','outliers','Fit '+r'$\\chi^2/N_{df}$ = '+str(round(out.redchi,1))]);\n",
    "    img = plt.imread(\"nhsportsmouth.png\");\n",
    "    ax = plt.axes([0.2,0.7, 0.3, 0.3], frameon=True);  # Change the numbers in this array to position your image [left, bottom, width, height])\n",
    "    ax.imshow(img);\n",
    "    ax.axis('off'); # get rid of the ticks and ticklabels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def division_care_group_loc_plot():\n",
    "    df_division_care_group = df_max_feature.copy()\n",
    "    df_division_care_group['division-care_group'] = df_division_care_group['division-care_group'].str.replace(' and ',' & ')\n",
    "    divider = df_division_care_group['division-care_group'].str.split(\" Division - \",n=1,expand=True)\n",
    "    df_division_care_group['division'] = divider[0]\n",
    "    df_division_care_group['care_group'] = divider[1]\n",
    "    #df_division_care_group['division'] = df_division_care_group['division'].str.replace(' and ',' & ')\n",
    "    df_division_care_group.loc[df_division_care_group.division == 'Corporate Functions', 'care_group'] = 'Corporate Functions'\n",
    "    df_division_care_group = df_division_care_group[df_division_care_group['division-care_group'] != 'Corporate Functions']\n",
    "    df_division_care_group = df_division_care_group[df_division_care_group.care_group != 'Pathology']\n",
    "    #df_division_care_group.loc[df_division_care_group.division == 'Surgical & Outpatients', 'division'] = 'Surgical & Outpatients + Corporate Functions'\n",
    "    #df_division_care_group.loc[df_division_care_group.division == 'Corporate Functions', 'division'] = 'Surgical & Outpatients + Corporate Functions'\n",
    "\n",
    "    division_counts = df_division_care_group['division'].value_counts()\n",
    "    division_percentages = 100*division_counts/sum(division_counts.values)\n",
    "    division_labels = [str(i) for i in division_counts.index]\n",
    "\n",
    "    df_division_care_group['division_sorted'] = pd.Categorical(df_division_care_group['division'], division_labels)\n",
    "    df_division_care_group = df_division_care_group.sort_values(\"division_sorted\")\n",
    "    df_division_care_group.head()\n",
    "\n",
    "    care_group_labels = []\n",
    "    for i in division_counts.index:\n",
    "        df_division = df_division_care_group[df_division_care_group.division == i]\n",
    "        per_division_counts = df_division['care_group'].value_counts()\n",
    "        care_group_labels += [str(i) for i in per_division_counts.index]\n",
    "\n",
    "    df_division_care_group['care_group_sorted'] = pd.Categorical(df_division_care_group['care_group'], care_group_labels)\n",
    "    df_division_care_group = df_division_care_group.sort_values(['division_sorted','care_group_sorted'])\n",
    "    df_division_care_group.head()\n",
    "\n",
    "    care_group_counts = df_division_care_group['care_group'].value_counts()[df_division_care_group['care_group'].unique()]\n",
    "    care_group_percentages = 100*care_group_counts/sum(care_group_counts.values)\n",
    "    for care_group,percentage in care_group_percentages.iteritems():\n",
    "        if percentage <= 2.210928:\n",
    "            df_division_care_group.loc[df_division_care_group['care_group'] == care_group, 'care_group'] = 'Other'\n",
    "    care_group_counts = df_division_care_group['care_group'].value_counts()[df_division_care_group['care_group'].unique()]\n",
    "    care_group_percentages = 100*care_group_counts/sum(care_group_counts.values)\n",
    "    care_group_labels = care_group_counts.index\n",
    "\n",
    "    # Create colors\n",
    "    a, b, c, d, e =[plt.cm.Reds, plt.cm.Oranges, plt.cm.Greens, plt.cm.Blues, plt.cm.Purples]\n",
    "    a, c, d, e = [plt.cm.Reds, plt.cm.Greens, plt.cm.Blues, plt.cm.Purples]\n",
    "    division_colour_dict = {'Medicine & Urgent Care':plt.cm.Reds, 'Networked Services':plt.cm.Greens, 'Surgical & Outpatients':plt.cm.Blues, 'Clinical Delivery':plt.cm.Purples}\n",
    "    inner_ring_colours = [division_colour_dict[division] for division in division_labels]\n",
    "\n",
    "    # First Ring (Inside)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('equal')\n",
    "    division_labels_newlines = [i.replace(' ','\\n') for i in division_labels]\n",
    "    #mypie, _ = ax.pie(division_counts, radius=2, labels=division_labels_newlines, labeldistance=0.6, colors=[a(0.9), b(0.9), c(0.9), d(0.9), e(0.9)], textprops={'color':'w'})\n",
    "    mypie, _ = ax.pie(division_counts, radius=1.9, labels=division_labels_newlines, labeldistance=0.5, colors=[inner_ring_colours[0](0.9), inner_ring_colours[1](0.9), inner_ring_colours[2](0.9), inner_ring_colours[3](0.9)], textprops={'color':'w'}, startangle=90, counterclock=False)\n",
    "    plt.setp( mypie, width=1.9, edgecolor='white')\n",
    "\n",
    "    # Second Ring (oUTside)\n",
    "    care_group_colours = []\n",
    "    for i,index in enumerate(division_counts.index):\n",
    "        df_division = df_division_care_group[df_division_care_group.division == index]\n",
    "        per_division_counts = df_division['care_group'].value_counts()\n",
    "        slice_colour = 8\n",
    "        for j in range(len(per_division_counts)):\n",
    "            care_group_colours.append(inner_ring_colours[i](slice_colour/10))\n",
    "            slice_colour -= 1\n",
    "    mypie2, texts = ax.pie(care_group_counts, radius=2.2, labels=care_group_labels, colors=care_group_colours, startangle=90, counterclock=False)\n",
    "    plt.setp( mypie2, width=0.4, edgecolor='white')\n",
    "    for text, color in zip(texts, care_group_colours):\n",
    "        text.set_color(color)\n",
    "    plt.margins(0,0)\n",
    "\n",
    "    plt.title('Medicine patient safety events at QA - division (inner ring) and care group (outer ring)', y=1.6);\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    df_division_care_group.specialty = df_division_care_group.specialty.str.replace(' Rehabilitation','\\nRehab')\n",
    "    df_division_care_group.specialty = df_division_care_group.specialty.str.replace('Ear Nose and Throat','ENT')\n",
    "    df_division_care_group.specialty = df_division_care_group.specialty.str.replace('Acute ','Acute\\n')\n",
    "    \n",
    "    specialty_counts = df_division_care_group['specialty'].value_counts()[df_division_care_group['specialty'].unique()]\n",
    "\n",
    "    specialty_labels = []\n",
    "    for i in care_group_counts.index:\n",
    "        df_care_group = df_division_care_group[df_division_care_group.care_group == i]\n",
    "        per_care_group_counts = df_care_group['specialty'].value_counts()\n",
    "        specialty_labels += [i + ' - ' + str(j) for j in per_care_group_counts.index]\n",
    "\n",
    "        \n",
    "    specialty_labels = [re.sub(r\"Surgery\\s-\\s(.+)\\sSurgery\",r\"Surgery - \\1\",s) for s in specialty_labels]\n",
    "    df_division_care_group.specialty = df_division_care_group.specialty.str.replace(' Surgery','')\n",
    "    df_division_care_group['care_group-specialty'] = df_division_care_group['care_group'] + ' - ' + df_division_care_group['specialty']\n",
    "    df_division_care_group['specialty_sorted'] = pd.Categorical(df_division_care_group['care_group-specialty'], specialty_labels)\n",
    "    df_division_care_group = df_division_care_group.sort_values(['division_sorted','care_group_sorted','specialty_sorted'])\n",
    "    df_division_care_group.head()\n",
    "\n",
    "    specialty_counts = df_division_care_group['care_group-specialty'].value_counts()[df_division_care_group['care_group-specialty'].unique()]\n",
    "    specialty_percentages = 100*specialty_counts/sum(specialty_counts.values)\n",
    "\n",
    "    for specialty,percentage in specialty_percentages.iteritems():\n",
    "        if percentage <= 1.407625:\n",
    "            df_division_care_group.loc[df_division_care_group['care_group-specialty'] == specialty, 'care_group-specialty'] = specialty.split(\" - \")[0] + ' - Other'\n",
    "\n",
    "    specialty_counts = df_division_care_group['care_group-specialty'].value_counts()[df_division_care_group['care_group-specialty'].unique()]\n",
    "    specialty_percentages = 100*specialty_counts/sum(specialty_counts.values)\n",
    "    specialty_labels_split = specialty_counts.index.str.split(\" - \",n=1)\n",
    "    specialty_labels = [el[1] for el in specialty_labels_split]\n",
    "\n",
    "    # Create colors\n",
    "    a, b, c, d, e =[plt.cm.Reds, plt.cm.Oranges, plt.cm.Greens, plt.cm.Blues, plt.cm.Purples]\n",
    "    a, c, d, e = [plt.cm.Reds, plt.cm.Greens, plt.cm.Blues, plt.cm.Purples]\n",
    "    division_colour_dict = {'Medicine & Urgent Care':plt.cm.Reds, 'Networked Services':plt.cm.Greens, 'Surgical & Outpatients':plt.cm.Blues, 'Clinical Delivery':plt.cm.Purples}\n",
    "    inner_ring_colours = [division_colour_dict[division] for division in division_labels]\n",
    "\n",
    "    # First Ring (Inside)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('equal')\n",
    "    division_labels_newlines = [i.replace(' ','\\n') for i in division_labels]\n",
    "    #mypie, _ = ax.pie(division_counts, radius=2, labels=division_labels_newlines, labeldistance=0.6, colors=[a(0.9), b(0.9), c(0.9), d(0.9), e(0.9)], textprops={'color':'w'})\n",
    "    mypie, _ = ax.pie(division_counts, radius=1.9, labels=division_labels_newlines, labeldistance=0.5, colors=[inner_ring_colours[0](0.9), inner_ring_colours[1](0.9), inner_ring_colours[2](0.9), inner_ring_colours[3](0.9)], textprops={'color':'w'}, startangle=90, counterclock=False)\n",
    "    plt.setp( mypie, width=1.9, edgecolor='white')\n",
    "\n",
    "    # Second Ring (oUTside)\n",
    "    care_group_labels_newlines = [i.replace(' ','\\n') for i in care_group_labels]\n",
    "    care_group_labels_newlines[care_group_labels_newlines.index('Renal\\n&\\nTransplantation')] = 'Renal &\\nTransplantation'\n",
    "    #care_group_labels_newlines[care_group_labels_newlines.index('Critical\\nCare,\\nTheatres,\\nAnaesthetics\\n&\\nHSDU')] = 'Critical Care, Theatres,\\nAnaesthetics\\n& HSDU'\n",
    "    #care_group_labels_newlines[care_group_labels_newlines.index('Imaging\\nRadiology')] = 'Imaging Radiology'\n",
    "    #care_group_colours = [a(0.8), a(0.7), a(0.6), a(0.5), b(0.8), c(0.8), c(0.7), c(0.6), d(0.8), d(0.7), d(0.6), e(0.8), e(0.7), e(0.6)]\n",
    "    care_group_colours = [a(0.8), a(0.7), a(0.6), c(0.8), c(0.7), c(0.6), d(0.8), d(0.7), d(0.6), e(0.8), e(0.7), e(0.6)]\n",
    "    care_group_colours = []\n",
    "    for i,index in enumerate(division_counts.index):\n",
    "        df_division = df_division_care_group[df_division_care_group.division == index]\n",
    "        per_division_counts = df_division['care_group'].value_counts()\n",
    "        slice_colour = 8\n",
    "        for j in range(len(per_division_counts)):\n",
    "            care_group_colours.append(inner_ring_colours[i](slice_colour/10))\n",
    "            slice_colour -= 1\n",
    "    mypie2, texts = ax.pie(care_group_counts, radius=3.6, labels=care_group_labels_newlines, labeldistance=0.7, colors=care_group_colours, textprops={'color':'k'},startangle=90, counterclock=False)\n",
    "    plt.setp( mypie2, width=1.7, edgecolor='white')\n",
    "    care_group_label_colours = ['k' if i%2==0 else 'w' for i in range(len(care_group_counts))]\n",
    "    for text, color in zip(texts, care_group_label_colours):\n",
    "        text.set_color(color)\n",
    "    plt.margins(0,0)\n",
    "\n",
    "    # Third Ring (oUTside)\n",
    "    specialty_colours = [a(0.5),a(0.49),a(0.48),a(0.47),a(0.4),a(0.39),a(0.38),a(0.37),a(0.36),a(0.35),a(0.34),a(0.33),a(0.32),a(0.31),a(0.30),a(0.29),a(0.28),a(0.27),a(0.26),a(0.25),a(0.24),c(0.50),c(0.49),c(0.48),c(0.47),c(0.46),c(0.45),c(0.40),c(0.39),c(0.38),c(0.37),c(0.36),c(0.30),c(0.29),c(0.28),c(0.27),d(0.50),d(0.40),d(0.39),d(0.38),d(0.37),d(0.30),e(0.50),e(0.49),e(0.48),e(0.47),e(0.46),e(0.45),e(0.44),e(0.40),e(0.39),e(0.38),e(0.37)]\n",
    "    specialty_colours = []\n",
    "    for i,div_index in enumerate(division_counts.index):\n",
    "        df_division = df_division_care_group[df_division_care_group.division == div_index]\n",
    "        slice_colour_orig = 0.5\n",
    "        division_care_group_counts = df_division['care_group'].value_counts()[df_division['care_group'].unique()]\n",
    "        for j,care_index in enumerate(division_care_group_counts.index):\n",
    "            df_care_group = df_division[df_division['care_group-specialty'].str.startswith(care_index)]\n",
    "            per_care_group_counts = df_care_group['care_group-specialty'].value_counts()[df_care_group['care_group-specialty'].unique()]\n",
    "            slice_colour = slice_colour_orig - j/10\n",
    "            for k in per_care_group_counts:\n",
    "                specialty_colours.append(inner_ring_colours[i](slice_colour))\n",
    "                slice_colour -= 0.01\n",
    "    mypie3, texts = ax.pie(specialty_counts, radius=3.9, labels=specialty_labels, colors=specialty_colours, startangle=90, counterclock=False)\n",
    "    plt.setp( mypie3, width=0.3, edgecolor='white')\n",
    "    for text, color in zip(texts, specialty_colours):\n",
    "        text.set_color(color)\n",
    "    plt.margins(0,0)\n",
    "\n",
    "    plt.title('Medicine patient safety events at QA - division (inner ring), care group (middle ring), specialty (outer ring)', y=2.3);\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Department of Critical Care (E5) (DCCQ)', 'location'] = 'E5'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Acute Medical Unit (AMU) (MAU)', 'location'] = 'AMU'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Haematology & Oncology Day Unit (HODU)', 'location'] = 'HODU'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Childrens Assesment Unit (CAU)', 'location'] = 'CAU'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Hepatology Clinic (Nurse led) (on C5)', 'location'] = 'Hepatology'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Cardiac Day Unit (CDU)', 'location'] = 'CDU'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Surgical Assessment Unit (SAU)', 'location'] = 'SAU'\n",
    "    df_division_care_group.location = df_division_care_group.location.str.replace(' QA','')\n",
    "    df_division_care_group.location = df_division_care_group.location.str.replace(' Department','')\n",
    "    df_division_care_group.location = df_division_care_group.location.str.replace(' Unit','')\n",
    "    df_division_care_group.location = df_division_care_group.location.str.replace(' Centre','')\n",
    "    df_division_care_group.location = df_division_care_group.location.str.replace(' Clinic','')\n",
    "    df_division_care_group.location = df_division_care_group.location.str.replace(' and ',' & ')\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Theatre Admissions Suite (TAS)', 'location'] = 'TAS'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Pre-Operative Assessment (POA)', 'location'] = 'POA'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Theatre - D1', 'location'] = 'D1'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Research & Development', 'location'] = 'R&D'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Reception (Atrium)', 'location'] = 'Reception'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'G5 Endoscopy Suites', 'location'] = 'G5'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'F3 Stroke Rehabilitation', 'location'] = 'F3'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'F4 Acute Stroke Ward', 'location'] = 'F4'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'F1  Neuro Rehab', 'location'] = 'F1'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Emergency (ED)', 'location'] = 'ED'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'B6  Antenatal Ward', 'location'] = 'B6'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'B7 Postnatal Ward', 'location'] = 'B7'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'B9 NICU', 'location'] = 'B9'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'A7 Starfish', 'location'] = 'A7'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'A8 Shipwreck', 'location'] = 'A8'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'B5  - Mary Rose Ward', 'location'] = 'B5'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'B8 Labour Ward', 'location'] = 'B8'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Radiotherapy  Dept within CHOC', 'location'] = 'Radiotherapy'\n",
    "    df_division_care_group.loc[df_division_care_group.location == 'Surgical High Care (SHCU)', 'location'] = 'SHCU'\n",
    "\n",
    "    location_counts = df_division_care_group['location'].value_counts()[df_division_care_group['location'].unique()]\n",
    "\n",
    "    location_labels = []\n",
    "    for i in care_group_counts.index:\n",
    "        df_care_group = df_division_care_group[df_division_care_group.care_group == i]\n",
    "        per_care_group_counts = df_care_group['location'].value_counts()\n",
    "        location_labels += [i + ' - ' + str(j) for j in per_care_group_counts.index]\n",
    "\n",
    "    df_division_care_group['care_group-location'] = df_division_care_group['care_group'] + ' - ' + df_division_care_group['location']\n",
    "    df_division_care_group['location_sorted'] = pd.Categorical(df_division_care_group['care_group-location'], location_labels)\n",
    "    df_division_care_group = df_division_care_group.sort_values(['division_sorted','care_group_sorted','location_sorted'])\n",
    "    df_division_care_group.head()\n",
    "    \n",
    "    location_counts = df_division_care_group['care_group-location'].value_counts()[df_division_care_group['care_group-location'].unique()]\n",
    "    location_percentages = 100*location_counts/sum(location_counts.values)\n",
    "\n",
    "    for location,percentage in location_percentages.iteritems():\n",
    "        if percentage < 1.407625:\n",
    "            df_division_care_group.loc[df_division_care_group['care_group-location'] == location, 'care_group-location'] = location.split(\" - \")[0] + ' - Other'\n",
    "\n",
    "    location_counts = df_division_care_group['care_group-location'].value_counts()[df_division_care_group['care_group-location'].unique()]\n",
    "    location_percentages = 100*location_counts/sum(location_counts.values)\n",
    "    location_labels_split = location_counts.index.str.split(\" - \",n=1)\n",
    "    location_labels = [el[1] for el in location_labels_split]\n",
    "\n",
    "    # Create colors\n",
    "    a, b, c, d, e =[plt.cm.Reds, plt.cm.Oranges, plt.cm.Greens, plt.cm.Blues, plt.cm.Purples]\n",
    "    a, c, d, e = [plt.cm.Reds, plt.cm.Greens, plt.cm.Blues, plt.cm.Purples]\n",
    "\n",
    "    # First Ring (Inside)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('equal')\n",
    "    division_labels_newlines = [i.replace(' ','\\n') for i in division_labels]\n",
    "    #mypie, _ = ax.pie(division_counts, radius=2, labels=division_labels_newlines, labeldistance=0.6, colors=[a(0.9), b(0.9), c(0.9), d(0.9), e(0.9)], textprops={'color':'w'})\n",
    "    mypie, _ = ax.pie(division_counts, radius=1.9, labels=division_labels_newlines, labeldistance=0.5, colors=[inner_ring_colours[0](0.9), inner_ring_colours[1](0.9), inner_ring_colours[2](0.9), inner_ring_colours[3](0.9)], textprops={'color':'w'}, startangle=90, counterclock=False)\n",
    "    plt.setp( mypie, width=1.9, edgecolor='white')\n",
    "\n",
    "    # Second Ring (oUTside)\n",
    "    care_group_labels_newlines = [i.replace(' ','\\n') for i in care_group_labels]\n",
    "    care_group_labels_newlines[care_group_labels_newlines.index('Renal\\n&\\nTransplantation')] = 'Renal &\\nTransplantation'\n",
    "    #care_group_labels_newlines[care_group_labels_newlines.index('Critical\\nCare,\\nTheatres,\\nAnaesthetics\\n&\\nHSDU')] = 'Critical Care, Theatres,\\nAnaesthetics\\n& HSDU'\n",
    "    care_group_colours = []\n",
    "    for i,index in enumerate(division_counts.index):\n",
    "        df_division = df_division_care_group[df_division_care_group.division == index]\n",
    "        per_division_counts = df_division['care_group'].value_counts()\n",
    "        slice_colour = 8\n",
    "        for j in range(len(per_division_counts)):\n",
    "            care_group_colours.append(inner_ring_colours[i](slice_colour/10))\n",
    "            slice_colour -= 1\n",
    "    mypie2, texts = ax.pie(care_group_counts, radius=3.6, labels=care_group_labels_newlines, labeldistance=0.7, colors=care_group_colours, textprops={'color':'k'},startangle=90, counterclock=False)\n",
    "    plt.setp( mypie2, width=1.7, edgecolor='white')\n",
    "    care_group_label_colours = ['k' if i%2==0 else 'w' for i in range(len(care_group_counts))]\n",
    "    for text, color in zip(texts, care_group_label_colours):\n",
    "        text.set_color(color)\n",
    "    plt.margins(0,0)\n",
    "\n",
    "    # Third Ring (oUTside)\n",
    "    location_colours = [a(0.5),a(0.49),a(0.48),a(0.47),a(0.4),a(0.39),a(0.38),a(0.37),a(0.36),a(0.35),a(0.34),a(0.33),a(0.32),a(0.31),a(0.30),a(0.29),a(0.28),a(0.27),a(0.26),a(0.25),a(0.24),c(0.50),c(0.49),c(0.48),c(0.47),c(0.46),c(0.45),c(0.40),c(0.39),c(0.38),c(0.37),c(0.36),c(0.30),c(0.29),c(0.28),c(0.27),d(0.50),d(0.40),d(0.39),d(0.38),d(0.37),d(0.30),e(0.50),e(0.49),e(0.48),e(0.47),e(0.46),e(0.45),e(0.44),e(0.40),e(0.39),e(0.38),e(0.37)]\n",
    "    location_colours = []\n",
    "    for i,div_index in enumerate(division_counts.index):\n",
    "        df_division = df_division_care_group[df_division_care_group.division == div_index]\n",
    "        slice_colour_orig = 0.5\n",
    "        division_care_group_counts = df_division['care_group'].value_counts()[df_division['care_group'].unique()]\n",
    "        for j,care_index in enumerate(division_care_group_counts.index):\n",
    "            df_care_group = df_division[df_division['care_group-location'].str.startswith(care_index)]\n",
    "            per_care_group_counts = df_care_group['care_group-location'].value_counts()[df_care_group['care_group-location'].unique()]\n",
    "            slice_colour = slice_colour_orig - j/10\n",
    "            for k in per_care_group_counts:\n",
    "                location_colours.append(inner_ring_colours[i](slice_colour))\n",
    "                slice_colour -= 0.01\n",
    "    mypie3, texts = ax.pie(location_counts, radius=3.9, labels=location_labels, colors=location_colours, startangle=90, counterclock=False)\n",
    "    plt.setp( mypie3, width=0.3, edgecolor='white')\n",
    "    for text, color in zip(texts, location_colours):\n",
    "        text.set_color(color)\n",
    "    plt.margins(0,0)\n",
    "\n",
    "    plt.title('Medicine patient safety events at QA - division (inner ring), care group (middle ring), location (outer ring)', y=2.3);\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    for specialty,percentage in specialty_percentages.iteritems():\n",
    "        if percentage < 0.860021:\n",
    "            df_division_care_group.loc[df_division_care_group['care_group-specialty'] == specialty, 'care_group-specialty'] = specialty.split(\" - \")[0] + ' - Other'\n",
    "\n",
    "    specialty_counts = df_division_care_group['care_group-specialty'].value_counts()[df_division_care_group['care_group-specialty'].unique()]\n",
    "    specialty_percentages = 100*specialty_counts/sum(specialty_counts.values)\n",
    "    specialty_labels_split = specialty_counts.index.str.split(\" - \",n=1)\n",
    "    specialty_labels = [el[1] for el in specialty_labels_split]\n",
    "\n",
    "    location_counts = df_division_care_group['location'].value_counts()[df_division_care_group['location'].unique()]\n",
    "\n",
    "    location_labels = []\n",
    "    for i in care_group_counts.index:\n",
    "        df_care_group = df_division_care_group[df_division_care_group.care_group == i]\n",
    "        per_care_group_counts = df_care_group['location'].value_counts()\n",
    "        location_labels += [i + ' - ' + str(j) for j in per_care_group_counts.index]\n",
    "\n",
    "    df_division_care_group['care_group-location'] = df_division_care_group['care_group'] + ' - ' + df_division_care_group['location']\n",
    "\n",
    "    location_counts = df_division_care_group['care_group-location'].value_counts()[df_division_care_group['care_group-location'].unique()]\n",
    "    location_percentages = 100*location_counts/sum(location_counts.values)\n",
    "\n",
    "    for location,percentage in location_percentages.iteritems():\n",
    "        if percentage < 0.701596 + 0.000001:\n",
    "            df_division_care_group.loc[df_division_care_group['care_group-location'] == location, 'care_group-location'] = location.split(\" - \")[0] + ' - Other'\n",
    "\n",
    "    location_counts = df_division_care_group['care_group-location'].value_counts()[df_division_care_group['care_group-location'].unique()]\n",
    "    location_percentages = 100*location_counts/sum(location_counts.values)\n",
    "    location_labels_split = location_counts.index.str.split(\" - \",n=1)\n",
    "    location_labels = [el[1] for el in location_labels_split]\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "    # Create colors\n",
    "    a, b, c, d, e =[plt.cm.Reds, plt.cm.Oranges, plt.cm.Greens, plt.cm.Blues, plt.cm.Purples]\n",
    "    a, c, d, e = [plt.cm.Reds, plt.cm.Greens, plt.cm.Blues, plt.cm.Purples]\n",
    "    division_colour_dict = {'Medicine & Urgent Care':plt.cm.Reds, 'Networked Services':plt.cm.Greens, 'Surgical & Outpatients':plt.cm.Blues, 'Clinical Delivery':plt.cm.Purples}\n",
    "    inner_ring_colours = [division_colour_dict[division] for division in division_labels]\n",
    "\n",
    "    # First Ring (Inside)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('equal')\n",
    "    division_labels_newlines = [i.replace(' ','\\n') for i in division_labels]\n",
    "    division_labels_newlines[division_labels_newlines.index('Surgical\\n&\\nOutpatients')] = 'Surgical &\\nOutpatients'\n",
    "    #mypie, _ = ax.pie(division_counts, radius=2, labels=division_labels_newlines, labeldistance=0.6, colors=[a(0.9), b(0.9), c(0.9), d(0.9), e(0.9)], textprops={'color':'w'})\n",
    "    mypie, _ = ax.pie(division_counts, radius=0.8, labels=division_labels_newlines, labeldistance=0.4, colors=[inner_ring_colours[0](0.9), inner_ring_colours[1](0.9), inner_ring_colours[2](0.9), inner_ring_colours[3](0.9)], textprops={'color':'w'}, startangle=90, counterclock=False)\n",
    "    plt.setp( mypie, width=0.8, edgecolor='white')\n",
    "\n",
    "    # Second Ring (oUTside)\n",
    "    care_group_labels_newlines = [i.replace(' ','\\n') for i in care_group_labels]\n",
    "    care_group_labels_newlines[care_group_labels_newlines.index('Renal\\n&\\nTransplantation')] = 'Renal &\\nTransplantation'\n",
    "    care_group_labels_newlines[care_group_labels_newlines.index('Critical\\nCare,\\nTheatres,\\nAnaesthetics\\n&\\nHSDU')] = 'Critical Care, Theatres,\\nAnaesthetics\\n& HSDU'\n",
    "    #care_group_labels_newlines[care_group_labels_newlines.index('Imaging\\nRadiology')] = 'Imaging Radiology'\n",
    "    #care_group_colours = [a(0.8), a(0.7), a(0.6), a(0.5), b(0.8), c(0.8), c(0.7), c(0.6), d(0.8), d(0.7), d(0.6), e(0.8), e(0.7), e(0.6)]\n",
    "    care_group_colours = [a(0.8), a(0.7), a(0.6), c(0.8), c(0.7), c(0.6), d(0.8), d(0.7), d(0.6), e(0.8), e(0.7), e(0.6)]\n",
    "    care_group_colours = []\n",
    "    for i,index in enumerate(division_counts.index):\n",
    "        df_division = df_division_care_group[df_division_care_group.division == index]\n",
    "        per_division_counts = df_division['care_group'].value_counts()\n",
    "        slice_colour = 8\n",
    "        for j in range(len(per_division_counts)):\n",
    "            care_group_colours.append(inner_ring_colours[i](slice_colour/10))\n",
    "            slice_colour -= 1\n",
    "    mypie2, texts = ax.pie(care_group_counts, radius=2.7, labels=care_group_labels_newlines, labeldistance=0.6, colors=care_group_colours, textprops={'color':'k'},startangle=90, counterclock=False)\n",
    "    plt.setp( mypie2, width=1.9, edgecolor='white')\n",
    "    care_group_label_colours = ['k' if i%2==0 else 'w' for i in range(len(care_group_counts))]\n",
    "    for text, color in zip(texts, care_group_label_colours):\n",
    "        text.set_color(color)\n",
    "    plt.margins(0,0)\n",
    "\n",
    "    # Third Ring (oUTside)\n",
    "    specialty_labels_newlines = [i.replace(' ','\\n') for i in specialty_labels]\n",
    "    specialty_colours = []\n",
    "    for i,div_index in enumerate(division_counts.index):\n",
    "        df_division = df_division_care_group[df_division_care_group.division == div_index]\n",
    "        slice_colour_orig = 0.5\n",
    "        division_care_group_counts = df_division['care_group'].value_counts()[df_division['care_group'].unique()]\n",
    "        for j,care_index in enumerate(division_care_group_counts.index):\n",
    "            df_care_group = df_division[df_division['care_group-specialty'].str.startswith(care_index)]\n",
    "            per_care_group_counts = df_care_group['care_group-specialty'].value_counts()[df_care_group['care_group-specialty'].unique()]\n",
    "            slice_colour = slice_colour_orig - j/10\n",
    "            for k in per_care_group_counts:\n",
    "                specialty_colours.append(inner_ring_colours[i](slice_colour))\n",
    "                slice_colour -= 0.01\n",
    "    mypie3, texts = ax.pie(specialty_counts, radius=3.6, labels=specialty_labels_newlines, labeldistance=0.8, colors=specialty_colours, startangle=90, counterclock=False)\n",
    "    plt.setp( mypie3, width=0.9, edgecolor='white')\n",
    "    specialty_label_colours = ['k' if i%2==0 else 'gray' for i in range(len(specialty_counts))]\n",
    "    for text, color in zip(texts, specialty_label_colours):\n",
    "        text.set_color(color)\n",
    "    plt.margins(0,0)\n",
    "\n",
    "    # Fourth Ring (oUTside)\n",
    "    location_colours = [a(0.5),a(0.49),a(0.48),a(0.47),a(0.4),a(0.39),a(0.38),a(0.37),a(0.36),a(0.35),a(0.34),a(0.33),a(0.32),a(0.31),a(0.30),a(0.29),a(0.28),a(0.27),a(0.26),a(0.25),a(0.24),c(0.50),c(0.49),c(0.48),c(0.47),c(0.46),c(0.45),c(0.40),c(0.39),c(0.38),c(0.37),c(0.36),c(0.30),c(0.29),c(0.28),c(0.27),d(0.50),d(0.40),d(0.39),d(0.38),d(0.37),d(0.30),e(0.50),e(0.49),e(0.48),e(0.47),e(0.46),e(0.45),e(0.44),e(0.40),e(0.39),e(0.38),e(0.37)]\n",
    "    location_colours = []\n",
    "    for i,div_index in enumerate(division_counts.index):\n",
    "        df_division = df_division_care_group[df_division_care_group.division == div_index]\n",
    "        slice_colour_orig = 0.5\n",
    "        division_care_group_counts = df_division['care_group'].value_counts()[df_division['care_group'].unique()]\n",
    "        for j,care_index in enumerate(division_care_group_counts.index):\n",
    "            df_care_group = df_division[df_division['care_group-location'].str.startswith(care_index)]\n",
    "            per_care_group_counts = df_care_group['care_group-location'].value_counts()[df_care_group['care_group-location'].unique()]\n",
    "            slice_colour = slice_colour_orig - j/10\n",
    "            for k in per_care_group_counts:\n",
    "                location_colours.append(inner_ring_colours[i](slice_colour))\n",
    "                slice_colour -= 0.01\n",
    "    mypie4, texts = ax.pie(location_counts, radius=3.9, labels=location_labels, colors=location_colours, startangle=90, counterclock=False)\n",
    "    plt.setp( mypie4, width=0.3, edgecolor='white')\n",
    "    for text, color in zip(texts, location_colours):\n",
    "        text.set_color(color)\n",
    "    plt.margins(0,0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap():\n",
    "    df_heatmap = df_max_feature.copy()\n",
    "    \n",
    "    def find_ward(location):\n",
    "        ward = re.findall(r\"[A-G][0-9]\",location)\n",
    "        if len(ward)>0: return ward[0]\n",
    "        else: return location\n",
    "\n",
    "    def find_ward1to4(location):\n",
    "        ward1to4 = re.findall(r\"[D-G][1-4]\",location)\n",
    "        if len(ward1to4)>0: return ward1to4[0][0]+'3'\n",
    "        else: return location\n",
    "\n",
    "    def find_wardBCF5to7(location):\n",
    "        ward5to7 = re.findall(r\"(B|C|F)[5-7]\",location)\n",
    "        if len(ward5to7)>0: return ward5to7[0][0]+'6'\n",
    "        else: return location\n",
    "\n",
    "    def find_wardEG6to8(location):\n",
    "        ward6to8 = re.findall(r\"(E|G)[6-8]\",location)\n",
    "        if len(ward6to8)>0: return ward6to8[0][0]+'7'\n",
    "        else: return location\n",
    "\n",
    "    def find_acronym(location):\n",
    "        acronym = re.findall(r\"\\(([A-Z]+)\\)\",location)\n",
    "        if len(acronym)>0: return acronym[0]\n",
    "        else: return location\n",
    "\n",
    "    def find_haematology_or_oncology(location):\n",
    "        if 'Haematology' in location or 'Oncology' in location: return 'Haematology & Oncology Centre'\n",
    "        else: return location\n",
    "\n",
    "    def find_ED(location):\n",
    "        if 'Emergency Department' in location: return 'Emergency Department'\n",
    "        else: return location\n",
    "\n",
    "    def find_Ophthalmology(location):\n",
    "        if 'Ophthalmology' in location: return 'Eye'\n",
    "        else: return location\n",
    "\n",
    "    def find_surgical(location):\n",
    "        if 'Surgical' in location: return 'E3'\n",
    "        else: return location\n",
    "\n",
    "    df_heatmap['location'] = np.vectorize(find_ward)(df_heatmap['location'])\n",
    "    df_heatmap['location'] = np.vectorize(find_ward1to4)(df_heatmap['location'])\n",
    "    df_heatmap['location'] = np.vectorize(find_wardBCF5to7)(df_heatmap['location'])\n",
    "    df_heatmap['location'] = np.vectorize(find_wardEG6to8)(df_heatmap['location'])\n",
    "    #df_heatmap['location'] = np.vectorize(find_acronym)(df_heatmap['location'])\n",
    "    df_heatmap['location'] = np.vectorize(find_haematology_or_oncology)(df_heatmap['location'])\n",
    "    df_heatmap['location'] = np.vectorize(find_ED)(df_heatmap['location'])\n",
    "    df_heatmap['location'] = np.vectorize(find_Ophthalmology)(df_heatmap['location'])\n",
    "    df_heatmap['location'] = np.vectorize(find_surgical)(df_heatmap['location'])\n",
    "\n",
    "\n",
    "\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'A7', 'location'] = 'Paediatric Unit'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'A8', 'location'] = 'Paediatric Unit'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Administration Offices Renal & Transplant', 'location'] = 'Renal Unit'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Renal Day Unit  QA', 'location'] = 'Renal Unit'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Anti Coagulation Clinic QA', 'location'] = 'Haematology & Oncology Centre'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Cardiac Catheter Laboratory', 'location'] = 'Cardiology'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Cardiac Day Unit (CDU)', 'location'] = 'Cardiology'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Cardiology Outpatients QA', 'location'] = 'C Level Outpatients'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Childrens Assesment Unit (CAU)', 'location'] = 'Paediatric Unit'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Colorectal Outpatients QA', 'location'] = 'C Level Outpatients'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Diabetes and Endocrinology Centre QA', 'location'] = 'Diabetes and Endocrinology'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'ENT Outpatients QA', 'location'] = 'Ear, Nose & Throat'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Early Pregnancy Assessment Unit', 'location'] = 'A5'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Endoscopy Unit D Level QA', 'location'] = 'Endoscopy Unit'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Eye Day Case Unit QA', 'location'] = 'Eye Unit'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Gastroenterology  Outpatients QA', 'location'] = 'D Level Outpatients'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Gynae Outpatients QA', 'location'] = 'Gynaecology Outpatients'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Haemodialysis Unit', 'location'] = 'Renal Haemodialysis'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Maxillofacial Outpatients QA', 'location'] = 'D Level Outpatients'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Ophthalmology Eye Emergency Department', 'location'] = 'Eye Unit'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Pathology Department', 'location'] = 'Pathology'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Peritoneal Dialysis', 'location'] = 'Renal Unit'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Radiotherapy  Dept within CHOC', 'location'] = 'F6'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Receipt and Delivery', 'location'] = 'Reception'\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Reception (Atrium)', 'location'] = 'Reception'\n",
    "    df_heatmap['location'] = df_heatmap['location'].str.replace(' Unit','')\n",
    "    df_heatmap.loc[df_heatmap['location'] == 'Renal', 'location'] = 'Renal Unit'\n",
    "\n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != \"Administration Offices Women and Children's\"]\n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'Anaesthetics Department'] \n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'Antenatal Clinic'] \n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'Diagnostic Imaging'] \n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'ICT  Centre QA'] \n",
    "    df_heatmap = df_heatmap[~df_heatmap['location'].str.contains('Maternity')] \n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'Observation Ward']\n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'Occupational Health']\n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'Orthopaedic Pre Operation Clinic QA']\n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'Pre-Operative Assessment (POA)']\n",
    "    df_heatmap = df_heatmap[~df_heatmap['location'].str.contains('QA@ Home ')] \n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'Research & Development Department']\n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'Respiratory Day']\n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'TIA Clinic']\n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'Theatres QA']\n",
    "    df_heatmap = df_heatmap[df_heatmap['location'] != 'Transplant and Retrieval  QA']\n",
    "\n",
    "    location_counts = df_heatmap['location'].value_counts().sort_index(ascending=True)\n",
    "\n",
    "    df_heatmap_counts = location_counts.rename_axis('location').reset_index(name='events')\n",
    "\n",
    "    df_heatmap_counts['floor'] = df_heatmap_counts['location'].str[0]\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Acute Medical (AMU) (MAU)', 'floor'] = 'C'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Diabetes and Endocrinology', 'floor'] = 'C'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Discharge', 'floor'] = 'A'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Ear, Nose & Throat', 'floor'] = 'D'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Emergency Department', 'floor'] = 'C'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Endoscopy', 'floor'] = 'D'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Eye', 'floor'] = 'B'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Gynaecology Outpatients', 'floor'] = 'D'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Haematology & Oncology Centre', 'floor'] = 'B'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Medical Physics Department', 'floor'] = 'F'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Paediatric', 'floor'] = 'A'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Pathology', 'floor'] = 'E'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Pharmacy QA', 'floor'] = 'C'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Reception', 'floor'] = 'A'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Rheumatology Outpatients QA', 'floor'] = 'C'\n",
    "    df_heatmap_counts.loc[df_heatmap_counts['location'] == 'Theatre Admissions Suite (TAS)', 'floor'] = 'E'\n",
    "\n",
    "    def find_renal(location,floor):\n",
    "        if 'Renal' in location: return 'G'\n",
    "        else: return floor\n",
    "\n",
    "    df_heatmap_counts['floor'] = np.vectorize(find_renal)(df_heatmap_counts['location'],df_heatmap_counts['floor'])\n",
    "\n",
    "    df_heatmap_counts['location'] = df_heatmap_counts['location'].str.replace(' QA','')\n",
    "\n",
    "    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "    text_dict = {}\n",
    "    floors = ['A','B','C','D','E','F','G']\n",
    "    for floor in floors:\n",
    "        text = pytesseract.image_to_data(Image.open('level_maps_'+floor.lower()+'.png'),output_type='data.frame')\n",
    "        text = text.dropna()\n",
    "        text = text[text.text != ' ']\n",
    "        text = text[text.text != '  ']\n",
    "        text = text[text.text != '   ']\n",
    "        text = text[text.text != '    ']\n",
    "        text = text[text.text != '     ']\n",
    "        text_dict[floor] = text\n",
    "\n",
    "    text_dict['A'].loc[text_dict['A'].text == 'AS,', 'text'] = 'A5'\n",
    "    text_dict['A'].loc[text_dict['A'].text == 'Dischage', 'text'] = 'Discharge'\n",
    "    text_dict['B'].loc[text_dict['B'].text == 'Centre', 'text'] = 'Haematology & Oncology Centre'\n",
    "    text_dict['B'].loc[text_dict['B'].text == '(B8)', 'text'] = 'B8'\n",
    "    text_dict['B'].loc[text_dict['B'].text == 'Neonatal', 'text'] = 'B9'\n",
    "    text_dict['B'].loc[text_dict['B'].text == 'Eye', 'left'] = 1100\n",
    "    text_dict['C'].loc[text_dict['C'].text == 'Day', 'text'] = 'Acute Medical (AMU) (MAU)'\n",
    "    text_dict['C'].loc[text_dict['C'].text == 'Acute Medical (AMU) (MAU)', 'top'] = 354\n",
    "    text_dict['C'].loc[text_dict['C'].text == 'Ambulance', 'left'] = 700\n",
    "    text_dict['C'].loc[text_dict['C'].text == 'Ambulance', 'text'] = 'C Level Outpatients'\n",
    "    text_dict['C'].loc[text_dict['C'].text == 'C Level Outpatients', 'left'] = 50\n",
    "    text_dict['C'].loc[text_dict['C'].text == 'Diabetes', 'text'] = 'Diabetes and Endocrinology'\n",
    "    text_dict['C'].loc[text_dict['C'].text == 'Blood', 'text'] = 'Emergency Department'\n",
    "    text_dict['C'].loc[text_dict['C'].text == 'Emergency Department', 'left'] = 1100\n",
    "    text_dict['C'].loc[text_dict['C'].text == 'Rheumatology', 'text'] = 'Rheumatology Outpatients'\n",
    "    text_dict['D'].loc[text_dict['D'].text == 'Day', 'text'] = 'Day Surgery'\n",
    "    text_dict['D'].loc[text_dict['D'].text == 'Nose', 'text'] = 'Ear, Nose & Throat'\n",
    "    text_dict['D'].loc[text_dict['D'].text == 'Level', 'text'] = 'D Level Outpatients'\n",
    "    text_dict['D'].loc[text_dict['D'].text == 'Gynaecology', 'text'] = 'Gynaecology Outpatients'\n",
    "    text_dict['E'].loc[text_dict['E'].text == 'Theatre', 'text'] = 'Theatre Admissions Suite (TAS)'\n",
    "    text_dict['E'].loc[text_dict['E'].text == 'E5)', 'text'] = 'E5'\n",
    "    text_dict['F'].loc[text_dict['F'].text == 'Medical', 'text'] = 'Medical Physics Department'\n",
    "    text_dict['G'].loc[text_dict['G'].text == 'Unit', 'text'] = 'Renal Unit'\n",
    "    text_dict['G'].loc[text_dict['G'].text == 'G8,', 'text'] = 'G5'\n",
    "    text_dict['G'].loc[text_dict['G'].text == 'G5', 'top'] = 750\n",
    "    text_dict['G'].loc[text_dict['G'].text == 'G5', 'left'] = 700\n",
    "    text_dict['G'].loc[text_dict['G'].text == 'G2,', 'text'] = 'G3'\n",
    "    text_dict['G'].loc[text_dict['G'].text == 'Haemodialysis', 'text'] = 'Renal Haemodialysis'\n",
    "    text_dict['G'].loc[text_dict['G'].text == 'Outpatients', 'text'] = 'Renal Outpatients'\n",
    "\n",
    "    merged_dict = {}\n",
    "    for floor in floors:\n",
    "        merged_dict[floor] = pd.merge(left=df_heatmap_counts,right=text_dict[floor],left_on='location',right_on='text')\n",
    "\n",
    "    cm = plt.cm.get_cmap('RdYlGn_r')\n",
    "    vmin = df_heatmap_counts['events'].min()\n",
    "    vmax = df_heatmap_counts['events'].max()\n",
    "    area_multiplication = 6\n",
    "\n",
    "    for floor in floors:\n",
    "        plt.figure()\n",
    "        img=mpimg.imread('level_maps_'+floor.lower()+'.png')\n",
    "        plt.figure(figsize=(60,6))\n",
    "        imgplot = plt.imshow(img)\n",
    "        fig = plt.gca()\n",
    "        x = merged_dict[floor].left.values + merged_dict[floor].width.values/2\n",
    "        y = merged_dict[floor].top.values + merged_dict[floor].height.values/2\n",
    "        size = merged_dict[floor].events.values*area_multiplication\n",
    "        color = merged_dict[floor].events.values\n",
    "        plt.scatter(x,y,s=size,c=color,vmin=vmin,vmax=vmax,cmap=cm,alpha=0.5)\n",
    "        plt.colorbar(label='# of medicine patient safety events')\n",
    "        fig.get_xaxis().set_visible(False)\n",
    "        fig.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations():\n",
    "    df_correlations = df_max_feature.drop(['inc_notes','inc_actiontaken','max_feature', 'hour', 'weekday', 'month'],axis=1).reset_index()\n",
    "    df_correlations = df_correlations.drop(['recordid'],axis=1)\n",
    "    df_correlations['inc_time'] = df_correlations['inc_time'].str.replace(':','')\n",
    "    df_correlations['inc_submittedtime'] = df_correlations['inc_submittedtime'].str.replace(':','')\n",
    "\n",
    "    df_correlations['inc_year'] = df_correlations.inc_dincident.dt.year\n",
    "    df_correlations['inc_month'] = df_correlations.inc_dincident.dt.month\n",
    "    df_correlations['inc_day'] = df_correlations.inc_dincident.dt.day\n",
    "    df_correlations['inc_week'] = df_correlations.inc_dincident.dt.week\n",
    "    df_correlations['inc_weekday'] = df_correlations.inc_dincident.dt.weekday\n",
    "    df_correlations['inc_quarter'] = df_correlations.inc_dincident.dt.quarter\n",
    "    df_correlations['inc_hour'] = df_correlations['inc_time'].str[:2]\n",
    "    df_correlations['inc_minute'] = df_correlations['inc_time'].str[2:4]\n",
    "    df_correlations = df_correlations.astype({'inc_hour':'int'})\n",
    "    df_correlations = df_correlations.astype({'inc_minute':'int'})\n",
    "    df_correlations['inc_datetime'] = df_correlations.apply(lambda x: x.loc['inc_dincident'] + dt.timedelta(hours=x.loc['inc_hour'],minutes=x.loc['inc_minute']), axis=1)\n",
    "    df_correlations['reported_year'] = df_correlations.inc_dreported.dt.year\n",
    "    df_correlations['reported_month'] = df_correlations.inc_dreported.dt.month\n",
    "    df_correlations['reported_day'] = df_correlations.inc_dreported.dt.day\n",
    "    df_correlations['reported_week'] = df_correlations.inc_dreported.dt.week\n",
    "    df_correlations['reported_weekday'] = df_correlations.inc_dreported.dt.weekday\n",
    "    df_correlations['reported_quarter'] = df_correlations.inc_dreported.dt.quarter\n",
    "    df_correlations['reported_hour'] = df_correlations['inc_submittedtime'].str[:2]\n",
    "    df_correlations['reported_minute'] = df_correlations['inc_submittedtime'].str[2:4]\n",
    "    df_correlations = df_correlations.astype({'reported_hour':'int'})\n",
    "    df_correlations = df_correlations.astype({'reported_minute':'int'})\n",
    "    df_correlations['reported_datetime'] = df_correlations.apply(lambda x: x.loc['inc_dreported'] + dt.timedelta(hours=x.loc['reported_hour'],minutes=x.loc['reported_minute']), axis=1)\n",
    "    df_correlations['diff'] = df_correlations['reported_datetime'] - df_correlations['inc_datetime']\n",
    "    df_correlations['reported-incident'] = df_correlations['diff'] / np.timedelta64(1, 'D')\n",
    "    df_correlations.head()\n",
    "    df_correlations = df_correlations.drop(['inc_dincident','inc_time','inc_dreported','inc_submittedtime','inc_datetime','reported_datetime','diff'],axis=1)\n",
    "\n",
    "    def cramers_v(x, y):\n",
    "        \"\"\"\n",
    "        Calculates Cramer's V statistic for categorical-categorical association.\n",
    "        Uses correction from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328.\n",
    "        This is a symmetric coefficient: V(x,y) = V(y,x)\n",
    "        Original function taken from: https://stackoverflow.com/a/46498792/5863503\n",
    "        Wikipedia: https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n",
    "        **Returns:** float in the range of [0,1]\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : list / NumPy ndarray / Pandas Series\n",
    "            A sequence of categorical measurements\n",
    "        y : list / NumPy ndarray / Pandas Series\n",
    "            A sequence of categorical measurements\n",
    "        \"\"\"\n",
    "        confusion_matrix = pd.crosstab(x,y)\n",
    "        chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "        n = confusion_matrix.sum().sum()\n",
    "        phi2 = chi2/n\n",
    "        r,k = confusion_matrix.shape\n",
    "        phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "        rcorr = r-((r-1)**2)/(n-1)\n",
    "        kcorr = k-((k-1)**2)/(n-1)\n",
    "        return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
    "\n",
    "\n",
    "    def theils_u(x, y):\n",
    "        \"\"\"\n",
    "        Calculates Theil's U statistic (Uncertainty coefficient) for categorical-categorical association.\n",
    "        This is the uncertainty of x given y: value is on the range of [0,1] - where 0 means y provides no information about\n",
    "        x, and 1 means y provides full information about x.\n",
    "        This is an asymmetric coefficient: U(x,y) != U(y,x)\n",
    "        Wikipedia: https://en.wikipedia.org/wiki/Uncertainty_coefficient\n",
    "        **Returns:** float in the range of [0,1]\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : list / NumPy ndarray / Pandas Series\n",
    "            A sequence of categorical measurements\n",
    "        y : list / NumPy ndarray / Pandas Series\n",
    "            A sequence of categorical measurements\n",
    "        \"\"\"\n",
    "        s_xy = conditional_entropy(x,y)\n",
    "        x_counter = Counter(x)\n",
    "        total_occurrences = sum(x_counter.values())\n",
    "        p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n",
    "        s_x = ss.entropy(p_x)\n",
    "        if s_x == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return (s_x - s_xy) / s_x\n",
    "\n",
    "\n",
    "    def correlation_ratio(categories, measurements):\n",
    "        \"\"\"\n",
    "        Calculates the Correlation Ratio (sometimes marked by the greek letter Eta) for categorical-continuous association.\n",
    "        Answers the question - given a continuous value of a measurement, is it possible to know which category is it\n",
    "        associated with?\n",
    "        Value is in the range [0,1], where 0 means a category cannot be determined by a continuous measurement, and 1 means\n",
    "        a category can be determined with absolute certainty.\n",
    "        Wikipedia: https://en.wikipedia.org/wiki/Correlation_ratio\n",
    "        **Returns:** float in the range of [0,1]\n",
    "        Parameters\n",
    "        ----------\n",
    "        categories : list / NumPy ndarray / Pandas Series\n",
    "            A sequence of categorical measurements\n",
    "        measurements : list / NumPy ndarray / Pandas Series\n",
    "            A sequence of continuous measurements\n",
    "        \"\"\"\n",
    "        #categories = convert(categories, 'array')\n",
    "        #measurements = convert(measurements, 'array')\n",
    "        fcat, _ = pd.factorize(categories)\n",
    "        cat_num = np.max(fcat)+1\n",
    "        y_avg_array = np.zeros(cat_num)\n",
    "        n_array = np.zeros(cat_num)\n",
    "        for i in range(0,cat_num):\n",
    "            cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "            n_array[i] = len(cat_measures)\n",
    "            y_avg_array[i] = np.average(cat_measures)\n",
    "        y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n",
    "        numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n",
    "        denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n",
    "        if numerator == 0:\n",
    "            eta = 0.0\n",
    "        else:\n",
    "            eta = np.sqrt(numerator/denominator)\n",
    "        return eta\n",
    "\n",
    "\n",
    "    def associations(dataset, nominal_columns=None, mark_columns=False, theil_u=False, plot=True,\n",
    "                              return_results = False, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate the correlation/strength-of-association of features in data-set with both categorical (eda_tools) and\n",
    "        continuous features using:\n",
    "         * Pearson's R for continuous-continuous cases\n",
    "         * Correlation Ratio for categorical-continuous cases\n",
    "         * Cramer's V or Theil's U for categorical-categorical cases\n",
    "        **Returns:** a DataFrame of the correlation/strength-of-association between all features\n",
    "        **Example:** see `associations_example` under `dython.examples`\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : NumPy ndarray / Pandas DataFrame\n",
    "            The data-set for which the features' correlation is computed\n",
    "        nominal_columns : string / list / NumPy ndarray\n",
    "            Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all\n",
    "            columns are categorical, or None (default) to state none are categorical\n",
    "        mark_columns : Boolean, default = False\n",
    "            if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on there type (eda_tools or\n",
    "            continuous), as provided by nominal_columns\n",
    "        theil_u : Boolean, default = False\n",
    "            In the case of categorical-categorical feaures, use Theil's U instead of Cramer's V\n",
    "        plot : Boolean, default = True\n",
    "            If True, plot a heat-map of the correlation matrix\n",
    "        return_results : Boolean, default = False\n",
    "            If True, the function will return a Pandas DataFrame of the computed associations\n",
    "        kwargs : any key-value pairs\n",
    "            Arguments to be passed to used function and methods\n",
    "        \"\"\"\n",
    "        #dataset = convert(dataset, 'dataframe')\n",
    "        columns = dataset.columns\n",
    "        corr = pd.DataFrame(index=columns, columns=columns)\n",
    "        for i in range(0,len(columns)):\n",
    "            for j in range(i,len(columns)):\n",
    "                if i == j:\n",
    "                    corr[columns[i]][columns[j]] = 0.0\n",
    "                else:\n",
    "                    if columns[i] in nominal_columns:\n",
    "                        if columns[j] in nominal_columns:\n",
    "                            if theil_u:\n",
    "                                corr[columns[j]][columns[i]] = theils_u(dataset[columns[i]],dataset[columns[j]])\n",
    "                                corr[columns[i]][columns[j]] = theils_u(dataset[columns[j]],dataset[columns[i]])\n",
    "                            else:\n",
    "                                cell = cramers_v(dataset[columns[i]],dataset[columns[j]])\n",
    "                                corr[columns[i]][columns[j]] = cell\n",
    "                                corr[columns[j]][columns[i]] = cell\n",
    "                        else:\n",
    "                            cell = correlation_ratio(dataset[columns[i]], dataset[columns[j]])\n",
    "                            corr[columns[i]][columns[j]] = cell\n",
    "                            corr[columns[j]][columns[i]] = cell\n",
    "                    else:\n",
    "                        if columns[j] in nominal_columns:\n",
    "                            cell = correlation_ratio(dataset[columns[j]], dataset[columns[i]])\n",
    "                            corr[columns[i]][columns[j]] = cell\n",
    "                            corr[columns[j]][columns[i]] = cell\n",
    "                        else:\n",
    "                            cell, _ = ss.pearsonr(dataset[columns[i]], dataset[columns[j]])\n",
    "                            corr[columns[i]][columns[j]] = cell\n",
    "                            corr[columns[j]][columns[i]] = cell\n",
    "        corr.fillna(value=np.nan, inplace=True)\n",
    "        if mark_columns:\n",
    "            marked_columns = ['{} (nom)'.format(col) if col in nominal_columns else '{} (con)'.format(col) for col in columns]\n",
    "            corr.columns = marked_columns\n",
    "            corr.index = marked_columns\n",
    "        if plot:\n",
    "            plt.figure(figsize=kwargs.get('figsize',None))\n",
    "            labels = np.round(np.array(corr),decimals=1).astype(str)\n",
    "            for i in range(0,len(columns)):\n",
    "                for j in range(i,len(columns)):\n",
    "                    if j>=i: labels[i][j] = ''\n",
    "            ax = sns.heatmap(corr, annot=labels, fmt='')\n",
    "            cbar = ax.collections[0].colorbar\n",
    "            cbar.set_ticks([-0.2, 0, 0.5, 0.9])\n",
    "            cbar.set_ticklabels(['-0.2 anti-associated', '0 no association', '0.5 associated', '0.9 closely associated'])\n",
    "            plt.title('Amount of association between variables. 1:fully associated, 0:no association, -1:fully anti-associated')\n",
    "            plt.show()\n",
    "        if return_results:\n",
    "            for i in range(0,len(columns)):\n",
    "                for j in range(i,len(columns)):\n",
    "                    if j>=i: \n",
    "                        corr[columns[i]][columns[j]] = 0.0\n",
    "            return corr\n",
    "\n",
    "    columns_list = df_correlations.columns.values\n",
    "    columns_list = [el.replace('inc_','') for el in columns_list]\n",
    "    columns_list = [el.replace('location','ward/dept/unit') for el in columns_list]\n",
    "    columns_list = [el.replace('loctype','location_type') for el in columns_list]\n",
    "    columns_list = [el.replace('show_other_contacts','any_other_patient_involved') for el in columns_list]\n",
    "    columns_list = [el.replace('show_employee','any_other_employee_involved') for el in columns_list]\n",
    "    columns_list = [el.replace('show_witness','any_witness') for el in columns_list]\n",
    "    columns_list = [el.replace('show_document','any_document_attached') for el in columns_list]\n",
    "    columns_list = [el.replace('rep_email','email') for el in columns_list]\n",
    "    columns_list = [el.replace('reportedby','role') for el in columns_list]\n",
    "    df_correlations.columns=columns_list\n",
    "    nominal_columns = df_correlations.select_dtypes('O').columns.values\n",
    "    #associations(df_correlations,nominal_columns,figsize=(11,11))\n",
    "\n",
    "    df_correlations_new = df_correlations.drop(['reported_year','reported_month','reported_day','reported_week','reported_weekday','reported_quarter','reported_hour','reported_minute','day','week','quarter','minute','any_other_patient_involved','any_other_employee_involved','any_witness','any_document_attached','employee_involved','email'],axis=1)\n",
    "    nominal_columns = df_correlations_new.select_dtypes('O').columns.values\n",
    "    returned_results = associations(df_correlations_new,nominal_columns,figsize=(11,11),return_results=True,plot=False)\n",
    "\n",
    "    df_association = pd.DataFrame(columns=['row','col','association','description','interest'])\n",
    "    while returned_results.values.max() > 0.2:\n",
    "        for row in list(returned_results.index):\n",
    "            for col in returned_results.columns:\n",
    "                if returned_results.at[row,col] == returned_results.values.max():\n",
    "                    df_association = df_association.append({'row':row,'col':col,'association':returned_results.values.max()},ignore_index=True)\n",
    "                    returned_results.at[row,col] = 0\n",
    "                    break\n",
    "\n",
    "    df_association = df_association.astype({'description':'object','interest':'object'})\n",
    "\n",
    "    df_association.loc[(df_association['row']=='result') & (df_association['col']=='severity'),'description'] = \"both contain options for 'near miss' and 'no harm'\"\n",
    "    df_association.loc[(df_association['row']=='result') & (df_association['col']=='severity'),'interest'] = \"expected\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='ward/dept/unit'),'description'] = \"wards/depts/units are within specific divsions & care groups\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='ward/dept/unit'),'interest'] = \"expected\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='specialty'),'description'] = \"the fact this isn't 1.0 shows that there are outliers outside of their care group\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='specialty'),'interest'] = \"interesting\"\n",
    "    df_association.loc[(df_association['row']=='email') & (df_association['col']=='year'),'description'] = \"recently, some staff have been reporting more\"\n",
    "    df_association.loc[(df_association['row']=='email') & (df_association['col']=='year'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='role') & (df_association['col']=='year'),'description'] = \"recently, some roles have been reporting more\"\n",
    "    df_association.loc[(df_association['row']=='role') & (df_association['col']=='year'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='email') & (df_association['col']=='hour'),'description'] = \"some staff are more likely to see incidents at certain hours\"\n",
    "    df_association.loc[(df_association['row']=='email') & (df_association['col']=='hour'),'interest'] = \"interesting\"\n",
    "    df_association.loc[(df_association['row']=='role') & (df_association['col']=='email'),'description'] = \"some staff are in multiple roles\"\n",
    "    df_association.loc[(df_association['row']=='role') & (df_association['col']=='email'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='email') & (df_association['col']=='reported-incident'),'description'] = \"some staff take longer to report after the incident\"\n",
    "    df_association.loc[(df_association['row']=='email') & (df_association['col']=='reported-incident'),'interest'] = \"interesting\"\n",
    "    df_association.loc[(df_association['row']=='email') & (df_association['col']=='weekday'),'description'] = \"some staff are more likely to see incidents on certain weekdays\"\n",
    "    df_association.loc[(df_association['row']=='email') & (df_association['col']=='weekday'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='email') & (df_association['col']=='month'),'description'] = \"some staff are more likely to see incidents on certain months\"\n",
    "    df_association.loc[(df_association['row']=='email') & (df_association['col']=='month'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='specialty'),'description'] = \"the fact this isn't 1.0 shows that there are outliers outside of their ward/dept/unit\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='specialty'),'interest'] = \"interesting\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='email'),'description'] = \"some divisions & care groups have more staff reporting\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='email'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='email'),'description'] = \"some location types have more staff reporting\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='email'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='location_type'),'description'] = \"some location types are more likely to be in specific wards/depts/units\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='location_type'),'interest'] = \"expected\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='email'),'description'] = \"some specialties have more staff reporting\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='email'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='location_type'),'description'] = \"some location types are more likely to be in specific divisions & care groups\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='location_type'),'interest'] = \"expected\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='email'),'description'] = \"some wards/depts/units have more staff reporting\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='email'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='reported-incident'),'description'] = \"some wards/depts/units take longer to report after the incident\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='reported-incident'),'interest'] = \"interesting\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='location_type'),'description'] = \"some location types are more likely to be in specific specialties\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='location_type'),'interest'] = \"expected\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='reported-incident'),'description'] = \"some location types take longer to report after the incident\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='reported-incident'),'interest'] = \"interesting\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='reported-incident'),'description'] = \"some divisions & care groups take longer to report after the incident\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='reported-incident'),'interest'] = \"interesting\"\n",
    "    df_association.loc[(df_association['row']=='severity') & (df_association['col']=='email'),'description'] = \"some staff see more severe incidents\"\n",
    "    df_association.loc[(df_association['row']=='severity') & (df_association['col']=='email'),'interest'] = \"interesting\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='year'),'description'] = \"some staff see incidents with specific results\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='year'),'interest'] = \"interesting\"\n",
    "    df_association.loc[(df_association['row']=='result') & (df_association['col']=='email'),'description'] = \"recently, some wards/depts/units have had more incidents\"\n",
    "    df_association.loc[(df_association['row']=='result') & (df_association['col']=='email'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='role'),'description'] = \"some roles see incidents in specific divisions & care groups\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='role'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='role'),'description'] = \"some roles see incidents in specific wards/depts/units\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='role'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='role'),'description'] = \"some roles see incidents in specific specialties\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='role'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='role'),'description'] = \"some roles see incidents in specific location types\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='role'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='year'),'description'] = \"recently, some specialties have had more incidents\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='year'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='result'),'description'] = \"some wards/depts/units see incidents with specific results\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='result'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='weekday'),'description'] = \"some wards/depts/units see incidents on specific weekdays\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='weekday'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='month'),'description'] = \"some wards/depts/units see incidents in specific months\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='month'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='hour'),'description'] = \"some wards/depts/units see incidents during specific hours\"\n",
    "    df_association.loc[(df_association['row']=='ward/dept/unit') & (df_association['col']=='hour'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='result') & (df_association['col']=='role'),'description'] = \"some roles see incidents with specific results\"\n",
    "    df_association.loc[(df_association['row']=='result') & (df_association['col']=='role'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='result'),'description'] = \"some specialties see incidents with specific results\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='result'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='result'),'description'] = \"some divisions & care groups see incidents with specific results\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='result'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='reported-incident'),'description'] = \"some specialties take longer to report after the incident\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='reported-incident'),'interest'] = \"interesting\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='result'),'description'] = \"some specialties see incidents in specific months\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='result'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='role') & (df_association['col']=='reported-incident'),'description'] = \"some roles take longer to report after the incident\"\n",
    "    df_association.loc[(df_association['row']=='role') & (df_association['col']=='reported-incident'),'interest'] = \"interesting\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='month'),'description'] = \"some locationt types see incidents with specific results\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='month'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='weekday'),'description'] = \"some specialties see incidents on specific weekdays\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='weekday'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='year'),'description'] = \"recently, some location types have had more incidents\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='year'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='hour'),'description'] = \"some specialties see incidents during specific hours\"\n",
    "    df_association.loc[(df_association['row']=='specialty') & (df_association['col']=='hour'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='role') & (df_association['col']=='weekday'),'description'] = \"some roles see incidents on specific weekdays\"\n",
    "    df_association.loc[(df_association['row']=='role') & (df_association['col']=='weekday'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='weekday'),'description'] = \"some location types see incidents on specific weekdays\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='weekday'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='hour'),'description'] = \"some location types see incidents during specific hours\"\n",
    "    df_association.loc[(df_association['row']=='location_type') & (df_association['col']=='hour'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='role') & (df_association['col']=='month'),'description'] = \"some roles see incidents in specific months\"\n",
    "    df_association.loc[(df_association['row']=='role') & (df_association['col']=='month'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='year'),'description'] = \"recently, some divisions & care groups have had more incidents\"\n",
    "    df_association.loc[(df_association['row']=='division-care_group') & (df_association['col']=='year'),'interest'] = \"interesting?\"\n",
    "    df_association.loc[(df_association['row']=='severity') & (df_association['col']=='role'),'description'] = \"some roles see more severe incidents\"\n",
    "    df_association.loc[(df_association['row']=='severity') & (df_association['col']=='role'),'interest'] = \"interesting\"\n",
    "\n",
    "    cm = sns.cm.rocket\n",
    "    color = df_association['association']\n",
    "    size = np.ones(len(df_association))*200\n",
    "    plt.figure(figsize=(17,df_association.index.max()/3))\n",
    "    plt.ylim(df_association.index.max()+1,-1)\n",
    "    plt.xlim(0.2,2.2)\n",
    "    plt.scatter(np.ones(len(df_association)),df_association.index,marker='s',c=color,cmap=cm,s=size)\n",
    "    plt.scatter(np.ones(len(df_association))*0.6,df_association.index,marker='*')\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.gca().get_yaxis().set_visible(False)\n",
    "    for i in df_association.index:\n",
    "        number_color = 'w' if df_association.at[i,'association']<0.66 else 'k'\n",
    "        plt.text(1,i,round(df_association.at[i,'association'],1),ha='center',va='center',color=number_color)\n",
    "        plt.text(0.3,i,df_association.at[i,'row'],ha='left',va='center')\n",
    "        plt.text(0.7,i,df_association.at[i,'col'],ha='left',va='center')\n",
    "        plt.text(1.1,i,df_association.at[i,'description'],ha='left',va='center')\n",
    "        interest_color = 'r' if df_association.at[i,'interest']=='expected' else 'g'\n",
    "        plt.text(2.0,i,df_association.at[i,'interest'],ha='left',va='center',color=interest_color)\n",
    "        plt.axhline(i+0.5,color='k',linewidth=1)\n",
    "    plt.show()\n",
    "    \n",
    "    del df_correlations,df_correlations_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes():\n",
    "    df_nb = df_max_feature.drop(['inc_dincident','inc_time','inc_dreported','inc_submittedtime','max_feature','hour','weekday','month','inc_rep_email','reported_hour'],axis=1).copy()\n",
    "    df_nb['division-care_group'] = df_nb['division-care_group'].str.replace(' and ',' & ')\n",
    "    divider = df_nb['division-care_group'].str.split(\" Division - \",n=1,expand=True)\n",
    "    df_nb['division'] = divider[0]\n",
    "    df_nb['care group'] = divider[1]\n",
    "    df_nb = df_nb.drop('division-care_group',axis=1)\n",
    "\n",
    "    df_nb.columns = [column.replace('inc_organisation','site') for column in df_nb.columns]\n",
    "    df_nb.columns = [column.replace('inc_locactual','ward/dept/unit') for column in df_nb.columns]\n",
    "    df_nb.columns = [column.replace('inc_specialty','specialty') for column in df_nb.columns]\n",
    "    df_nb.columns = [column.replace('inc_loctype','location type') for column in df_nb.columns]\n",
    "    df_nb.columns = [column.replace('inc_result','result') for column in df_nb.columns]\n",
    "    df_nb.columns = [column.replace('inc_severity','severity') for column in df_nb.columns]\n",
    "    df_nb.columns = [column.replace('inc_reportedby','reported by') for column in df_nb.columns]\n",
    "    df_nb.columns = [column.replace('show_other_contacts','other patients involved?') for column in df_nb.columns]\n",
    "    df_nb.columns = [column.replace('show_employee','other employees involved?') for column in df_nb.columns]\n",
    "    df_nb.columns = [column.replace('show_witness','any witnesses?') for column in df_nb.columns]\n",
    "    df_nb.columns = [column.replace('show_document','any documents attached?') for column in df_nb.columns]\n",
    "    df_nb.head()\n",
    "\n",
    "    category_columns = list(df_nb.columns.values)\n",
    "    category_columns.remove('inc_notes')\n",
    "    count_accuracies = []\n",
    "    tfidf_accuracies = []\n",
    "    for column in category_columns:\n",
    "        df_nb_dropped = df_nb.dropna(subset=[column])\n",
    "        y = df_nb_dropped[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        df_nb_dropped['inc_notes'], y,\n",
    "                                        test_size=0.33,\n",
    "                                        random_state=53)\n",
    "\n",
    "\n",
    "        # Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "        tfidf_vectorizer = CustomVectorizer(tokenizer=LemmaTokenizer())\n",
    "\n",
    "        # Transform the training data: tfidf_train \n",
    "        tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "        # Transform the test data: tfidf_test \n",
    "        tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "        # Print the first 10 features\n",
    "        #print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "        # Print the first 5 vectors of the tfidf training data\n",
    "        #print(tfidf_train.A[:5])\n",
    "\n",
    "\n",
    "        # Create the CountVectorizer DataFrame: count_df_nb\n",
    "        #count_df_nb = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "        # Create the TfidfVectorizer DataFrame: tfidf_df_nb\n",
    "        #tfidf_df_nb = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "        # Print the head of count_df_nb\n",
    "        #print(count_df_nb.head())\n",
    "\n",
    "        # Print the head of tfidf_df_nb\n",
    "        #print(tfidf_df_nb.head())\n",
    "\n",
    "        # Calculate the difference in columns: difference\n",
    "        #difference = set(count_df_nb.columns) - set(tfidf_df_nb.columns)\n",
    "        #print(difference)\n",
    "\n",
    "        # Check whether the DataFrames are equal\n",
    "        #print(count_df_nb.equals(tfidf_df_nb))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        tfidf_nb_classifier = MultinomialNB()\n",
    "        tfidf_nb_classifier.fit(tfidf_train, y_train)\n",
    "        tfidf_pred = tfidf_nb_classifier.predict(tfidf_test)\n",
    "        tfidf_accuracies.append(100*metrics.accuracy_score(y_test,tfidf_pred))    \n",
    "\n",
    "\n",
    "\n",
    "    tfidf_from50 = [int(round(accuracy/2)) for accuracy in tfidf_accuracies]\n",
    "    n_category_columns = len(category_columns)\n",
    "    colors = [['green' if tfidf_from50[j]>i else 'red' for j in range(n_category_columns)] for i in range(50)]\n",
    "    for i in range(50):\n",
    "        plt.scatter(x=np.ones(len(category_columns))*(i+1),y=category_columns,color=colors[i])\n",
    "    plt.xlim((0,51));\n",
    "    fig1 = plt.figure(1)\n",
    "    fig1.text(0, 0.95, \"Correct\", ha=\"center\", va=\"bottom\", size=\"large\", color=\"green\");\n",
    "    fig1.text(0.06, 0.95, \"/\", ha=\"center\", va=\"bottom\", size=\"large\");\n",
    "    fig1.text(0.13,0.95,\"Incorrect\", ha=\"center\", va=\"bottom\", size=\"large\", color=\"red\");\n",
    "    fig1.text(0.19, 0.95, \" prediction from freetext entry for medicine patient safety events\", va=\"bottom\", size=\"large\");\n",
    "    \n",
    "    non_crammed_columns = [column for column in category_columns]\n",
    "    non_crammed_columns.remove('location')\n",
    "    non_crammed_columns.remove('specialty')\n",
    "    #non_crammed_columns.remove('reported by')\n",
    "    \n",
    "    tfidf_accuracies = []\n",
    "    for column in non_crammed_columns:\n",
    "        df_dropped = df_nb.dropna(subset=[column])\n",
    "        df_dropped = df_dropped[df_dropped[column]!='']\n",
    "        classes = [str(i) for i in df_dropped[column].value_counts().index]\n",
    "        mapping = dict((el,i) for i,el in enumerate(classes)) \n",
    "        df_replaced = df_dropped.replace({column: mapping})\n",
    "        y = df_replaced[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        df_dropped['inc_notes'], y,\n",
    "                                        test_size=0.33,\n",
    "                                        random_state=53)\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "        tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "        tfidf_nb_classifier = MultinomialNB()\n",
    "        tfidf_nb_classifier.fit(tfidf_train, y_train)\n",
    "        tfidf_pred = tfidf_nb_classifier.predict(tfidf_test)\n",
    "        tfidf_accuracies.append(100*metrics.accuracy_score(y_test,tfidf_pred))\n",
    "\n",
    "        cm = metrics.confusion_matrix(y_test, tfidf_pred, labels=list(mapping.values()))\n",
    "    \n",
    "        plt.figure();\n",
    "        # https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "        fig, ax = plt.subplots(figsize=(11,11));\n",
    "        im = ax.imshow(cm, cmap=plt.cm.Blues);\n",
    "        # create an axes on the right side of ax. The width of cax will be 5%\n",
    "        # of ax and the padding between cax and ax will be fixed at 0.05 inch.\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        ax.figure.colorbar(im, cax=cax)\n",
    "        #plt.title('True vs Predicted \"'+str(column)+'\" from freetext of medicine patient safety events',x=1.2)        \n",
    "        # We want to show all ticks...\n",
    "        ax.set(xticks=np.arange(cm.shape[1]),\n",
    "               yticks=np.arange(cm.shape[0]),\n",
    "               # ... and label them with the respective list entries\n",
    "               xticklabels=classes, yticklabels=classes,\n",
    "               title='True vs Predicted \"'+str(column)+'\" from freetext of medicine patient safety events',\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label');\n",
    "        ax.xaxis.set_label_coords(1.09, -0.02);\n",
    "\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\");\n",
    "\n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        fmt = 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], fmt),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\");\n",
    "        fig.tight_layout();\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "donut_seg = 0\n",
    "for i,counts in theme_counts_series.items():\n",
    "    # Initialize the word cloud\n",
    "    width = int(1024*counts/theme_counts_max)\n",
    "    height = int(720*counts/theme_counts_max)\n",
    "    wc = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        width = width,\n",
    "        height = height\n",
    "    )\n",
    "\n",
    "    # Select row : component\n",
    "    component = components_df.iloc[i]\n",
    "\n",
    "    # Generate the cloud\n",
    "    component.nlargest().index = component.nlargest().index.map(str)\n",
    "    wc.generate_from_frequencies(component.nlargest())\n",
    "    wordcloud_words.append(component.nlargest().index)\n",
    "\n",
    "    # Display the generated image:\n",
    "    figure, (wc_fig, counts_fig) = plt.subplots(nrows=1,ncols=2, figsize=(width/50,height/100))\n",
    "    wc_fig.imshow(wc, interpolation='bilinear')\n",
    "    wc_fig.axis(\"off\");\n",
    "\n",
    "    counts_fig.axis('equal')\n",
    "    colors = ['w' for j in theme_counts_series.index]\n",
    "    colors[donut_seg] = 'b'\n",
    "    labels = ['' for val in theme_counts_series.values]\n",
    "    labels[donut_seg] = str(counts)+\"/\"+str(theme_counts_series.values.sum())\n",
    "    donut_seg += 1\n",
    "    mypie, _ = counts_fig.pie(theme_counts_series.values/theme_counts_max, colors=colors, labels=labels, startangle=90, counterclock=False)\n",
    "    plt.setp( mypie, width=0.4, edgecolor='black')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_max_feature = df[df['max_feature']==str(i)].copy()\n",
    "    \n",
    "    reportedby_plot()\n",
    "    \n",
    "    reporter_email()\n",
    "    \n",
    "    staff_involved_role()\n",
    "    \n",
    "    time_trend_plot()\n",
    "    \n",
    "    month_plot()\n",
    "    \n",
    "    day_plot()\n",
    "    \n",
    "    weekday_plot()\n",
    "\n",
    "    hour_plot()\n",
    "    plt.savefig('hour_'+str(i)+'.pdf')\n",
    "    \n",
    "    division_care_group_loc_plot()\n",
    "    \n",
    "    heatmap()\n",
    "    \n",
    "    correlations()\n",
    "    \n",
    "    #naive_bayes()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    print('-------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_19 = df[df['inc_notes'].str.contains(' wa ')]\n",
    "print(df_19['inc_notes'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
