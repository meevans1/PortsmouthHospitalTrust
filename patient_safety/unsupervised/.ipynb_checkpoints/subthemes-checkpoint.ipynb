{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with loading all necessary libraries\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_conn = pyodbc.connect('DRIVER={SQL Server};'\n",
    "                            'SERVER=L_AAGname;'\n",
    "                            'DATABASE=database_name;'\n",
    "                            'Trusted_Connection=yes') \n",
    "query = \"set transaction isolation level read uncommitted select b.description, c.description, d.description, a.inc_dincident,a.inc_time,a.inc_dreported,a.inc_submittedtime,a.inc_loctype,a.inc_result,a.inc_severity,a.show_other_contacts,a.show_employee,a.show_witness,a.show_document,a.inc_reportedby,a.inc_rep_email,a.inc_notes,a.inc_actiontaken from DatixCRM.dbo.code_unit b join DatixCRM.dbo.incidents_main a on a.inc_unit = b.code join DatixCRM.dbo.code_locactual c on a.inc_locactual = c.code join DatixCRM.dbo.code_specialty d on a.inc_specialty = d.code where a.inc_type='PAT' and a.inc_category='MEDIC' and a.inc_organisation='QA' and c.cod_parent2='QA'\"\n",
    "df = pd.read_sql(query, sql_conn)\n",
    "column_list = list(df.columns)\n",
    "column_list[0] = 'division-care_group'\n",
    "column_list[1] = 'location'\n",
    "column_list[2] = 'specialty'\n",
    "df['inc_rep_email'] = df['inc_rep_email'].str.split(\"@\",n=1,expand=True)[0]\n",
    "df.columns = column_list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.index < 1000]\n",
    "# index < 3491 is the highest number that doesn't give memory error during n_components loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_caps_series = df[~df['inc_notes'].str.isupper()]['inc_notes'].str.findall(r\"\\b[a-z]{2,}\\b\\s\\b[A-Z]{2,}\\b\\s\\b[a-z]{2,}\\b\")\n",
    "consecutive_caps_series = consecutive_caps_series[consecutive_caps_series.map(lambda d: len(d)) > 0]\n",
    "slist = []\n",
    "for x in consecutive_caps_series:\n",
    "    slist.extend(x)\n",
    "\n",
    "# function to get unique values \n",
    "def unique(list1): \n",
    "  \n",
    "    # intilize a null list \n",
    "    unique_list = [] \n",
    "      \n",
    "    # traverse for all elements \n",
    "    for x in list1: \n",
    "        # check if exists in unique_list or not \n",
    "        if x.split()[1] not in unique_list: \n",
    "            unique_list.append(x.split()[1])\n",
    "    return [string.lower() for string in unique_list]\n",
    "        \n",
    "abbreviations = [l.split()[1].lower() for l in slist]\n",
    "abbreviations.remove('morps')\n",
    "abbreviation_counts = Counter(abbreviations)\n",
    "print(abbreviation_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomedct = pd.read_csv('sct2_Description_Snapshot-en_INT_20190731.txt',sep=\"\\t\",usecols=['term'])\n",
    "medical_terms_series = snomedct['term'].str.lower().str.split().dropna()\n",
    "medical_terms_list = []\n",
    "for x in medical_terms_series:\n",
    "    medical_terms_list.extend(x)\n",
    "medical_terms_list = [medical_term for medical_term in medical_terms_list if medical_term.isalpha()]\n",
    "medical_terms_list = [medical_term.strip(\"()\") for medical_term in medical_terms_list]\n",
    "medical_terms_list = [medical_term.strip(\"(s\") for medical_term in medical_terms_list]\n",
    "medical_terms_counts = Counter(medical_terms_list)\n",
    "print(medical_terms_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_caps_series_location = df['location'].str.findall(r\"((?:\\b[A-Za-z&]+\\b\\s)+\\([A-Za-z][A-Za-z]+\\))\")\n",
    "consecutive_caps_series_notes = df['inc_notes'].str.findall(r\"((?:\\b[A-Za-z]+\\b\\s)+\\([A-Za-z][A-Za-z]+\\))\")\n",
    "consecutive_caps_series_action = df['inc_actiontaken'].str.findall(r\"((?:\\b[A-Za-z]+\\b\\s)+\\([A-Za-z][A-Za-z]+\\))\")\n",
    "consecutive_caps_series = pd.concat([consecutive_caps_series_location,consecutive_caps_series_notes,consecutive_caps_series_action])\n",
    "consecutive_caps_series = consecutive_caps_series[consecutive_caps_series.map(lambda d: len(d)) > 0]\n",
    "slist = []\n",
    "for x in consecutive_caps_series:\n",
    "    slist.extend(x)\n",
    "\n",
    "term_to_abbreviation_dict = {}\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\(([A-Za-z]+)\\)\", l)[0]\n",
    "    len_inside_brackets = len(inside_brackets)\n",
    "    num_words = len(l.split()) - 1\n",
    "    words_before_brackets = []\n",
    "    for i in range(len_inside_brackets):\n",
    "        if i<num_words: words_before_brackets.insert(0,l.split(\" \")[-i-2])\n",
    "    string_before_brackets = \" \".join(words_before_brackets)\n",
    "    if string_before_brackets[0].lower()==inside_brackets[0].lower(): \n",
    "        term_to_abbreviation_dict[string_before_brackets] = inside_brackets\n",
    "for key,val in dict(term_to_abbreviation_dict).items():\n",
    "    if val=='OD': del term_to_abbreviation_dict[key]\n",
    "    elif val=='PIVOTAL': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower().startswith('pri'): del term_to_abbreviation_dict[key]\n",
    "    elif val=='fresh': del term_to_abbreviation_dict[key]\n",
    "    elif val=='West': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower()=='oxynorm': del term_to_abbreviation_dict[key]\n",
    "    elif val=='methylprednisolone': del term_to_abbreviation_dict[key]\n",
    "    elif val=='cetraben': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Levemir': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Desmopressin': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower()=='oramorph': del term_to_abbreviation_dict[key]\n",
    "    elif val=='insulatard': del term_to_abbreviation_dict[key]\n",
    "    elif val=='missing': del term_to_abbreviation_dict[key]\n",
    "    elif val=='insulatard': del term_to_abbreviation_dict[key]\n",
    "    elif val=='SS': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Tramadol': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower()=='eprex': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Tuesday': del term_to_abbreviation_dict[key]\n",
    "    elif val=='cloudy': del term_to_abbreviation_dict[key]\n",
    "    elif val=='stable': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Solent': del term_to_abbreviation_dict[key]\n",
    "    elif val=='carer': del term_to_abbreviation_dict[key]\n",
    "term_to_abbreviation_dict['Intravenous'] = term_to_abbreviation_dict.pop('Intravenous Antibiotics')\n",
    "term_to_abbreviation_dict['Morphine sulphate'] = term_to_abbreviation_dict.pop('Morphine sulphate MR')\n",
    "term_to_abbreviation_dict['Department of Critical Care'] = 'DCCQ'\n",
    "term_to_abbreviation_dict['mau'] = 'amu'\n",
    "\n",
    "consecutive_caps_series_notes_inv = df['inc_notes'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_series_action_inv = df['inc_actiontaken'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_series_notes_inv = consecutive_caps_series_notes_inv[consecutive_caps_series_notes_inv.map(lambda d: len(d)) > 0]\n",
    "slist = []\n",
    "for x in consecutive_caps_series_notes_inv:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\", l)[0]\n",
    "    words_inside_brackets = len(inside_brackets.split())\n",
    "    word_before_brackets = l.split()[0]\n",
    "    if words_inside_brackets==len(word_before_brackets) and inside_brackets[1].lower()==word_before_brackets[0].lower() and inside_brackets.split()[1].lower()[0]==word_before_brackets[1].lower():\n",
    "        term_to_abbreviation_dict[inside_brackets[1:-1]] = word_before_brackets\n",
    "\n",
    "term_to_abbreviation_dict = {key.lower():val.lower() for (key,val) in term_to_abbreviation_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "dict.update(WORDS,abbreviation_counts)\n",
    "dict.update(WORDS,medical_terms_counts)\n",
    "for key in term_to_abbreviation_dict.keys():\n",
    "    if term_to_abbreviation_dict[key] not in WORDS.keys():\n",
    "        WORDS[key] = 1\n",
    "WORDS['moprs'] = 51\n",
    "WORDS['locker'] = 1\n",
    "floors = ['A','B','C','D','E','F','G']\n",
    "for floor in floors:\n",
    "    for i in range(9):\n",
    "        WORDS[floor+str(i+1)] = 1\n",
    "print(WORDS)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"patient\",\"patients\",\"pt\",\"pharmacy\",\"medicine\",\"kd\",\"mso\",\"event\",\"reported\",\"recoded\",\"coding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def american_to_british(tokens):\n",
    "    for t in tokens:\n",
    "        t = re.sub(r\"(...)or$\", r\"\\1our\", t)\n",
    "        t = re.sub(r\"([bt])er$\", r\"\\1re\", t)\n",
    "        t = re.sub(r\"([iy])z(e[drs]|e$|ing|ation)\", r\"\\1s\\2\", t)\n",
    "        t = re.sub(r\"^(s.?[iy])s(e[drs]|e$|ing|ation)\", r\"\\1z\\2\", t) # convert back words starting with s like size, seize\n",
    "        t = re.sub(r\"og$\", \"ogue\", t)\n",
    "        yield t\n",
    "        \n",
    "class CustomVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(american_to_british(tokenize(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward_num_series = df['location'].str.findall(r\"[A-G][0-9]\\s.+\")\n",
    "ward_num_series = ward_num_series[ward_num_series.map(lambda d: len(d)) > 0]\n",
    "slist = []\n",
    "for x in ward_num_series:\n",
    "    slist.extend(x)\n",
    "ward_name_to_num_dict = {}\n",
    "for l in slist:\n",
    "    l_split = l.split()\n",
    "    name = \" \".join(l_split[1:])\n",
    "    name = name.strip(\"- \")\n",
    "    ward_name_to_num_dict[name.lower()] = l_split[0].lower()\n",
    "ward_name_to_num_dict['dccq'] = 'e5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_lemma_dict = {'dos':'dose', 'ttos':'tto', 'cds':'cd', 'discharged':'discharge'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['inc_notes'] = df['inc_notes'].str.lower()\n",
    "for key,val in term_to_abbreviation_dict.items():\n",
    "    df['inc_notes'] = df['inc_notes'].str.replace(key,val)\n",
    "for key,val in ward_name_to_num_dict.items():\n",
    "    df['inc_notes'] = df['inc_notes'].str.replace(key,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        \n",
    "        tokens = [t for t in word_tokenize(doc) if t.isalnum()]\n",
    "        no_stops = [t for t in tokens if t not in my_stop_words]\n",
    "        lemmatized = [self.wnl.lemmatize(t) for t in no_stops]\n",
    "        corrected_lemma = [corrected_lemma_dict.get(t,t) for t in lemmatized]\n",
    "        corrected = [correction(t) for t in corrected_lemma]\n",
    "        return [t for t in corrected if len(t)>1]#corrected_lemma if len(t)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer: tfidf\n",
    "tfidf = CustomVectorizer(tokenizer=LemmaTokenizer())\n",
    "# add argument ngram_range=(1,2) to get word pairs like \"drug chart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(dataframe):\n",
    "    # Apply fit_transform to document: csr_mat\n",
    "    csr_mat = tfidf.fit_transform(dataframe['inc_notes'])\n",
    "    \n",
    "    # Get the words: words\n",
    "    words = tfidf.get_feature_names()\n",
    "    \n",
    "    return csr_mat,words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_components(k,dataframe):\n",
    "    \n",
    "    # Create an NMF instance: model\n",
    "    model = NMF(n_components=k)\n",
    "\n",
    "    # Fit the model to articles\n",
    "    model.fit(csr_mat)\n",
    "\n",
    "    # Transform the articles: nmf_features\n",
    "    nmf_features = model.transform(csr_mat)\n",
    "\n",
    "    # Create a pandas DataFrame: df\n",
    "    df_nmf = pd.DataFrame(nmf_features,index=dataframe['inc_notes'])\n",
    "\n",
    "    # Create a DataFrame: components_df\n",
    "    components_df = pd.DataFrame(model.components_,columns=words)\n",
    "    \n",
    "    df_nmf.columns = df_nmf.columns.astype(str)\n",
    "    df_nmf['max_feature'] = df_nmf.idxmax(axis=1)\n",
    "    \n",
    "    dataframe['max_feature'] = df_nmf['max_feature'].values\n",
    "    theme_counts_series = dataframe['max_feature'].value_counts()\n",
    "    theme_counts_series.index = theme_counts_series.index.astype(int)\n",
    "\n",
    "    donut_seg = 0\n",
    "    for i,counts in theme_counts_series.items():\n",
    "        # Initialize the word cloud\n",
    "        width = int(1024*counts/theme_counts_series.values.max())\n",
    "        height = int(720*counts/theme_counts_series.values.max())\n",
    "        wc = WordCloud(\n",
    "            background_color=\"white\",\n",
    "            width = width,\n",
    "            height = height\n",
    "        )\n",
    "\n",
    "        # Select row : component\n",
    "        component = components_df.iloc[i]\n",
    "\n",
    "        # Generate the cloud\n",
    "        component.nlargest().index = component.nlargest().index.map(str)\n",
    "        wc.generate_from_frequencies(component.nlargest())\n",
    "\n",
    "        # Display the generated image:\n",
    "        figure, (wc_fig, counts_fig) = plt.subplots(nrows=1,ncols=2, figsize=(width/50,height/100))\n",
    "        wc_fig.imshow(wc, interpolation='bilinear')\n",
    "        wc_fig.axis(\"off\");\n",
    "\n",
    "        counts_fig.axis('equal')\n",
    "        colors = ['w' for j in theme_counts_series.index]\n",
    "        colors[donut_seg] = 'b'\n",
    "        labels = ['' for val in theme_counts_series.values]\n",
    "        labels[donut_seg] = str(counts)+\"/\"+str(theme_counts_series.values.sum())\n",
    "        donut_seg += 1\n",
    "        mypie, _ = counts_fig.pie(theme_counts_series.values/theme_counts_series.values.max(), colors=colors, labels=labels, startangle=90, counterclock=False)\n",
    "        plt.setp( mypie, width=0.4, edgecolor='black')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig('3themes.pdf')\n",
    "        plt.savefig('3themes.png')\n",
    "\n",
    "        print('-------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ks = list(range(3,4))\n",
    "csr_mat, words = get_words(df)\n",
    "for k in ks:\n",
    "    set_model_components(k,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df.loc[df['max_feature']==str(1)]\n",
    "df_i = df_i.drop(['max_feature'],axis=1)\n",
    "csr_mat,words = get_words(df_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in list(range(2,13)):\n",
    "    print(\"number sub-themes: \"+str(k))\n",
    "    set_model_components(k,df_i)\n",
    "    print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for i in range(3):\n",
    "    df_i = df.loc[df['max_feature']==str(i)]\n",
    "    df_i = df_i.drop(['max_feature'],axis=1)\n",
    "    words = get_words(df_i)\n",
    "    for k in list(range(2,13)):\n",
    "        set_model_components(k,df_i)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
