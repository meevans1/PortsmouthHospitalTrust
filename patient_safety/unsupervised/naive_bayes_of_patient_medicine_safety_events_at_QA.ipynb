{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with loading all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_conn = pyodbc.connect('DRIVER={SQL Server};'\n",
    "                            'SERVER=L_AAGname;'\n",
    "                            'DATABASE=database_name;'\n",
    "                            'Trusted_Connection=yes') \n",
    "query = \"set transaction isolation level read uncommitted select inc_organisation,inc_locactual,inc_unit,inc_specialty,inc_loctype,inc_result,inc_severity,show_other_contacts,show_employee,show_witness,show_document,inc_reportedby,inc_notes from DatixCRM.dbo.incidents_main where inc_type='PAT' and inc_category='MEDIC'\"\n",
    "df = pd.read_sql(query, sql_conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['division'] = df['inc_unit'].str[:3]\n",
    "df['care group'] = df['inc_unit'].str[3:6]\n",
    "df = df.drop('inc_unit',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.columns = [column.replace('inc_organisation','site') for column in df.columns]\n",
    "df.columns = [column.replace('inc_locactual','ward/dept/unit') for column in df.columns]\n",
    "df.columns = [column.replace('inc_unit','division & care group') for column in df.columns]\n",
    "df.columns = [column.replace('inc_specialty','specialty') for column in df.columns]\n",
    "df.columns = [column.replace('inc_loctype','location type') for column in df.columns]\n",
    "df.columns = [column.replace('inc_result','result') for column in df.columns]\n",
    "df.columns = [column.replace('inc_severity','severity') for column in df.columns]\n",
    "df.columns = [column.replace('inc_reportedby','reported by') for column in df.columns]\n",
    "df.columns = [column.replace('show_other_contacts','other patients involved?') for column in df.columns]\n",
    "df.columns = [column.replace('show_employee','other employees involved?') for column in df.columns]\n",
    "df.columns = [column.replace('show_witness','any witnesses?') for column in df.columns]\n",
    "df.columns = [column.replace('show_document','any documents attached?') for column in df.columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df\n",
    "category_columns = list(df.columns.values)\n",
    "category_columns.remove('inc_notes')\n",
    "count_accuracies = []\n",
    "tfidf_accuracies = []\n",
    "for column in category_columns:\n",
    "    df_dropped = df.dropna(subset=[column])\n",
    "    y = df_dropped[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    df_dropped['inc_notes'], y,\n",
    "                                    test_size=0.33,\n",
    "                                    random_state=53)\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "    count_test = count_vectorizer.transform(X_test.values)\n",
    "    # Print the first 10 features of the count_vectorizer\n",
    "    #print(count_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "\n",
    "    # Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",max_df=0.7)\n",
    "\n",
    "    # Transform the training data: tfidf_train \n",
    "    tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "    # Transform the test data: tfidf_test \n",
    "    tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "    # Print the first 10 features\n",
    "    #print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "    # Print the first 5 vectors of the tfidf training data\n",
    "    #print(tfidf_train.A[:5])\n",
    "\n",
    "\n",
    "    # Create the CountVectorizer DataFrame: count_df\n",
    "    #count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "    # Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "    #tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "    # Print the head of count_df\n",
    "    #print(count_df.head())\n",
    "\n",
    "    # Print the head of tfidf_df\n",
    "    #print(tfidf_df.head())\n",
    "\n",
    "    # Calculate the difference in columns: difference\n",
    "    #difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "    #print(difference)\n",
    "\n",
    "    # Check whether the DataFrames are equal\n",
    "    #print(count_df.equals(tfidf_df))\n",
    "\n",
    "\n",
    "    count_nb_classifier = MultinomialNB()\n",
    "    count_nb_classifier.fit(count_train, y_train)\n",
    "    count_pred = count_nb_classifier.predict(count_test)\n",
    "    count_accuracies.append(100*metrics.accuracy_score(y_test,count_pred))\n",
    "    \n",
    "    tfidf_nb_classifier = MultinomialNB()\n",
    "    tfidf_nb_classifier.fit(tfidf_train, y_train)\n",
    "    tfidf_pred = tfidf_nb_classifier.predict(tfidf_test)\n",
    "    tfidf_accuracies.append(100*metrics.accuracy_score(y_test,tfidf_pred))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_from50 = [int(round(accuracy/2)) for accuracy in count_accuracies]\n",
    "n_category_columns = len(category_columns)\n",
    "colors = [['green' if count_from50[j]>i else 'red' for j in range(n_category_columns)] for i in range(50)]\n",
    "#category_columns = [column.replace('inc_organisation','site') for column in category_columns]\n",
    "#category_columns = [column.replace('inc_locactual','ward/dept/unit') for column in category_columns]\n",
    "#category_columns = [column.replace('inc_unit','division & care group') for column in category_columns]\n",
    "#category_columns = [column.replace('inc_specialty','specialty') for column in category_columns]\n",
    "#category_columns = [column.replace('inc_loctype','location type') for column in category_columns]\n",
    "#category_columns = [column.replace('inc_result','result') for column in category_columns]\n",
    "#category_columns = [column.replace('inc_severity','severity') for column in category_columns]\n",
    "#category_columns = [column.replace('inc_reportedby','reported by') for column in category_columns]\n",
    "#category_columns = [column.replace('show_other_contacts','other patients involved?') for column in category_columns]\n",
    "#category_columns = [column.replace('show_employee','other employees involved?') for column in category_columns]\n",
    "#category_columns = [column.replace('show_witness','any witnesses?') for column in category_columns]\n",
    "#category_columns = [column.replace('show_document','any documents attached?') for column in category_columns]\n",
    "for i in range(50):\n",
    "    plt.scatter(x=np.ones(len(category_columns))*(i+1),y=category_columns,color=colors[i])\n",
    "plt.xlim((0,51));\n",
    "fig1 = plt.figure(1)\n",
    "fig1.text(0, 0.95, \"Correct\", ha=\"center\", va=\"bottom\", size=\"large\", color=\"green\");\n",
    "fig1.text(0.06, 0.95, \"/\", ha=\"center\", va=\"bottom\", size=\"large\");\n",
    "fig1.text(0.13,0.95,\"Incorrect\", ha=\"center\", va=\"bottom\", size=\"large\", color=\"red\");\n",
    "fig1.text(0.19, 0.95, \" prediction from freetext entry for medicine patient safety events\", va=\"bottom\", size=\"large\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_from50 = [int(round(accuracy/2)) for accuracy in tfidf_accuracies]\n",
    "colors = [['green' if tfidf_from50[j]>i else 'red' for j in range(n_category_columns)] for i in range(50)]\n",
    "for i in range(50):\n",
    "    plt.scatter(x=np.ones(len(category_columns))*(i+1),y=category_columns,color=colors[i])\n",
    "plt.xlim((0,51));\n",
    "fig1 = plt.figure(1)\n",
    "fig1.text(0, 0.95, \"Correct\", ha=\"center\", va=\"bottom\", size=\"large\", color=\"green\");\n",
    "fig1.text(0.06, 0.95, \"/\", ha=\"center\", va=\"bottom\", size=\"large\");\n",
    "fig1.text(0.13,0.95,\"Incorrect\", ha=\"center\", va=\"bottom\", size=\"large\", color=\"red\");\n",
    "fig1.text(0.19, 0.95, \" prediction from freetext entry for medicine patient safety events\", va=\"bottom\", size=\"large\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_crammed_columns = [column for column in category_columns]\n",
    "non_crammed_columns.remove('ward/dept/unit')\n",
    "non_crammed_columns.remove('specialty')\n",
    "non_crammed_columns.remove('reported by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count_confusion_matrix():\n",
    "    count_accuracies = []\n",
    "    for column in non_crammed_columns:\n",
    "        df_dropped = df.dropna(subset=[column])\n",
    "        df_dropped = df_dropped[df_dropped[column]!='']\n",
    "        classes = [str(i) for i in df_dropped[column].value_counts().index]\n",
    "        mapping = dict((el,i) for i,el in enumerate(classes)) \n",
    "        df_replaced = df_dropped.replace({column: mapping})\n",
    "        y = df_replaced[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        df_dropped['inc_notes'], y,\n",
    "                                        test_size=0.33,\n",
    "                                        random_state=53)\n",
    "        count_vectorizer = CountVectorizer(stop_words='english')\n",
    "        count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "        count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "        count_nb_classifier = MultinomialNB()\n",
    "        count_nb_classifier.fit(count_train, y_train)\n",
    "        count_pred = count_nb_classifier.predict(count_test)\n",
    "        count_accuracies.append(100*metrics.accuracy_score(y_test,count_pred))\n",
    "\n",
    "        cm = metrics.confusion_matrix(y_test, count_pred, labels=list(mapping.values()))\n",
    "    \n",
    "        plt.figure();\n",
    "        # https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "        fig, ax = plt.subplots(figsize=(11,11));\n",
    "        im = ax.imshow(cm, cmap=plt.cm.Blues);\n",
    "        # create an axes on the right side of ax. The width of cax will be 5%\n",
    "        # of ax and the padding between cax and ax will be fixed at 0.05 inch.\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        ax.figure.colorbar(im, cax=cax)\n",
    "        #plt.title('True vs Predicted \"'+str(column)+'\" from freetext of medicine patient safety events',x=1.2)        \n",
    "        # We want to show all ticks...\n",
    "        ax.set(xticks=np.arange(cm.shape[1]),\n",
    "               yticks=np.arange(cm.shape[0]),\n",
    "               # ... and label them with the respective list entries\n",
    "               xticklabels=classes, yticklabels=classes,\n",
    "               title='True vs Predicted \"'+str(column)+'\" from freetext of medicine patient safety events',\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label');\n",
    "        ax.xaxis.set_label_coords(1.06, -0.01);\n",
    "\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\");\n",
    "\n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        fmt = 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], fmt),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\");\n",
    "        fig.tight_layout();\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_count_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tfidf_confusion_matrix():\n",
    "    tfidf_accuracies = []\n",
    "    for column in non_crammed_columns:\n",
    "        df_dropped = df.dropna(subset=[column])\n",
    "        df_dropped = df_dropped[df_dropped[column]!='']\n",
    "        classes = [str(i) for i in df_dropped[column].value_counts().index]\n",
    "        mapping = dict((el,i) for i,el in enumerate(classes)) \n",
    "        df_replaced = df_dropped.replace({column: mapping})\n",
    "        y = df_replaced[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        df_dropped['inc_notes'], y,\n",
    "                                        test_size=0.33,\n",
    "                                        random_state=53)\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "        tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "        tfidf_nb_classifier = MultinomialNB()\n",
    "        tfidf_nb_classifier.fit(tfidf_train, y_train)\n",
    "        tfidf_pred = tfidf_nb_classifier.predict(tfidf_test)\n",
    "        tfidf_accuracies.append(100*metrics.accuracy_score(y_test,tfidf_pred))\n",
    "\n",
    "        cm = metrics.confusion_matrix(y_test, tfidf_pred, labels=list(mapping.values()))\n",
    "    \n",
    "        plt.figure();\n",
    "        # https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "        fig, ax = plt.subplots(figsize=(11,11));\n",
    "        im = ax.imshow(cm, cmap=plt.cm.Blues);\n",
    "        # create an axes on the right side of ax. The width of cax will be 5%\n",
    "        # of ax and the padding between cax and ax will be fixed at 0.05 inch.\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        ax.figure.colorbar(im, cax=cax)\n",
    "        #plt.title('True vs Predicted \"'+str(column)+'\" from freetext of medicine patient safety events',x=1.2)        \n",
    "        # We want to show all ticks...\n",
    "        ax.set(xticks=np.arange(cm.shape[1]),\n",
    "               yticks=np.arange(cm.shape[0]),\n",
    "               # ... and label them with the respective list entries\n",
    "               xticklabels=classes, yticklabels=classes,\n",
    "               title='True vs Predicted \"'+str(column)+'\" from freetext of medicine patient safety events',\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label');\n",
    "        ax.xaxis.set_label_coords(1.06, -0.01);\n",
    "\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\");\n",
    "\n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        fmt = 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], fmt),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\");\n",
    "        fig.tight_layout();\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_tfidf_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0.1,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def count_train_and_predict(column,alpha):\n",
    "    df_dropped = df.dropna(subset=[column])\n",
    "    y = df_dropped[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    df_dropped['inc_notes'], y,\n",
    "                                    test_size=0.33,\n",
    "                                    random_state=53)\n",
    "\n",
    "    # Initialize a CountVectorizer object: count_vectorizer\n",
    "    count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "    # Transform the training data: count_train \n",
    "    count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "    # Transform the test data: count_test \n",
    "    count_test = count_vectorizer.transform(X_test.values)\n",
    "    \n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(count_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(count_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test,pred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    for column in category_columns:\n",
    "        print('Column: ', column, ' Score: ', count_train_and_predict(column,alpha))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0.1,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def tfidf_train_and_predict(column,alpha):\n",
    "    df_dropped = df.dropna(subset=[column])\n",
    "    y = df_dropped[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    df_dropped['inc_notes'], y,\n",
    "                                    test_size=0.33,\n",
    "                                    random_state=53)\n",
    "\n",
    "    # Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",max_df=0.7)\n",
    "\n",
    "    # Transform the training data: tfidf_train \n",
    "    tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "    # Transform the test data: tfidf_test \n",
    "    tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "    \n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test,pred)\n",
    "    \n",
    "    # Get the class labels: class_labels\n",
    "    class_labels = nb_classifier.classes_\n",
    "\n",
    "    # Extract the features: feature_names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "    feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "    # Print the first class label and the bottom 20  feat_with_weights entries\n",
    "    #for i in range(len(class_labels)):\n",
    "    #    print(class_labels[i], feat_with_weights[-20:])\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    for column in category_columns:\n",
    "        print('Column: ', column, ' Score: ', tfidf_train_and_predict(column,alpha))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
