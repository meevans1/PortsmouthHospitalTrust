{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation into medication patient safety events\n",
    "\n",
    "Proof of concept to see whether Natural Language Processing (NLP) can find the themes within medication patient safety events.\n",
    "\n",
    "This notebook is one of the main examples of using NLP in this project.\n",
    "\n",
    "Start with loading all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for manipulating data in dataframes\n",
    "import pyodbc # for reading sql into pandas\n",
    "import numpy as np # for numerical calculations\n",
    "from collections import Counter # for counting the number of words in dictionaries\n",
    "import re # for finding regular expressions in text\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk import word_tokenize # to break sentences into words\n",
    "from nltk.stem import WordNetLemmatizer # to find the lemmas of words\n",
    "\n",
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from wordcloud import WordCloud # to visualise wordclous\n",
    "import matplotlib.pyplot as plt # other visualisations\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the start date for your report here!\n",
    "The code then automatically calculates the end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2019,9,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = str(start_date + relativedelta(months=+3))\n",
    "start_date = str(start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define medical dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomedct = pd.read_csv('sct2_Description_Snapshot-en_INT_20190731.txt',sep=\"\\t\",usecols=['term'])\n",
    "medical_terms_series = snomedct['term'].str.lower().str.split().dropna()\n",
    "medical_terms_list = []\n",
    "for x in medical_terms_series:\n",
    "    medical_terms_list.extend(x)\n",
    "medical_terms_list = [medical_term for medical_term in medical_terms_list if medical_term.isalpha()]\n",
    "medical_terms_list = [medical_term.strip(\"()\") for medical_term in medical_terms_list]\n",
    "medical_terms_list = [medical_term.strip(\"(\") for medical_term in medical_terms_list]\n",
    "medical_terms_counts = Counter(medical_terms_list)\n",
    "print(medical_terms_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read SQL into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_conn = pyodbc.connect('DRIVER={SQL Server};'\n",
    "                            'SERVER=L_AAGDATIX;'\n",
    "                            'DATABASE=DatixCRM;'\n",
    "                            'Trusted_Connection=yes') \n",
    "query = \"set transaction isolation level read uncommitted select b.description,a.inc_dincident,a.inc_severity,a.inc_notes,a.inc_actiontaken from DatixCRM.dbo.code_locactual b join DatixCRM.dbo.incidents_main a on a.inc_locactual=b.code where a.inc_type='PAT' and a.inc_category='MEDIC' and a.inc_organisation='QA' and b.cod_parent2='QA'\"\n",
    "df = pd.read_sql(query, sql_conn)\n",
    "df = df.dropna()\n",
    "column_list = list(df.columns)\n",
    "column_list[0] = 'location'\n",
    "df.columns = column_list\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the abbreviations that doctors use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['inc_notes'].str.contains('o2')]['inc_notes'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_caps_after = snomedct['term'].str.findall(r\"((?:\\b[A-Za-z]+\\b\\s)+\\([A-Z][A-Z]+\\))\")\n",
    "consecutive_caps_after = consecutive_caps_after[consecutive_caps_after.astype(str)!='[]']\n",
    "consecutive_caps_after = consecutive_caps_after.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_after:\n",
    "    slist.extend(x)\n",
    "\n",
    "term_to_abbreviation_dict = {}\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\(([A-Za-z]+)\\)\", l)[0]\n",
    "    len_inside_brackets = len(inside_brackets)\n",
    "    num_words = len(l.split()) - 1\n",
    "    if len_inside_brackets == num_words:\n",
    "        words_before_brackets = []\n",
    "        for i in range(len_inside_brackets):\n",
    "            if i<num_words: words_before_brackets.insert(0,l.split(\" \")[-i-2])\n",
    "        string_before_brackets = \" \".join(words_before_brackets)\n",
    "        if all(words_before_brackets[i].lower()[0]==inside_brackets[i].lower() for i in range(len_inside_brackets)):\n",
    "            if string_before_brackets not in term_to_abbreviation_dict.keys() and inside_brackets not in term_to_abbreviation_dict.values():\n",
    "                term_to_abbreviation_dict[string_before_brackets] = inside_brackets\n",
    "        \n",
    "\n",
    "consecutive_caps_before = snomedct['term'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_before = consecutive_caps_before[consecutive_caps_before.astype(str)!='[]']\n",
    "consecutive_caps_before = consecutive_caps_before.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_before:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\", l)[0]\n",
    "    inside_brackets = inside_brackets.strip('()')\n",
    "    words_inside_brackets = inside_brackets.split()\n",
    "    num_words_inside_brackets = len(words_inside_brackets)\n",
    "    word_before_brackets = l.split()[0]\n",
    "    if num_words_inside_brackets==len(word_before_brackets) and all(words_inside_brackets[i].lower()[0]==word_before_brackets[i].lower() for i in range(num_words_inside_brackets)):\n",
    "        if inside_brackets not in term_to_abbreviation_dict.keys() and word_before_brackets not in term_to_abbreviation_dict.values():\n",
    "            term_to_abbreviation_dict[inside_brackets] = word_before_brackets\n",
    "            \n",
    "\n",
    "consecutive_caps_dash = snomedct['term'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\-\\s(?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\")\n",
    "consecutive_caps_dash = consecutive_caps_dash[consecutive_caps_dash.astype(str)!='[]']\n",
    "consecutive_caps_dash = consecutive_caps_dash.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_dash:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    after_dash = l.split(' - ')[1]\n",
    "    words_after_dash = after_dash.split()\n",
    "    num_words_after_dash = len(words_after_dash)\n",
    "    word_before_dash = l.split(' - ')[0]\n",
    "    if num_words_after_dash==len(word_before_dash) and all(words_after_dash[i].lower()[0]==word_before_dash[i].lower() for i in range(num_words_after_dash)):\n",
    "        if after_dash not in term_to_abbreviation_dict.keys():\n",
    "            term_to_abbreviation_dict[after_dash] = word_before_dash\n",
    "\n",
    "\n",
    "\n",
    "consecutive_caps_series_location = df['location'].str.findall(r\"((?:\\b[A-Za-z&]+\\b\\s)+\\([A-Za-z][A-Za-z]+\\))\")\n",
    "consecutive_caps_series_notes = df['inc_notes'].str.findall(r\"((?:\\b[A-Za-z]+\\b\\s)+\\([A-Za-z][A-Za-z]+\\))\")\n",
    "consecutive_caps_series_action = df['inc_actiontaken'].str.findall(r\"((?:\\b[A-Za-z]+\\b\\s)+\\([A-Za-z][A-Za-z]+\\))\")\n",
    "consecutive_caps_after = pd.concat([consecutive_caps_series_location,consecutive_caps_series_notes,consecutive_caps_series_action])\n",
    "consecutive_caps_after = consecutive_caps_after[consecutive_caps_after.astype(str)!='[]']\n",
    "consecutive_caps_after = consecutive_caps_after.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_after:\n",
    "    slist.extend(x)\n",
    "\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\(([A-Za-z]+)\\)\", l)[0]\n",
    "    len_inside_brackets = len(inside_brackets)\n",
    "    num_words = len(l.split()) - 1\n",
    "    if len_inside_brackets == num_words:\n",
    "        words_before_brackets = []\n",
    "        for i in range(len_inside_brackets):\n",
    "            if i<num_words: words_before_brackets.insert(0,l.split()[-i-2])\n",
    "        string_before_brackets = \" \".join(words_before_brackets)\n",
    "        if all(words_before_brackets[i].lower()[0]==inside_brackets[i].lower() for i in range(len_inside_brackets)):\n",
    "            if string_before_brackets not in term_to_abbreviation_dict.keys() and inside_brackets not in term_to_abbreviation_dict.values():\n",
    "                term_to_abbreviation_dict[string_before_brackets] = inside_brackets\n",
    "        \n",
    "\n",
    "consecutive_caps_series_location = df['location'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_series_notes = df['inc_notes'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_series_action = df['inc_actiontaken'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\")\n",
    "consecutive_caps_before = pd.concat([consecutive_caps_series_location,consecutive_caps_series_notes,consecutive_caps_series_action])\n",
    "consecutive_caps_before = consecutive_caps_before[consecutive_caps_before.astype(str)!='[]']\n",
    "consecutive_caps_before = consecutive_caps_before.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_before:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    inside_brackets = re.findall(r\"\\((?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\\)\", l)[0]\n",
    "    inside_brackets = inside_brackets.strip('()')\n",
    "    words_inside_brackets = inside_brackets.split()\n",
    "    num_words_inside_brackets = len(words_inside_brackets)\n",
    "    word_before_brackets = l.split()[0]\n",
    "    if num_words_inside_brackets==len(word_before_brackets) and all(words_inside_brackets[i].lower()[0]==word_before_brackets[i].lower() for i in range(num_words_inside_brackets)):\n",
    "        if inside_brackets not in term_to_abbreviation_dict.keys() and word_before_brackets not in term_to_abbreviation_dict.values():\n",
    "            term_to_abbreviation_dict[inside_brackets] = word_before_brackets\n",
    "            \n",
    "\n",
    "consecutive_caps_series_location = df['location'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\-\\s(?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\")\n",
    "consecutive_caps_series_notes = df['inc_notes'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\-\\s(?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\")\n",
    "consecutive_caps_series_action = df['inc_actiontaken'].str.findall(r\"[A-Za-z][A-Za-z]+\\s\\-\\s(?:\\b[A-Za-z]+\\b\\s)(?:\\b[A-Za-z]+\\b\\s?)+\")\n",
    "consecutive_caps_dash = pd.concat([consecutive_caps_series_location,consecutive_caps_series_notes,consecutive_caps_series_action])\n",
    "consecutive_caps_dash = consecutive_caps_dash[consecutive_caps_dash.astype(str)!='[]']\n",
    "consecutive_caps_dash = consecutive_caps_dash.dropna()\n",
    "slist = []\n",
    "for x in consecutive_caps_dash:\n",
    "    slist.extend(x)\n",
    "for l in slist:\n",
    "    after_dash = l.split('-')[1]\n",
    "    words_after_dash = after_dash.split()\n",
    "    num_words_after_dash = len(words_after_dash)\n",
    "    word_before_dash = l.split(' - ')[0]\n",
    "    if num_words_after_dash==len(word_before_dash) and all(words_after_dash[i].lower()[0]==word_before_dash[i].lower() for i in range(num_words_after_dash)):\n",
    "        after_dash = after_dash.strip()\n",
    "        if after_dash not in term_to_abbreviation_dict.keys() and word_before_dash not in term_to_abbreviation_dict.values():\n",
    "            term_to_abbreviation_dict[after_dash] = word_before_dash\n",
    "            \n",
    "for key,val in dict(term_to_abbreviation_dict).items():\n",
    "    if val=='OD': del term_to_abbreviation_dict[key]\n",
    "    elif val=='PIVOTAL': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower().startswith('pri'): del term_to_abbreviation_dict[key]\n",
    "    elif val=='fresh': del term_to_abbreviation_dict[key]\n",
    "    elif val=='West': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower()=='oxynorm': del term_to_abbreviation_dict[key]\n",
    "    elif val=='methylprednisolone': del term_to_abbreviation_dict[key]\n",
    "    elif val=='cetraben': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Levemir': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Desmopressin': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower()=='oramorph': del term_to_abbreviation_dict[key]\n",
    "    elif val=='insulatard': del term_to_abbreviation_dict[key]\n",
    "    elif val=='missing': del term_to_abbreviation_dict[key]\n",
    "    elif val=='insulatard': del term_to_abbreviation_dict[key]\n",
    "    elif val=='SS': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Tramadol': del term_to_abbreviation_dict[key]\n",
    "    elif val.lower()=='eprex': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Tuesday': del term_to_abbreviation_dict[key]\n",
    "    elif val=='cloudy': del term_to_abbreviation_dict[key]\n",
    "    elif val=='stable': del term_to_abbreviation_dict[key]\n",
    "    elif val=='Solent': del term_to_abbreviation_dict[key]\n",
    "    elif val=='carer': del term_to_abbreviation_dict[key]\n",
    "term_to_abbreviation_dict['Intravenous Antibiotics'] = 'IV'\n",
    "term_to_abbreviation_dict['Intravenous'] = 'IV'\n",
    "term_to_abbreviation_dict['Morphine sulphate MR'] = 'MS'\n",
    "term_to_abbreviation_dict['Morphine sulphate'] = 'MS'\n",
    "term_to_abbreviation_dict['milligram'] = 'mg'\n",
    "term_to_abbreviation_dict['Department of Critical Care'] = 'DCCQ'\n",
    "term_to_abbreviation_dict['mau'] = 'amu'\n",
    "term_to_abbreviation_dict['controlled drug'] = 'cd'\n",
    "term_to_abbreviation_dict['patient(.{1,3})own drug'] = \"pod\"\n",
    "term_to_abbreviation_dict['twice a day'] = \"bd\"\n",
    "term_to_abbreviation_dict['twice daily'] = \"bd\"\n",
    "term_to_abbreviation_dict['to take out'] = \"tto\"\n",
    "term_to_abbreviation_dict['mino2'] = \"min o2\"\n",
    "term_to_abbreviation_dict['lo2'] = \"l o2\"\n",
    "term_to_abbreviation_dict[' o2'] = \" oxygen\"\n",
    "term_to_abbreviation_dict[' po2'] = \" partial pressure of oxygen\"\n",
    "term_to_abbreviation_dict['spo2'] = \"peripheral capillary oxygen saturation\"\n",
    "term_to_abbreviation_dict['sao2'] = \"oxygen saturation\"\n",
    "term_to_abbreviation_dict['fio2'] = \"fraction of inspired oxygen\"\n",
    "\n",
    "#term_to_abbreviation_dict = {key.lower():val.lower() for (key,val) in term_to_abbreviation_dict.items()}\n",
    "for key,val in term_to_abbreviation_dict.items():\n",
    "    print(key+\" & \"+val+'\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = [v.lower() for v in term_to_abbreviation_dict.values()]\n",
    "abbreviation_counts = Counter(abbreviations)\n",
    "print(abbreviation_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove entries with incident date before 1st April 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['inc_severity']!='']\n",
    "df = df[df['inc_dincident'] > '2016-03-31']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define spelling correction tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter()\n",
    "english_WORDS = Counter(words(open('big.txt').read()))\n",
    "dict.update(WORDS,english_WORDS)\n",
    "dict.update(WORDS,medical_terms_counts)\n",
    "dict.update(WORDS,abbreviation_counts)\n",
    "floors = ['A','B','C','D','E','F','G']\n",
    "for floor in floors:\n",
    "    for i in range(9):\n",
    "        WORDS[floor+str(i+1)] = 1\n",
    "WORDS['nomad'] = 1\n",
    "print(WORDS)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add words that you don't want in the wordclouds like, union(['useless','word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"patient\",\"patients\",\"pt\",\"pharmacy\",\"medicine\",\"kd\",\"mso\",\"event\",\"reported\",\"recoded\",\"coding\",\"did\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define American->British correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def american_to_british(tokens):\n",
    "    for t in tokens:\n",
    "        t = re.sub(r\"(...)or$\", r\"\\1our\", t)\n",
    "        t = re.sub(r\"([bt])er$\", r\"\\1re\", t)\n",
    "        t = re.sub(r\"([iy])z(e[drs]|e$|ing|ation)\", r\"\\1s\\2\", t)\n",
    "        t = re.sub(r\"^(s.?[iy])s(e[drs]|e$|ing|ation)\", r\"\\1z\\2\", t) # convert back words starting with s like size, seize\n",
    "        t = re.sub(r\"og$\", \"ogue\", t)\n",
    "        yield t\n",
    "        \n",
    "class CustomVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(american_to_british(tokenize(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward_num_series = df['location'].str.findall(r\"[A-G][0-9]\\s.+\")\n",
    "ward_num_series = ward_num_series[ward_num_series.map(lambda d: len(d)) > 0]\n",
    "slist = []\n",
    "for x in ward_num_series:\n",
    "    slist.extend(x)\n",
    "ward_name_to_num_dict = {}\n",
    "for l in slist:\n",
    "    l_split = l.split()\n",
    "    name = \" \".join(l_split[1:])\n",
    "    name = name.strip(\"- \")\n",
    "    ward_name_to_num_dict[name.lower()] = l_split[0].lower()\n",
    "ward_name_to_num_dict['dccq'] = 'e5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add words that come out wrong after lemmatization, like {'dos':'dose'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_correction_dict = {' dos ':' dose ', ' doses ':' dose ', ' ttos ':' tto ', ' cds ':' cd ', ' discharged ':' discharge ', 'non clinical':''}\n",
    "corrected_lemma_dict = {'stat':'stated'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['inc_notes'] = df['inc_notes'].str.lower()\n",
    "df['inc_notes'] = df['inc_notes'].replace(term_to_abbreviation_dict, regex=True)\n",
    "df['inc_notes'] = df['inc_notes'].replace(pre_correction_dict, regex=True)\n",
    "df['inc_notes'] = df['inc_notes'].replace(ward_name_to_num_dict, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        \n",
    "        tokens = [t for t in word_tokenize(doc) if t.isalpha()]\n",
    "        no_stops = [t for t in tokens if t not in my_stop_words]\n",
    "        lemmatized = [self.wnl.lemmatize(t) for t in no_stops]\n",
    "        corrected_lemma = [corrected_lemma_dict.get(t,t) for t in lemmatized]\n",
    "        corrected = [correction(t) for t in corrected_lemma]\n",
    "        return [t for t in corrected if len(t)>1]#corrected_lemma if len(t)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer: tfidf\n",
    "tfidf = CustomVectorizer(tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fit_transform to document: csr_mat\n",
    "csr_mat = tfidf.fit_transform(df['inc_notes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the words: words\n",
    "words = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print words \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month_year'] = df['inc_dincident'].map(lambda x: x.strftime('%Y-%m'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_year_value_counts = df['month_year'].value_counts().sort_index()\n",
    "df['inc_severity'] = pd.Categorical(df['inc_severity'], [\"NMISS\", \"NONE\", \"LOW\", \"MODRTE\", \"SEVERE\", \"DEATH\"])\n",
    "df = df.sort_values('inc_severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def severity_over_time_plot(dataframe):\n",
    "    severity_counts = dataframe['inc_severity'].value_counts()[dataframe['inc_severity'].unique()]\n",
    "    cumulative_bottom = 0\n",
    "    month_years = list(month_year_value_counts.index)\n",
    "    bins = list(month_year_value_counts.index)\n",
    "    bins.append('9999-12')\n",
    "    color_dict = {'NMISS':'b','NONE':'g','LOW':'y','MODRTE':'orange','SEVERE':'r','DEATH':'k'}\n",
    "    plt.figure(figsize=(17,14))\n",
    "    for i,counts in severity_counts.items():\n",
    "        height_severity_total,_ = np.histogram(dataframe[dataframe['inc_severity']==i]['month_year'].values, bins=bins)\n",
    "        plt.bar(month_years, height_severity_total, bottom=cumulative_bottom, label=i, color=color_dict[i]);\n",
    "        cumulative_bottom += height_severity_total\n",
    "    plt.xticks(rotation='vertical');\n",
    "    plt.legend()\n",
    "    plt.title('Medicine Safety Learning Events by month')\n",
    "    plt.show()\n",
    "severity_over_time_plot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def severity_pie(dataframe):\n",
    "    severity_counts = dataframe['inc_severity'].value_counts()[dataframe['inc_severity'].unique()]\n",
    "    colors = ['b','g','y','orange','r','k']\n",
    "    wedges = plt.pie(severity_counts, colors=colors, startangle=90, counterclock=False);\n",
    "    plt.legend(list(severity_counts.index), loc=(1,0.5))\n",
    "    plt.title('Severity of SLE\\n'+str(dataframe['inc_dincident'].min().strftime('%b'))+' '+str(dataframe['inc_dincident'].min().strftime('%y'))+' - '+str(dataframe['inc_dincident'].max().strftime('%b'))+' '+str(dataframe['inc_dincident'].max().strftime('%y')))\n",
    "    plt.show()\n",
    "severity_pie(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wordclouds_and_plots():\n",
    "    donut_seg = 0\n",
    "    for i,counts in theme_counts_series.items():\n",
    "        # Initialize the word cloud\n",
    "        width = 1024 #int(1024*counts/theme_counts_max)\n",
    "        height = 720 #int(720*counts/theme_counts_max)\n",
    "        wc = WordCloud(\n",
    "            background_color=\"white\",\n",
    "            width = width,\n",
    "            height = height\n",
    "        )\n",
    "\n",
    "        # Select row : component\n",
    "        component = components_df.iloc[i]\n",
    "\n",
    "        # Generate the cloud\n",
    "        component.nlargest().index = component.nlargest().index.map(str)\n",
    "        wc.generate_from_frequencies(component.nlargest())\n",
    "        for word in range(5):\n",
    "            if component.nlargest().index[word] not in unique_wordcloud_words: \n",
    "                unique_wordcloud_words.append(component.nlargest().index[word])\n",
    "\n",
    "        # Display the generated image:\n",
    "        figure, (wc_fig, counts_fig) = plt.subplots(nrows=1,ncols=2, figsize=(width/50,height/100))\n",
    "        wc_fig.imshow(wc, interpolation='bilinear')\n",
    "        wc_fig.axis(\"off\");\n",
    "\n",
    "        counts_fig.axis('equal')\n",
    "        colors = ['w' for j in theme_counts_series.index]\n",
    "        colors[donut_seg] = 'b'\n",
    "        labels = ['' for val in theme_counts_series.values]\n",
    "        labels[donut_seg] = str(counts)+\"/\"+str(theme_counts_series.values.sum())\n",
    "        donut_seg += 1\n",
    "        mypie, texts = counts_fig.pie(theme_counts_series.values/theme_counts_max, colors=colors, labels=labels, startangle=90, counterclock=False)\n",
    "        for text in texts: text.set_fontsize(20)\n",
    "        plt.setp( mypie, width=0.4, edgecolor='black')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        df_max_feature = df[df['max_feature']==str(i)].copy()\n",
    "        \n",
    "        severity_over_time_plot(df_max_feature)\n",
    "        \n",
    "        df_max_feature_date = df_max_feature[df_max_feature['inc_dincident'] >= start_date].copy()\n",
    "        df_max_feature_date = df_max_feature_date[df_max_feature_date['inc_dincident'] < end_date]\n",
    "        severity_pie(df_max_feature_date)\n",
    "\n",
    "        print('-------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unique_wordcloud_words = []\n",
    "oxygen_keyword = False\n",
    "k = 4\n",
    "while oxygen_keyword==False:\n",
    "    # Create an NMF instance: model\n",
    "    model = NMF(n_components=k)\n",
    "\n",
    "    # Fit the model to articles\n",
    "    model.fit(csr_mat)\n",
    "    \n",
    "    # Transform the articles: nmf_features\n",
    "    nmf_features = model.transform(csr_mat)\n",
    "\n",
    "    # Create a pandas DataFrame: df\n",
    "    df_nmf = pd.DataFrame(nmf_features,index=df['inc_notes'])\n",
    "\n",
    "    # Create a DataFrame: components_df\n",
    "    components_df = pd.DataFrame(model.components_,columns=words)\n",
    "    \n",
    "    df_nmf.columns = df_nmf.columns.astype(str)\n",
    "    df_nmf['max_feature'] = df_nmf.idxmax(axis=1)\n",
    "    \n",
    "    df['max_feature'] = df_nmf['max_feature'].values\n",
    "    theme_counts_series = df['max_feature'].value_counts()\n",
    "    theme_counts_series.index = theme_counts_series.index.astype(int)\n",
    "    theme_counts_max = theme_counts_series.values.max()\n",
    "    \n",
    "    print(\"number themes: \"+str(k))\n",
    "    print_wordclouds_and_plots()\n",
    "\n",
    "    print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "    \n",
    "    if 'oxygen' in unique_wordcloud_words: oxygen_keyword=True\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
